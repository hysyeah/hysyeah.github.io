<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[golang container list实现]]></title>
    <url>%2F2023%2F09%2F24%2Fgolang-container-list%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[golang源码中的container列表实现。 源码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210// Element 表示列表中的一个节点type Element struct &#123; // Next and previous pointers in the doubly-linked list of elements. // To simplify the implementation, internally a list l is implemented // as a ring, such that &amp;l.root is both the next element of the last // list element (l.Back()) and the previous element of the first list // element (l.Front()). // 指向下一个节点和前一个节点的指针 next, prev *Element // The list to which this element belongs. // List指针表示该节点所属的列表 list *List // The value stored with this element. // 节点的值 Value any&#125;// Next 返回下一个节点口的指针func (e *Element) Next() *Element &#123; // 如果e.list != nil 且下一个节点不是root则返回e.next if p := e.next; e.list != nil &amp;&amp; p != &amp;e.list.root &#123; return p &#125; return nil&#125;// Prev 返回上一个节点的指针func (e *Element) Prev() *Element &#123; // 如果e.list != nil 且下一个节点不是root则返回e.prev if p := e.prev; e.list != nil &amp;&amp; p != &amp;e.list.root &#123; return p &#125; return nil&#125;// List 表示一个双向链表// 链表的最后一个节点的next指针和链表的第一个节点的prev指针都指向roottype List struct &#123; root Element // sentinel list element, only &amp;root, root.prev, and root.next are used len int // current list length excluding (this) sentinel element&#125;// Init 初始化或清除一个列表func (l *List) Init() *List &#123; l.root.next = &amp;l.root l.root.prev = &amp;l.root l.len = 0 return l&#125;// New 初始化一个列表并返回其指针func New() *List &#123; return new(List).Init() &#125;// Len 返回列表的长度func (l *List) Len() int &#123; return l.len &#125;// Front 返回列表的第一个节点指针func (l *List) Front() *Element &#123; if l.len == 0 &#123; return nil &#125; return l.root.next&#125;// Back 返回列表的最后一个指针func (l *List) Back() *Element &#123; if l.len == 0 &#123; return nil &#125; return l.root.prev&#125;// lazyInit lazily initializes a zero List value.func (l *List) lazyInit() &#123; if l.root.next == nil &#123; l.Init() &#125;&#125;// insert 在节点at之后插入节点e, 并返回节点e指针func (l *List) insert(e, at *Element) *Element &#123; e.prev = at e.next = at.next e.prev.next = e e.next.prev = e e.list = l l.len++ return e&#125;// insertValue 对insert的封装func (l *List) insertValue(v any, at *Element) *Element &#123; return l.insert(&amp;Element&#123;Value: v&#125;, at)&#125;// remove 将节点从列表中移除func (l *List) remove(e *Element) &#123; e.prev.next = e.next e.next.prev = e.prev e.next = nil // avoid memory leaks e.prev = nil // avoid memory leaks e.list = nil l.len--&#125;// move 将节点e移动到节点at之后func (l *List) move(e, at *Element) &#123; if e == at &#123; return &#125; e.prev.next = e.next e.next.prev = e.prev e.prev = at e.next = at.next e.prev.next = e e.next.prev = e&#125;// Remove 将节点从列表中移除func (l *List) Remove(e *Element) any &#123; if e.list == l &#123; // if e.list == l, l must have been initialized when e was inserted // in l or l == nil (e is a zero Element) and l.remove will crash l.remove(e) &#125; return e.Value&#125;// PushFront 将节点插入到列表头func (l *List) PushFront(v any) *Element &#123; l.lazyInit() return l.insertValue(v, &amp;l.root)&#125;// PushBack 将节点插入到列表尾部func (l *List) PushBack(v any) *Element &#123; l.lazyInit() return l.insertValue(v, l.root.prev)&#125;// InsertBefore 将v插入到节点mark之前func (l *List) InsertBefore(v any, mark *Element) *Element &#123; if mark.list != l &#123; return nil &#125; // see comment in List.Remove about initialization of l return l.insertValue(v, mark.prev)&#125;// InsertAfter 将v插入到节点mark之后func (l *List) InsertAfter(v any, mark *Element) *Element &#123; if mark.list != l &#123; return nil &#125; // see comment in List.Remove about initialization of l return l.insertValue(v, mark)&#125;// MoveToFront 将节点e移动到列表头func (l *List) MoveToFront(e *Element) &#123; if e.list != l || l.root.next == e &#123; return &#125; // see comment in List.Remove about initialization of l l.move(e, &amp;l.root)&#125;// MoveToBack 将节点e移动到列表末尾func (l *List) MoveToBack(e *Element) &#123; if e.list != l || l.root.prev == e &#123; return &#125; // see comment in List.Remove about initialization of l l.move(e, l.root.prev)&#125;// MoveBefore 将节点e移动到mark之前func (l *List) MoveBefore(e, mark *Element) &#123; if e.list != l || e == mark || mark.list != l &#123; return &#125; l.move(e, mark.prev)&#125; // MoveAfter 将节点e移动到mark之后func (l *List) MoveAfter(e, mark *Element) &#123; if e.list != l || e == mark || mark.list != l &#123; return &#125; l.move(e, mark)&#125;// PushBackList 将一个列表中的所有元素加入l中，在l之后func (l *List) PushBackList(other *List) &#123; l.lazyInit() for i, e := other.Len(), other.Front(); i &gt; 0; i, e = i-1, e.Next() &#123; l.insertValue(e.Value, l.root.prev) &#125;&#125;// PushFrontList 将一个列表中的所有元素加入l中，在l之前func (l *List) PushFrontList(other *List) &#123; l.lazyInit() for i, e := other.Len(), other.Back(); i &gt; 0; i, e = i-1, e.Prev() &#123; l.insertValue(e.Value, &amp;l.root) &#125;&#125; 使用示例12345678910111213141516171819package mainimport ( "container/list" "fmt")func main() &#123; l := list.New() e4 := l.PushBack(4) e1 := l.PushFront(1) l.InsertBefore(3, e4) l.InsertAfter(2, e1) // Iterate through list and print its contents. for e := l.Front(); e != nil; e = e.Next() &#123; fmt.Println(e.Value) &#125;&#125; REF: container/list/list.go]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s expiring lru cache]]></title>
    <url>%2F2023%2F09%2F24%2Fk8s-expiring-lru-cache%2F</url>
    <content type="text"><![CDATA[LRUExpireCache 是一种支持数据过期的 LRU（最近最少使用）缓存策略。当缓存达到最大大小（maxsize）后，在Add操作中最近最少使用的项目将会被移除，在Get操作中如果项目过期将会被移除。下面是k8s源码中LRUExpireCache的实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import ( "container/list" "sync" "time")// Clock defines an interface for obtaining the current timetype Clock interface &#123; Now() time.Time&#125;// realClock implements the Clock interface by calling time.Now()type realClock struct&#123;&#125;func (realClock) Now() time.Time &#123; return time.Now() &#125;// LRUExpireCache is a cache that ensures the mostly recently accessed keys are returned with// a ttl beyond which keys are forcibly expired.// LRUExpireCache 实现一个带过期时间的LRU Cachetype LRUExpireCache struct &#123; // clock is used to obtain the current time // 用于获取当前时间 clock Clock lock sync.Mutex maxSize int // 官方提供的列表实现(双向链表) evictionList list.List entries map[interface&#123;&#125;]*list.Element&#125;// NewLRUExpireCache creates an expiring cache with the given size// NewLRUExpireCache 创建一个指定大小LRU Expire Cachefunc NewLRUExpireCache(maxSize int) *LRUExpireCache &#123; return NewLRUExpireCacheWithClock(maxSize, realClock&#123;&#125;)&#125;// NewLRUExpireCacheWithClock creates an expiring cache with the given size, using the specified clock to obtain the current time.func NewLRUExpireCacheWithClock(maxSize int, clock Clock) *LRUExpireCache &#123; if maxSize &lt;= 0 &#123; panic("maxSize must be &gt; 0") &#125; return &amp;LRUExpireCache&#123; clock: clock, maxSize: maxSize, entries: map[interface&#123;&#125;]*list.Element&#123;&#125;, &#125;&#125;// cacheEntry 包括key, value, expireTimetype cacheEntry struct &#123; key interface&#123;&#125; value interface&#123;&#125; expireTime time.Time&#125;// Add adds the value to the cache at key with the specified maximum duration.func (c *LRUExpireCache) Add(key interface&#123;&#125;, value interface&#123;&#125;, ttl time.Duration) &#123; c.lock.Lock() defer c.lock.Unlock() // Key already exists oldElement, ok := c.entries[key] // 如果key已经存在 if ok &#123; // 将元素移到列表头 c.evictionList.MoveToFront(oldElement) // 更新值和过期时间 oldElement.Value.(*cacheEntry).value = value oldElement.Value.(*cacheEntry).expireTime = c.clock.Now().Add(ttl) return &#125; // Make space if necessary // 如果列表长度已经大于设置的最大值 if c.evictionList.Len() &gt;= c.maxSize &#123; // 从列表和entries删除最旧的元素 toEvict := c.evictionList.Back() c.evictionList.Remove(toEvict) delete(c.entries, toEvict.Value.(*cacheEntry).key) &#125; // Add new entry // 插入最新的元素 entry := &amp;cacheEntry&#123; key: key, value: value, expireTime: c.clock.Now().Add(ttl), &#125; element := c.evictionList.PushFront(entry) c.entries[key] = element&#125;// Get returns the value at the specified key from the cache if it exists and is not// expired, or returns false.func (c *LRUExpireCache) Get(key interface&#123;&#125;) (interface&#123;&#125;, bool) &#123; c.lock.Lock() defer c.lock.Unlock() element, ok := c.entries[key] if !ok &#123; return nil, false &#125; // 如果这个元素已过期则移除 if c.clock.Now().After(element.Value.(*cacheEntry).expireTime) &#123; c.evictionList.Remove(element) delete(c.entries, key) return nil, false &#125; // 将元素移到列表头 c.evictionList.MoveToFront(element) return element.Value.(*cacheEntry).value, true&#125;// Remove removes the specified key from the cache if it exists// Remove 从Cache中移除指定的keyfunc (c *LRUExpireCache) Remove(key interface&#123;&#125;) &#123; c.lock.Lock() defer c.lock.Unlock() element, ok := c.entries[key] if !ok &#123; return &#125; c.evictionList.Remove(element) delete(c.entries, key)&#125;// Keys returns all unexpired keys in the cache.//// Keep in mind that subsequent calls to Get() for any of the returned keys// might return "not found".//// Keys are returned ordered from least recently used to most recently used.func (c *LRUExpireCache) Keys() []interface&#123;&#125; &#123; c.lock.Lock() defer c.lock.Unlock() now := c.clock.Now() val := make([]interface&#123;&#125;, 0, c.evictionList.Len()) for element := c.evictionList.Back(); element != nil; element = element.Prev() &#123; // Only return unexpired keys if !now.After(element.Value.(*cacheEntry).expireTime) &#123; val = append(val, element.Value.(*cacheEntry).key) &#125; &#125; return val&#125; REF: lruexpirecache.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s expiring cache]]></title>
    <url>%2F2023%2F09%2F23%2Fk8s-expiring-cache%2F</url>
    <content type="text"><![CDATA[如何实现一个带ttl的缓存。今天来学习下k8s源码中是怎么实现的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205package cacheimport ( "container/heap" "sync" "time" "k8s.io/utils/clock")// NewExpiring 返回初始化后的Expiringfunc NewExpiring() *Expiring &#123; return NewExpiringWithClock(clock.RealClock&#123;&#125;)&#125;// NewExpiringWithClock 与NewExpiring的不同之处在于可以传入一个clock参数，方便在测试中使用func NewExpiringWithClock(clock clock.Clock) *Expiring &#123; // clock使用的是k8s封装过后的clock return &amp;Expiring&#123; clock: clock, cache: make(map[interface&#123;&#125;]entry), &#125;&#125;// Expiring is a map whose entries expire after a per-entry timeout.// Expiring 实现一个可以设置过期时间的的缓存type Expiring struct &#123; // AllowExpiredGet causes the expiration check to be skipped on Get. // It should only be used when a key always corresponds to the exact same value. // Thus when this field is true, expired keys are considered valid // until the next call to Set (which causes the GC to run). // It may not be changed concurrently with calls to Get. // 如果AllowExpiredGet为true, Get操作跳过过期检查 AllowExpiredGet bool clock clock.Clock // mu protects the below fields mu sync.RWMutex // cache is the internal map that backs the cache. cache map[interface&#123;&#125;]entry // generation is used as a cheap resource version for cache entries. Cleanups // are scheduled with a key and generation. When the cleanup runs, it first // compares its generation with the current generation of the entry. It // deletes the entry iff the generation matches. This prevents cleanups // scheduled for earlier versions of an entry from deleting later versions of // an entry when Set() is called multiple times with the same key. // // The integer value of the generation of an entry is meaningless. // 当进行多次Set操作时,上次Set 操作中的gc可能还没有完成 // 如果generation 与entry中的generation不一致,则说明值是新值 // 则会跳过删除操作 generation uint64 heap expiringHeap&#125;type entry struct &#123; val interface&#123;&#125; expiry time.Time generation uint64&#125;// Get looks up an entry in the cache.// Get 根据key从map中获取对应的值func (c *Expiring) Get(key interface&#123;&#125;) (val interface&#123;&#125;, ok bool) &#123; c.mu.RLock() defer c.mu.RUnlock() e, ok := c.cache[key] if !ok &#123; return nil, false &#125; // 如果不是跳过过期检查且key已经过期，返回nil, false if !c.AllowExpiredGet &amp;&amp; !c.clock.Now().Before(e.expiry) &#123; return nil, false &#125; return e.val, true&#125;// Set sets a key/value/expiry entry in the map, overwriting any previous entry// with the same key. The entry expires at the given expiry time, but its TTL// may be lengthened or shortened by additional calls to Set(). Garbage// collection of expired entries occurs during calls to Set(), however calls to// Get() will not return expired entries that have not yet been garbage// collected.// Set 设置值func (c *Expiring) Set(key interface&#123;&#125;, val interface&#123;&#125;, ttl time.Duration) &#123; now := c.clock.Now() expiry := now.Add(ttl) c.mu.Lock() defer c.mu.Unlock() // 增加generation c.generation++ c.cache[key] = entry&#123; val: val, expiry: expiry, generation: c.generation, &#125; // Run GC inline before pushing the new entry. // 进行gc操作 // 会一直进行gc清除操作直到队列长度等于0或者不存在过期的数据 c.gc(now) // 插入到优先队列中 heap.Push(&amp;c.heap, &amp;expiringHeapEntry&#123; key: key, expiry: expiry, generation: c.generation, &#125;)&#125;// Delete deletes an entry in the map.func (c *Expiring) Delete(key interface&#123;&#125;) &#123; c.mu.Lock() defer c.mu.Unlock() c.del(key, 0)&#125;// del deletes the entry for the given key. The generation argument is the// generation of the entry that should be deleted. If the generation has been// changed (e.g. if a set has occurred on an existing element but the old// cleanup still runs), this is a noop. If the generation argument is 0, the// entry's generation is ignored and the entry is deleted.//// del must be called under the write lock.func (c *Expiring) del(key interface&#123;&#125;, generation uint64) &#123; e, ok := c.cache[key] if !ok &#123; return &#125; // 这里会对generation进行比较，如果不相等则直接返回 if generation != 0 &amp;&amp; generation != e.generation &#123; return &#125; delete(c.cache, key)&#125;// Len returns the number of items in the cache.func (c *Expiring) Len() int &#123; c.mu.RLock() defer c.mu.RUnlock() return len(c.cache)&#125;func (c *Expiring) gc(now time.Time) &#123; for &#123; // Return from gc if the heap is empty or the next element is not yet // expired. // // heap[0] is a peek at the next element in the heap, which is not obvious // from looking at the (*expiringHeap).Pop() implementation below. // heap.Pop() swaps the first entry with the last entry of the heap, then // calls (*expiringHeap).Pop() which returns the last element. // 如果heap长度为0 且第一个key没有过期,则直接返回 if len(c.heap) == 0 || now.Before(c.heap[0].expiry) &#123; return &#125; // 从堆中pop, 然后删除map中的数据 cleanup := heap.Pop(&amp;c.heap).(*expiringHeapEntry) c.del(cleanup.key, cleanup.generation) &#125;&#125;type expiringHeapEntry struct &#123; key interface&#123;&#125; expiry time.Time generation uint64&#125;// expiringHeap is a min-heap ordered by expiration time of its entries. The// expiring cache uses this as a priority queue to efficiently organize entries// which will be garbage collected once they expire.// expiringHeap 使用的是最小堆，根据过期时间进行排序// expiring cache使用最小堆作为一个优先队列，实现快速的过期删除type expiringHeap []*expiringHeapEntryvar _ heap.Interface = &amp;expiringHeap&#123;&#125;func (cq expiringHeap) Len() int &#123; return len(cq)&#125;// Less 较小的时间排在前面func (cq expiringHeap) Less(i, j int) bool &#123; return cq[i].expiry.Before(cq[j].expiry)&#125;func (cq expiringHeap) Swap(i, j int) &#123; cq[i], cq[j] = cq[j], cq[i]&#125;func (cq *expiringHeap) Push(c interface&#123;&#125;) &#123; *cq = append(*cq, c.(*expiringHeapEntry))&#125;func (cq *expiringHeap) Pop() interface&#123;&#125; &#123; c := (*cq)[cq.Len()-1] *cq = (*cq)[:cq.Len()-1] return c&#125; REF: expiring.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[asymmetric cryptography]]></title>
    <url>%2F2023%2F08%2F06%2Fasymmetric-cryptography%2F</url>
    <content type="text"><![CDATA[公钥密码学，也称为非对称密码学，是使用相关密钥对的密码系统领域。每个密钥对由公钥和相应的私钥组成。密钥对是通过基于数学问题的单向函数的加密算法生成的。公钥密码学的安全性取决于保持私钥的机密性；公钥可以公开分发而不会危及安全性。 密钥对的生成非对称密钥对生成通常使用一个不可预测的(大且随机的)数字并通过非对称密钥算法来生成密钥对。 可以通过openssl生成相应的证书，具体命令可参考 公钥加密公钥加密是使用公钥对消息进行加密，使用公钥进行加密的信息只能通过对应的密钥才能进行解密。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package mainimport ( "crypto/rand" "crypto/rsa" "crypto/x509" "encoding/pem" "fmt" "io/ioutil" "os")func main() &#123; // 读取公钥 pubPEMData, err := ioutil.ReadFile("server.crt") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; block, _ := pem.Decode(pubPEMData) if block == nil &#123; fmt.Println("failed to parse certificate PEM") os.Exit(1) &#125; pub, err := x509.ParseCertificate(block.Bytes) if err != nil &#123; fmt.Println("failed to parse certificate: " + err.Error()) os.Exit(1) &#125; rsaPub, ok := pub.PublicKey.(*rsa.PublicKey) if !ok &#123; fmt.Println("not RSA public key") os.Exit(1) &#125; // 读取私钥 privPEMData, err := ioutil.ReadFile("server.key") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; block, _ = pem.Decode(privPEMData) if block == nil &#123; fmt.Println("failed to parse key PEM") os.Exit(1) &#125; priv, err := x509.ParsePKCS8PrivateKey(block.Bytes) if err != nil &#123; fmt.Println("failed to parse key: " + err.Error()) os.Exit(1) &#125; // 加密字符串 message := []byte("Hello, World!") encryptedMessage, err := rsa.EncryptPKCS1v15(rand.Reader, rsaPub, message) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; // 解密字符串 decryptedMessage, err := rsa.DecryptPKCS1v15(rand.Reader, priv.(*rsa.PrivateKey), encryptedMessage) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; fmt.Println(string(decryptedMessage))&#125; 数字签名数字签名是一种使用发送者的私钥对消息进行签名的方法，并可以由任何拥有发送者公钥访问权限的人进行验证。该验证证明发送者有私钥的访问权限。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package mainimport ( "crypto" "crypto/rand" "crypto/rsa" "crypto/sha256" "crypto/x509" "encoding/pem" "fmt" "io/ioutil" "os")func main() &#123; message := "Hello, world!" // 读取私钥 privPEMData, err := ioutil.ReadFile("server.key") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; block, _ := pem.Decode(privPEMData) if block == nil &#123; fmt.Println("failed to parse key PEM") os.Exit(1) &#125; priv, err := x509.ParsePKCS8PrivateKey(block.Bytes) if err != nil &#123; fmt.Println("failed to parse key: " + err.Error()) os.Exit(1) &#125; // 对消息进行hash hash := sha256.Sum256([]byte(message)) // 通过密钥对hash进行签名 signature, err := rsa.SignPKCS1v15(rand.Reader, priv.(*rsa.PrivateKey), crypto.SHA256, hash[:]) if err != nil &#123; fmt.Println("failed to sign message: " + err.Error()) os.Exit(1) &#125; // 读取公钥 pubPEMData, err := ioutil.ReadFile("server.crt") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; block, _ = pem.Decode(pubPEMData) if block == nil &#123; fmt.Println("failed to parse certificate PEM") os.Exit(1) &#125; pub, err := x509.ParseCertificate(block.Bytes) if err != nil &#123; fmt.Println("failed to parse certificate: " + err.Error()) os.Exit(1) &#125; rsaPub, ok := pub.PublicKey.(*rsa.PublicKey) if !ok &#123; fmt.Println("not RSA public key") os.Exit(1) &#125; // 将签名和hash传给验证者，通过公钥进行验证 err = rsa.VerifyPKCS1v15(rsaPub, crypto.SHA256, hash[:], signature) // 如果未出错表明得到了验证 if err != nil &#123; fmt.Println("failed to verify signature: " + err.Error()) os.Exit(1) &#125; else &#123; fmt.Println("signature verified") &#125;&#125; Alice使用自己的密钥对sha256算法hash后的HelloBob!进行签名，然后Bob使用Alice的公钥对签名进行验证。 REF: https://en.wikipedia.org/wiki/Public-key_cryptography https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5551094/]]></content>
      <tags>
        <tag>cryptography</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s webhook生成ssl证书]]></title>
    <url>%2F2023%2F07%2F30%2Fk8s-webhook%E7%94%9F%E6%88%90ssl%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[当我们使用k8s的webhook机制时，webhook服务必须是https服务。如果不是https服务 ，则会报如下错误。http: server gave HTTP response to HTTPS client 记录下证书生成的步骤： 1234567891011121314151617181920212223242526272829openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr -config san.cnf###openssl req 命令用于为服务器生成一个新的证书签名请求 (CSR)。该命令中指定的选项如下：-new：该选项指定要生成一个新的 CSR。-newkey rsa:2048：该选项指定要生成一个新的 RSA 密钥对，密钥长度为 2048 位。-nodes：该选项指定私钥不应该使用密码加密。-keyout server.key：该选项指定私钥应该写入的文件名。-out server.csr：该选项指定 CSR 应该写入的文件名。-config san.cnf：该选项指定生成 CSR 时要使用的配置文件。在本例中，配置文件是 san.cnf，其中包含有关证书主体的信息和应包含在证书中的任何主题备用名称 (SANs)。# ca.crt, ca.key 为k8s集群中的文件， 在/etc/kubenetes/pki目录下openssl x509 -req -days 365 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -extensions v3_req -extfile san.cnf###openssl x509 命令用于将证书签名请求 (CSR) 签名为一个 SSL/TLS 证书。该命令中指定的选项如下：-req：该选项指定输入文件是一个证书请求。-days 365：该选项指定证书的有效期限，以天为单位。在本例中，证书的有效期限为 365 天。-in server.csr：该选项指定输入文件是一个证书请求，该请求应该签名为一个 SSL/TLS 证书。在本例中，输入文件是 server.csr。-CA ca.crt：该选项指定用于签名证书的 CA 证书。在本例中，CA 证书是 ca.crt。-CAkey ca.key：该选项指定用于签名证书的 CA 私钥。在本例中，CA 私钥是 ca.key。-CAcreateserial：该选项指定在签名证书时创建一个新的序列号文件。在本例中，序列号文件将被创建并用于签名证书。-out server.crt：该选项指定输出文件是签名后的 SSL/TLS 证书。在本例中，输出文件是 server.crt。-extensions v3_req：该选项指定证书应包含的证书扩展。在本例中，证书扩展是 v3_req。-extfile san.cnf：该选项指定证书扩展应该包含的信息。在本例中，证书扩展信息包含在 san.cnf 文件中。 1234567891011121314151617181920# san.cnf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name]countryName = USstateOrProvinceName = CalifornialocalityName = San FranciscoorganizationName = Example CorporganizationalUnitName = IT DepartmentcommonName = example.com[v3_req]subjectAltName = @alt_names[alt_names]DNS.1 = example.comDNS.2 = www.example.comIP.1 = 192.0.2.1 san.cnf 文件是一个 OpenSSL 配置文件，用于指定要在 SSL/TLS 证书中包含的主题备用名称 (SANs)。在上面的示例中，[req] 段指定了一些请求相关的选项，例如不提示用户输入信息 (prompt = no)。[req_distinguished_name] 段指定了证书请求的主题信息，例如国家、州、城市和组织名。[v3_req] 段指定了证书扩展信息，其中 subjectAltName 指定了一个名为 alt_names 的部分，该部分列出了所有的 SANs。在 [alt_names] 段中，DNS.1 指定了一个 DNS 名称 example.com，IP.1 指定了一个 IP 地址 192.168.1.1。 在实际使用中，可以根据需要添加、修改或删除 san.cnf 文件中的 SANs，以便生成适合特定环境的证书请求。 12345// 证书生成openssl genrsa -out ca.key 2048openssl req -new -x509 -days 3650 -key ca.key -out ca.crt -subj "/CN=hysyeah CA/O=hysyeah/C=CN"openssl req -new -newkey rsa:2048 -nodes -keyout server.key -out server.csr -config san.cnfopenssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -extensions v3_req -extfile san.cnf]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之storageversion]]></title>
    <url>%2F2023%2F07%2F30%2Fkube-controller%E4%B9%8Bstorageversion%2F</url>
    <content type="text"><![CDATA[Lease, storageVersionLease(租约)租约提供了一种机制来锁定共享资源并协调集合成员之间的活动，Kubernetes 也使用 Lease 确保在任何给定时间某个组件只有一个实例在运行。 这在高可用配置中由 kube-controller-manager 和 kube-scheduler 等控制平面组件进行使用， 这些组件只应有一个实例激活运行，而其他实例待机。 StorageVersion 是一种内部版本资源，记录了 APIServerID和其所支持的对象版本。 当Lease被删除时，说明API Server实例发生了变化，所以对象支持的版本也有可能发生变化，所以需要对storageVersion作相应的处理。 所以storageversiongc的作用就是当API Server发生时更新storageversion中记录API Server支持的版本。当storageversion发生变化时，也会查询lease信息从而更新storageversion 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// staging/src/k8s.io/api/apiserverinternal/v1alpha1/types.gotype StorageVersion struct &#123; metav1.TypeMeta `json:",inline"` // The name is &lt;group&gt;.&lt;resource&gt;. metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"` // Spec is an empty spec. It is here to comply with Kubernetes API style. Spec StorageVersionSpec `json:"spec" protobuf:"bytes,2,opt,name=spec"` // API server instances report the version they can decode and the version they // encode objects to when persisting objects in the backend. Status StorageVersionStatus `json:"status" protobuf:"bytes,3,opt,name=status"`&#125;type StorageVersionSpec struct&#123;&#125;// API server instances report the versions they can decode and the version they// encode objects to when persisting objects in the backend.type StorageVersionStatus struct &#123; // The reported versions per API server instance. // +optional // +listType=map // +listMapKey=apiServerID StorageVersions []ServerStorageVersion `json:"storageVersions,omitempty" protobuf:"bytes,1,opt,name=storageVersions"` // If all API server instances agree on the same encoding storage version, // then this field is set to that version. Otherwise this field is left empty. // API servers should finish updating its storageVersionStatus entry before // serving write operations, so that this field will be in sync with the reality. // +optional CommonEncodingVersion *string `json:"commonEncodingVersion,omitempty" protobuf:"bytes,2,opt,name=commonEncodingVersion"`// The latest available observations of the storageVersion's state. // +optional // +listType=map // +listMapKey=type Conditions []StorageVersionCondition `json:"conditions,omitempty" protobuf:"bytes,3,opt,name=conditions"`&#125;// An API server instance reports the version it can decode and the version it// encodes objects to when persisting objects in the backend.// ServerStorageVersion 记录了API server实例可以decode/encode的版本type ServerStorageVersion struct &#123; // The ID of the reporting API server. APIServerID string `json:"apiServerID,omitempty" protobuf:"bytes,1,opt,name=apiServerID"` // 当对象进行持久化时API Server采用EncodingVersion对对象进行编码 EncodingVersion string `json:"encodingVersion,omitempty" protobuf:"bytes,2,opt,name=encodingVersion"` // API server可以解码的版本， encodingVersion必须在decodableVersions中 // +listType=set DecodableVersions []string `json:"decodableVersions,omitempty" protobuf:"bytes,3,opt,name=decodableVersions"` // API Serever可以处理的version,DecodableVersions必须包含所有的ServedVersions // +listType=set ServedVersions []string `json:"servedVersions,omitempty" protobuf:"bytes,4,opt,name=servedVersions"`&#125; 1234567891011121314151617181920// staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go// 启动时会创建对应的storageversionfunc (c completedConfig) NewWithDelegate(delegationTarget genericapiserver.DelegationTarget) (*APIAggregator, error) &#123;... // Technically an apiserver only needs to update storage version once during bootstrap. // Reconcile StorageVersion objects every 10 minutes will help in the case that the // StorageVersion objects get accidentally modified/deleted by a different agent. In that // case, the reconciliation ensures future storage migration still works. If nothing gets // changed, the reconciliation update is a noop and gets short-circuited by the apiserver, // therefore won't change the resource version and trigger storage migration.go wait.PollImmediateUntil(10*time.Minute, func() (bool, error) &#123; // All apiservers (aggregator-apiserver, kube-apiserver, apiextensions-apiserver) // share the same generic apiserver config. The same StorageVersion manager is used // to register all built-in resources when the generic apiservers install APIs. s.GenericAPIServer.StorageVersionManager.UpdateStorageVersions(hookContext.LoopbackClientConfig, s.GenericAPIServer.APIServerID, c.GenericConfig.MergedResourceConfig) return false, nil&#125;, hookContext.StopCh)...&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205// pkg/controller/storageversiongc/gc_controller.go// 可以看出Controller会监听两种资源对象,lease, storageversiontype Controller struct &#123; kubeclientset kubernetes.Interface leaseLister coordlisters.LeaseLister leasesSynced cache.InformerSynced storageVersionSynced cache.InformerSynced leaseQueue workqueue.RateLimitingInterface storageVersionQueue workqueue. RateLimitingInterface&#125;// NewStorageVersionGC creates a new Controller.func NewStorageVersionGC(ctx context.Context, clientset kubernetes.Interface, leaseInformer coordinformers.LeaseInformer, storageVersionInformer apiserverinternalinformers.StorageVersionInformer) *Controller &#123;c := &amp;Controller&#123; kubeclientset: clientset, leaseLister: leaseInformer.Lister(), leasesSynced: leaseInformer.Informer().HasSynced, storageVersionSynced: storageVersionInformer.Informer().HasSynced, leaseQueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "storage_version_garbage_collector_leases"), storageVersionQueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "storage_version_garbage_collector_storageversions"),&#125; logger := klog.FromContext(ctx) leaseInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; // 这里只监听lease删除的事件 // lease删除了说明API Server发生了变化，这很好理解 // 但为什么不监听Create事件呢？ // 因为一个实例在启动的过程中会创建对应StorageVersion DeleteFunc: func(obj interface&#123;&#125;) &#123; c.onDeleteLease(logger, obj) &#125;, &#125;) // use the default resync period from the informer storageVersionInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; c.onAddStorageVersion(logger, obj) &#125;, UpdateFunc: func(old, newObj interface&#123;&#125;) &#123; c.onUpdateStorageVersion(logger, old, newObj) &#125;, &#125;)return c&#125;// Run starts one worker.func (c *Controller) Run(ctx context.Context) &#123; logger := klog.FromContext(ctx) defer utilruntime.HandleCrash() defer c.leaseQueue.ShutDown() defer c.storageVersionQueue.ShutDown() defer logger.Info("Shutting down storage version garbage collector") logger.Info("Starting storage version garbage collector") if !cache.WaitForCacheSync(ctx.Done(), c.leasesSynced, c.storageVersionSynced) &#123; utilruntime.HandleError(fmt.Errorf("timed out waiting for caches to sync")) return&#125;// Identity lease deletion and storageversion update don't happen too often. Start one// worker for each of them.// runLeaseWorker handles legit identity lease deletion, while runStorageVersionWorker// handles storageversion creation/update with non-existing id. The latter should rarely// happen. It's okay for the two workers to conflict on update.go wait.UntilWithContext(ctx, c.runLeaseWorker, time.Second)go wait.UntilWithContext(ctx, c.runStorageVersionWorker, time.Second) &lt;-ctx.Done()&#125;func (c *Controller) runLeaseWorker(ctx context.Context) &#123; for c.processNextLease(ctx) &#123; &#125;&#125;func (c *Controller) processNextLease(ctx context.Context) bool &#123;key, quit := c.leaseQueue.Get() if quit &#123; return false &#125; defer c.leaseQueue.Done(key) err := c.processDeletedLease(ctx, key.(string)) if err == nil &#123; c.leaseQueue.Forget(key) return true &#125; utilruntime.HandleError(fmt.Errorf("lease %v failed with: %v", key, err)) c.leaseQueue.AddRateLimited(key) return true&#125;func (c *Controller) runStorageVersionWorker(ctx context.Context) &#123; for c.processNextStorageVersion(ctx) &#123; &#125;&#125;func (c *Controller) processNextStorageVersion(ctx context.Context) bool &#123; key, quit := c.storageVersionQueue.Get() if quit &#123; return false &#125; defer c.storageVersionQueue.Done(key) err := c.syncStorageVersion(ctx, key.(string)) if err == nil &#123; c.storageVersionQueue.Forget(key) return true &#125; utilruntime.HandleError(fmt.Errorf("storage version %v failed with: %v", key, err)) c.storageVersionQueue.AddRateLimited(key) return true&#125;// 处理删除lease时的逻辑func (c *Controller) processDeletedLease(ctx context.Context, name string) error &#123; _, err := c.kubeclientset.CoordinationV1().Leases(metav1.NamespaceSystem).Get(ctx, name, metav1.GetOptions&#123;&#125;) // lease没有被删除，不需要做任何事 if err == nil &#123; return nil &#125; // 除IsNotFound的错误外，直接return if !apierrors.IsNotFound(err) &#123; return err &#125; // 获取所有的storageversion对象，只有当删除lease时才会触发，所以调用频率不高 storageVersionList, err := c.kubeclientset.InternalV1alpha1().StorageVersions().List(ctx, metav1.ListOptions&#123;&#125;) if err != nil &#123; return err &#125; var errors []error for _, sv := range storageVersionList.Items &#123; var serverStorageVersions []apiserverinternalv1alpha1. ServerStorageVersion hasStaleRecord := false for _, ssv := range sv.Status.StorageVersions &#123; // 如果ssv.APIServerID == name, 则应该被删除 // 因为此时APIServerID已经不是原来的实例了,则原有实例支持的version需要更新 if ssv.APIServerID == name &#123; hasStaleRecord = true continue &#125; serverStorageVersions = append(serverStorageVersions, ssv) &#125; if !hasStaleRecord &#123; continue &#125; if err := c.updateOrDeleteStorageVersion(ctx, &amp;sv, serverStorageVersions); err != nil &#123; errors = append(errors, err) &#125;&#125;return utilerrors.NewAggregate(errors)&#125;func (c *Controller) syncStorageVersion(ctx context.Context, name string) error &#123; sv, err := c.kubeclientset.InternalV1alpha1().StorageVersions().Get(ctx, name, metav1.GetOptions&#123;&#125;) if apierrors.IsNotFound(err) &#123; // The problematic storage version that was added/updated recently is gone. // Nothing we need to do here. return nil &#125; if err != nil &#123; return err &#125; hasInvalidID := false var serverStorageVersions []apiserverinternalv1alpha1. ServerStorageVersion // 遍历所有的StorageVersions for _, v := range sv.Status.StorageVersions &#123; // 根据APIServerID获取对应的lease lease, err := c.kubeclientset.CoordinationV1().Leases(metav1. NamespaceSystem).Get(ctx, v.APIServerID, metav1.GetOptions&#123;&#125;) // 如找不到对应的APIServer则需要更新serverStorageVersions if err != nil || lease == nil || lease.Labels == nil || lease.Labels[controlplane.IdentityLeaseComponentLabelKey] != controlplane.KubeAPIServer &#123; // We cannot find a corresponding identity lease from apiserver as well. // We need to clean up this storage version. hasInvalidID = true continue &#125; serverStorageVersions = append(serverStorageVersions, v) &#125; if !hasInvalidID &#123; return nil &#125; return c.updateOrDeleteStorageVersion(ctx, sv, serverStorageVersions)&#125;func (c *Controller) updateOrDeleteStorageVersion(ctx context.Context, sv *apiserverinternalv1alpha1.StorageVersion, serverStorageVersions []apiserverinternalv1alpha1.ServerStorageVersion) error &#123; if len(serverStorageVersions) == 0 &#123; return c.kubeclientset.InternalV1alpha1().StorageVersions().Delete( ctx, sv.Name, metav1.DeleteOptions&#123;&#125;)&#125; sv.Status.StorageVersions = serverStorageVersions storageversion.SetCommonEncodingVersion(sv) _, err := c.kubeclientset.InternalV1alpha1().StorageVersions().UpdateStatus(ctx, sv, metav1.UpdateOptions&#123;&#125;) return err&#125; REF: Lease staging/src/k8s.io/api/apiserverinternal/v1alpha1/types.go staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go pkg/controller/storageversiongc/gc_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之statefulset]]></title>
    <url>%2F2023%2F07%2F15%2Fkube-controller%E4%B9%8Bstatefulset%2F</url>
    <content type="text"><![CDATA[StatefulSetStatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 如果希望使用存储卷为工作负载提供持久存储，可以使用 StatefulSet 作为解决方案的一部分。 尽管 StatefulSet 中的单个 Pod 仍可能出现故障， 但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易。 使用 StatefulSetStatefulSet 对于需要满足以下一个或多个需求的应用程序很有价值： 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和扩缩。 有序的、自动的滚动更新。在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。 如果应用程序不需要任何稳定的标识符或有序的部署、删除或扩缩， 则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于你的无状态应用部署需要。 限制给定 Pod 的存储必须由 PersistentVolume Provisioner 基于所请求的 storage class 来制备，或者由管理员预先制备。删除或者扩缩 StatefulSet 并不会删除它关联的存储卷。 这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。StatefulSet 当前需要无头服务来负责 Pod 的网络标识。你需要负责创建此服务。当删除一个 StatefulSet 时，该 StatefulSet 不提供任何终止 Pod 的保证。 为了实现 StatefulSet 中的 Pod 可以有序且体面地终止，可以在删除之前将 StatefulSet 缩容到 0。在默认 Pod 管理策略(OrderedReady) 时使用滚动更新， 可能进入需要人工干预才能修复的损坏状态。 以上来自官方文档 源码分析1234567891011121314151617181920212223242526// pkg/controller/statefulset/stateful_set.gotype StatefulSetController struct &#123; // client interface kubeClient clientset.Interface // control returns an interface capable of syncing a stateful set. // Abstracted out for testing. control StatefulSetControlInterface // podControl is used for patching pods. podControl controller.PodControlInterface // podLister is able to list/get pods from a shared informer's store podLister corelisters.PodLister // podListerSynced returns true if the pod shared informer has synced at least once podListerSynced cache.InformerSynced // setLister is able to list/get stateful sets from a shared informer's store setLister appslisters.StatefulSetLister // setListerSynced returns true if the stateful set shared informer has synced at least once setListerSynced cache.InformerSynced // pvcListerSynced returns true if the pvc shared informer has synced at least once pvcListerSynced cache.InformerSynced // revListerSynced returns true if the rev shared informer has synced at least once revListerSynced cache.InformerSynced // StatefulSets that need to be synced. queue workqueue.RateLimitingInterface // eventBroadcaster is the core of event processing pipeline. eventBroadcaster record.EventBroadcaster&#125; 123456789101112131415161718192021222324252627282930313233// pkg/controller/statefulset/stateful_set_control.go// 定义了StatefulSet操作的一些方法type StatefulSetControlInterface interface &#123; // UpdateStatefulSet implements the control logic for Pod creation, update, and deletion, and // persistent volume creation, update, and deletion. // If an implementation returns a non-nil error, the invocation will be retried using a rate-limited strategy. // Implementors should sink any errors that they do not wish to trigger a retry, and they may feel free to // exit exceptionally at any point provided they wish the update to be re-run at a later point in time. // 实现Pod和pv的创建，更新，删除操作 UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error) // ListRevisions returns a array of the ControllerRevisions that represent the revisions of set. If the returned // error is nil, the returns slice of ControllerRevisions is valid. ListRevisions(set *apps.StatefulSet) ([]*apps.ControllerRevision, error) // AdoptOrphanRevisions adopts any orphaned ControllerRevisions that match set's Selector. If all adoptions are // successful the returned error is nil. AdoptOrphanRevisions(set *apps.StatefulSet, revisions []*apps.ControllerRevision) error&#125;// ControllerRevision 实现了一个不可变的状态数据快照。客户端负责序列化和反序列化包含其内部状态的对象// 一旦成功创建了 ControllerRevision，就不能对其进行更新,但是可以被删除。API 服务器会拒绝所有试图修改 Data 字段的请求// 主要被 DaemonSet 和 StatefulSet 控制器用于更新和回滚// vendor/k8s.io/api/apps/v1/types.gotype ControllerRevision struct &#123; metav1.TypeMeta `json:",inline"` metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"` // Data is the serialized representation of the state. Data runtime.RawExtension `json:"data,omitempty" protobuf:"bytes,2,opt,name=data"` // Revision indicates the revision of the state represented by Data. Revision int64 `json:"revision" protobuf:"varint,3,opt,name=revision"`&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308// pkg/controller/statefulset/stateful_set.go// 新建一个StatefulSet Controller// 都是一样的操作，设置EventHandler,初始化队列等等func NewStatefulSetController( ctx context.Context, podInformer coreinformers.PodInformer, setInformer appsinformers.StatefulSetInformer, pvcInformer coreinformers.PersistentVolumeClaimInformer, revInformer appsinformers.ControllerRevisionInformer, kubeClient clientset.Interface,) *StatefulSetController &#123; logger := klog.FromContext(ctx) eventBroadcaster := record.NewBroadcaster() recorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "statefulset-controller"&#125;) ssc := &amp;StatefulSetController&#123; kubeClient: kubeClient, control: NewDefaultStatefulSetControl( NewStatefulPodControl( kubeClient, podInformer.Lister(), pvcInformer.Lister(), recorder), NewRealStatefulSetStatusUpdater(kubeClient, setInformer.Lister()), history.NewHistory(kubeClient, revInformer.Lister()), recorder, ), pvcListerSynced: pvcInformer.Informer().HasSynced, revListerSynced: revInformer.Informer().HasSynced, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "statefulset"), podControl: controller.RealPodControl&#123;KubeClient: kubeClient, Recorder: recorder&#125;, eventBroadcaster: eventBroadcaster, &#125; podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; // lookup the statefulset and enqueue AddFunc: func(obj interface&#123;&#125;) &#123; ssc.addPod(logger, obj) &#125;, // lookup current and old statefulset if labels changed UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; ssc.updatePod(logger, oldObj, newObj) &#125;, // lookup statefulset accounting for deletion tombstones DeleteFunc: func(obj interface&#123;&#125;) &#123; ssc.deletePod(logger, obj) &#125;, &#125;) ssc.podLister = podInformer.Lister() ssc.podListerSynced = podInformer.Informer().HasSynced setInformer.Informer().AddEventHandler( cache.ResourceEventHandlerFuncs&#123; AddFunc: ssc.enqueueStatefulSet, UpdateFunc: func(old, cur interface&#123;&#125;) &#123; oldPS := old.(*apps.StatefulSet) curPS := cur.(*apps.StatefulSet) if oldPS.Status.Replicas != curPS.Status.Replicas &#123; logger.V(4).Info("Observed updated replica count for StatefulSet", "statefulSet", klog.KObj(curPS), "oldReplicas", oldPS.Status.Replicas, "newReplicas", curPS.Status.Replicas) &#125; ssc.enqueueStatefulSet(cur) &#125;, DeleteFunc: ssc.enqueueStatefulSet, &#125;, ) ssc.setLister = setInformer.Lister() ssc.setListerSynced = setInformer.Informer().HasSynced // TODO: Watch volumes return ssc&#125;// Run runs the statefulset controller.func (ssc *StatefulSetController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() // Start events processing pipeline. ssc.eventBroadcaster.StartStructuredLogging(0) ssc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: ssc.kubeClient.CoreV1().Events("")&#125;) defer ssc.eventBroadcaster.Shutdown() defer ssc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting stateful set controller") defer logger.Info("Shutting down statefulset controller") if !cache.WaitForNamedCacheSync("stateful set", ctx.Done(), ssc.podListerSynced, ssc.setListerSynced, ssc.pvcListerSynced, ssc.revListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, ssc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;// 入口函数func (ssc *StatefulSetController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() // Start events processing pipeline. ssc.eventBroadcaster.StartStructuredLogging(0) ssc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: ssc.kubeClient.CoreV1().Events("")&#125;) defer ssc.eventBroadcaster.Shutdown() defer ssc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting stateful set controller") defer logger.Info("Shutting down statefulset controller") if !cache.WaitForNamedCacheSync("stateful set", ctx.Done(), ssc.podListerSynced, ssc.setListerSynced, ssc.pvcListerSynced, ssc.revListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, ssc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (ssc *StatefulSetController) worker(ctx context.Context) &#123; for ssc.processNextWorkItem(ctx) &#123; &#125;&#125;func (ssc *StatefulSetController) processNextWorkItem(ctx context.Context) bool &#123; key, quit := ssc.queue.Get() if quit &#123; return false &#125; defer ssc.queue.Done(key) // 调谐函数ssc.sync,所以我们主要的分析集中在sync函数 if err := ssc.sync(ctx, key.(string)); err != nil &#123; utilruntime.HandleError(fmt.Errorf("error syncing StatefulSet %v, requeuing: %v", key.(string), err)) ssc.queue.AddRateLimited(key) &#125; else &#123; ssc.queue.Forget(key) &#125; return true&#125;// StatefulSet的调谐函数func (ssc *StatefulSetController) sync(ctx context.Context, key string) error &#123; startTime := time.Now() logger := klog.FromContext(ctx) defer func() &#123; logger.V(4).Info("Finished syncing statefulset", "key", key, "time", time.Since(startTime)) &#125;() // 通过队列中的key取出命名空间和资源名称 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 获取StatefulSet set, err := ssc.setLister.StatefulSets(namespace).Get(name) // 如果已经删除了则直接返回 if errors.IsNotFound(err) &#123; logger.Info("StatefulSet has been deleted", "key", key) return nil &#125; // 其它错误 if err != nil &#123; utilruntime.HandleError(fmt.Errorf("unable to retrieve StatefulSet %v from store: %v", key, err)) return err &#125; // 获取selector selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("error converting StatefulSet %v selector: %v", key, err)) // This is a non-transient error, so don't retry. return nil &#125; // 收养孤儿ControllerRevisions if err := ssc.adoptOrphanRevisions(ctx, set); err != nil &#123; return err &#125; // 通过sts和selector获取对应的pod pods, err := ssc.getPodsForStatefulSet(ctx, set, selector) if err != nil &#123; return err &#125; // 逻辑操作 return ssc.syncStatefulSet(ctx, set, pods)&#125;// 收养孤儿ControllerRevisionsfunc (ssc *StatefulSetController) adoptOrphanRevisions(ctx context.Context, set *apps.StatefulSet) error &#123; // 获取sts的ControllerRevision列表 revisions, err := ssc.control.ListRevisions(set) if err != nil &#123; return err &#125; orphanRevisions := make([]*apps.ControllerRevision, 0) for i := range revisions &#123; // 获取OwnerReference if metav1.GetControllerOf(revisions[i]) == nil &#123; // OwnerReference为空 orphanRevisions = append(orphanRevisions, revisions[i]) &#125; &#125; // 存在孤儿ControllerRevisions if len(orphanRevisions) &gt; 0 &#123; // 判断是否能收养，再进行一次check canAdoptErr := ssc.canAdoptFunc(ctx, set)(ctx) if canAdoptErr != nil &#123; return fmt.Errorf("can't adopt ControllerRevisions: %v", canAdoptErr) &#125; return ssc.control.AdoptOrphanRevisions(set, orphanRevisions) &#125; return nil&#125;func (ssc *StatefulSetController) syncStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) error &#123; logger := klog.FromContext(ctx) logger.V(4).Info("Syncing StatefulSet with pods", "statefulSet", klog.KObj(set), "pods", len(pods)) var status *apps.StatefulSetStatus var err error status, err = ssc.control.UpdateStatefulSet(ctx, set, pods) if err != nil &#123; return err &#125; logger.V(4).Info("Successfully synced StatefulSet", "statefulSet", klog.KObj(set)) // One more sync to handle the clock skew. This is also helping in requeuing right after status update if set.Spec.MinReadySeconds &gt; 0 &amp;&amp; status != nil &amp;&amp; status.AvailableReplicas != *set.Spec.Replicas &#123; ssc.enqueueSSAfter(set, time.Duration(set.Spec.MinReadySeconds)*time.Second) &#125; return nil&#125;// sts调谐的核心逻辑，默认采用[monotonic update strategy],扩容按照一定的顺序进行，只有当所有pod都Ready了才会创建后面的Pod,// 缩容是如扩容是不一样的顺序(比如扩容是1-&gt;2-&gt;3,缩容则是3-&gt;2-&gt;1)// 如果采用的是[burst strategy]则没有这么多的限制，Pod将会被迅速创建和删除，并且没有特定的顺序。func (ssc *defaultStatefulSetControl) UpdateStatefulSet(ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod) (*apps.StatefulSetStatus, error) &#123; set = set.DeepCopy() // set is modified when a new revision is created in performUpdate. Make a copy now to avoid mutation errors. // 获取所有的Revisions revisions, err := ssc.ListRevisions(set) if err != nil &#123; return nil, err &#125; // 对revisions进行排序 history.SortControllerRevisions(revisions) currentRevision, updateRevision, status, err := ssc.performUpdate(ctx, set, pods, revisions) if err != nil &#123; return nil, utilerrors.NewAggregate([]error&#123;err, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)&#125;) &#125; // maintain the set's revision history limit // history保持一定的长度以避免占用过多的资源 return status, ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)&#125;func (ssc *defaultStatefulSetControl) performUpdate( ctx context.Context, set *apps.StatefulSet, pods []*v1.Pod, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, *apps.StatefulSetStatus, error) &#123; var currentStatus *apps.StatefulSetStatus logger := klog.FromContext(ctx) // get the current, and update revisions currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions) if err != nil &#123; return currentRevision, updateRevision, currentStatus, err &#125; // 执行更新逻辑并返回获取状态 currentStatus, err = ssc.updateStatefulSet(ctx, set, currentRevision, updateRevision, collisionCount, pods) if err != nil &amp;&amp; currentStatus == nil &#123; return currentRevision, updateRevision, nil, err &#125; // make sure to update the latest status even if there is an error with non-nil currentStatus statusErr := ssc.updateStatefulSetStatus(ctx, set, currentStatus) if statusErr == nil &#123; logger.V(4).Info("Updated status", "statefulSet", klog.KObj(set), "replicas", currentStatus.Replicas, "readyReplicas", currentStatus.ReadyReplicas, "currentReplicas", currentStatus.CurrentReplicas, "updatedReplicas", currentStatus.UpdatedReplicas) &#125; switch &#123; case err != nil &amp;&amp; statusErr != nil: klog.ErrorS(statusErr, "Could not update status", "statefulSet", klog.KObj(set)) return currentRevision, updateRevision, currentStatus, err case err != nil: return currentRevision, updateRevision, currentStatus, err case statusErr != nil: return currentRevision, updateRevision, currentStatus, statusErr &#125; logger.V(4).Info("StatefulSet revisions", "statefulSet", klog.KObj(set), "currentRevision", currentStatus.CurrentRevision, "updateRevision", currentStatus.UpdateRevision) return currentRevision, updateRevision, currentStatus, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356// pkg/controller/statefulset/stateful_set_control.go// StatefulSet控制器最核心逻辑。updateStatefulSet以特定的顺序creates,updates,deletes statefulset中的pod// 以达到预期的状态。// 如果更新策略为RollingUpdateStatefulSetStrategyType,sts中的所有pod必须在sts.Status.CurrentRevision中// 如果更新策略为OnDeleteStatefulSetStrategyType,目标状态对sts中的pod的revision没有任何意义// 如果更新策略为PartitionStatefulSetStrategyType所有pod ordinal低于UpdateStrategy.Partition.Ordinal必须存在// 于Status.CurrentRevision,其余的pod必须在Status.UpdateRevision.// 返回值：error=nil,说明StatefulSetStatus是有效的，更新必须记录。如果error!=nil,进行重试func (ssc *defaultStatefulSetControl) updateStatefulSet( ctx context.Context, set *apps.StatefulSet, currentRevision *apps.ControllerRevision, updateRevision *apps.ControllerRevision, collisionCount int32, pods []*v1.Pod) (*apps.StatefulSetStatus, error) &#123; logger := klog.FromContext(ctx) // 更新sts Revision并返回currentSet currentSet, err := ApplyRevision(set, currentRevision) if err != nil &#123; return nil, err &#125; // 更新sts Revision并返回updateSet updateSet, err := ApplyRevision(set, updateRevision) if err != nil &#123; return nil, err &#125; // 返回的状态，设置对应的值 status := apps.StatefulSetStatus&#123;&#125; status.ObservedGeneration = set.Generation status.CurrentRevision = currentRevision.Name status.UpdateRevision = updateRevision.Name status.CollisionCount = new(int32) *status.CollisionCount = collisionCount // 期望副本数 replicaCount := int(*set.Spec.Replicas) // slice that will contain all Pods such that getStartOrdinal(set) &lt;= getOrdinal(pod) &lt;= getEndOrdinal(set) // 记录在范围之内的pod replicas := make([]*v1.Pod, replicaCount) // slice that will contain all Pods such that getOrdinal(pod) &lt; getStartOrdinal(set) OR getOrdinal(pod) &gt; getEndOrdinal(set) // condemned记录的应该是要遗弃的pod condemned := make([]*v1.Pod, 0, len(pods)) unhealthy := 0 var firstUnhealthyPod *v1.Pod // First we partition pods into two lists valid replicas and condemned Pods // 遍历pods,分别记录到replicas和condemned for _, pod := range pods &#123; // 统计sts副本数 status.Replicas++ // count the number of running and ready replicas if isRunningAndReady(pod) &#123; // 统计Ready状态的副本数 status.ReadyReplicas++ // 统计Running和Available状态的副本数 if isRunningAndAvailable(pod, set.Spec.MinReadySeconds) &#123; status.AvailableReplicas++ &#125; &#125; // count the number of current and update replicas if isCreated(pod) &amp;&amp; !isTerminating(pod) &#123; if getPodRevision(pod) == currentRevision.Name &#123; status.CurrentReplicas++ &#125; if getPodRevision(pod) == updateRevision.Name &#123; status.UpdatedReplicas++ &#125; &#125; if podInOrdinalRange(pod, set) &#123; // if the ordinal of the pod is within the range of the current number of replicas, // insert it at the indirection of its ordinal // 将符合条件的pod添加到replicas列表中 replicas[getOrdinal(pod)-getStartOrdinal(set)] = pod &#125; else if getOrdinal(pod) &gt;= 0 &#123; // if the ordinal is valid, but not within the range add it to the condemned list condemned = append(condemned, pod) &#125; // If the ordinal could not be parsed (ord &lt; 0), ignore the Pod. &#125; // for any empty indices in the sequence [0,set.Spec.Replicas) create a new Pod at the correct revision // 为每个pod创建revision for ord := getStartOrdinal(set); ord &lt;= getEndOrdinal(set); ord++ &#123; replicaIdx := ord - getStartOrdinal(set) if replicas[replicaIdx] == nil &#123; replicas[replicaIdx] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord) &#125; &#125; // sort the condemned Pods by their ordinals // 对condemned pods根据ordinal排序 sort.Sort(ascendingOrdinal(condemned)) // find the first unhealthy Pod // 找到第一个unhealthy的pod,并统计unhealthy pod数量 for i := range replicas &#123; if !isHealthy(replicas[i]) &#123; unhealthy++ if firstUnhealthyPod == nil &#123; firstUnhealthyPod = replicas[i] &#125; &#125; &#125; for i := range condemned &#123; if !isHealthy(condemned[i]) &#123; unhealthy++ if firstUnhealthyPod == nil &#123; firstUnhealthyPod = condemned[i] &#125; &#125; &#125; if unhealthy &gt; 0 &#123; logger.V(4).Info("StatefulSet has unhealthy Pods", "statefulSet", klog.KObj(set), "unhealthyReplicas", unhealthy, "pod", klog.KObj(firstUnhealthyPod)) &#125; // If the StatefulSet is being deleted, don't do anything other than updating // status. // sts已经被删除，返回 if set.DeletionTimestamp != nil &#123; return &amp;status, nil &#125; monotonic := !allowsBurst(set) // Examine each replica with respect to its ordinal // 遍历replicas for i := range replicas &#123; // delete and recreate failed pods if isFailed(replicas[i]) &#123; ssc.recorder.Eventf(set, v1.EventTypeWarning, "RecreatingFailedPod", "StatefulSet %s/%s is recreating failed Pod %s", set.Namespace, set.Name, replicas[i].Name) // 删除pod if err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil &#123; return &amp;status, err &#125; if getPodRevision(replicas[i]) == currentRevision.Name &#123; status.CurrentReplicas-- &#125; if getPodRevision(replicas[i]) == updateRevision.Name &#123; status.UpdatedReplicas-- &#125; status.Replicas-- replicaOrd := i + getStartOrdinal(set) // 设置新的revision replicas[i] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, replicaOrd) &#125; // If we find a Pod that has not been created we create the Pod // 没有创建对应的pod,则创建 if !isCreated(replicas[i]) &#123; if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) &#123; if isStale, err := ssc.podControl.PodClaimIsStale(set, replicas[i]); err != nil &#123; return &amp;status, err &#125; else if isStale &#123; // If a pod has a stale PVC, no more work can be done this round. return &amp;status, err &#125; &#125; // 创建pod if err := ssc.podControl.CreateStatefulPod(ctx, set, replicas[i]); err != nil &#123; return &amp;status, err &#125; status.Replicas++ if getPodRevision(replicas[i]) == currentRevision.Name &#123; status.CurrentReplicas++ &#125; if getPodRevision(replicas[i]) == updateRevision.Name &#123; status.UpdatedReplicas++ &#125; // if the set does not allow bursting, return immediately if monotonic &#123; return &amp;status, nil &#125; // pod created, no more work possible for this round continue &#125; // 如果pod处于pending状态，会触发创建缺失PVC的动作 if isPending(replicas[i]) &#123; klog.V(4).Infof( "StatefulSet %s/%s is triggering PVC creation for pending Pod %s", set.Namespace, set.Name, replicas[i].Name) if err := ssc.podControl.createMissingPersistentVolumeClaims(ctx, set, replicas[i]); err != nil &#123; return &amp;status, err &#125; &#125; // If we find a Pod that is currently terminating, we must wait until graceful deletion // completes before we continue to make progress. // 如果一个pod处于terminating状态，我们必须等待pod优雅删除完成，才成继续往下走 if isTerminating(replicas[i]) &amp;&amp; monotonic &#123; logger.V(4).Info("StatefulSet is waiting for Pod to Terminate", "statefulSet", klog.KObj(set), "pod", klog.KObj(replicas[i])) return &amp;status, nil &#125; // If we have a Pod that has been created but is not running and ready we can not make progress. // We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its // ordinal, are Running and Ready. // 如果一个pod不是running和ready状态，我们可以退出这次调谐 if !isRunningAndReady(replicas[i]) &amp;&amp; monotonic &#123; logger.V(4).Info("StatefulSet is waiting for Pod to be Running and Ready", "statefulSet", klog.KObj(set), "pod", klog.KObj(replicas[i])) return &amp;status, nil &#125; // If we have a Pod that has been created but is not available we can not make progress. // We must ensure that all for each Pod, when we create it, all of its predecessors, with respect to its // ordinal, are Available. if !isRunningAndAvailable(replicas[i], set.Spec.MinReadySeconds) &amp;&amp; monotonic &#123; logger.V(4).Info("StatefulSet is waiting for Pod to be Available", "statefulSet", klog.KObj(set), "pod", klog.KObj(replicas[i])) return &amp;status, nil &#125; // Enforce the StatefulSet invariants retentionMatch := true if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) &#123; var err error retentionMatch, err = ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, replicas[i]) // An error is expected if the pod is not yet fully updated, and so return is treated as matching. if err != nil &#123; retentionMatch = true &#125; &#125; if identityMatches(set, replicas[i]) &amp;&amp; storageMatches(set, replicas[i]) &amp;&amp; retentionMatch &#123; continue &#125; // Make a deep copy so we don't mutate the shared cache replica := replicas[i].DeepCopy() if err := ssc.podControl.UpdateStatefulPod(ctx, updateSet, replica); err != nil &#123; return &amp;status, err &#125; &#125; if utilfeature.DefaultFeatureGate.Enabled(features.StatefulSetAutoDeletePVC) &#123; // Ensure ownerRefs are set correctly for the condemned pods. for i := range condemned &#123; if matchPolicy, err := ssc.podControl.ClaimsMatchRetentionPolicy(ctx, updateSet, condemned[i]); err != nil &#123; return &amp;status, err &#125; else if !matchPolicy &#123; if err := ssc.podControl.UpdatePodClaimForRetentionPolicy(ctx, updateSet, condemned[i]); err != nil &#123; return &amp;status, err &#125; &#125; &#125; &#125; // At this point, all of the current Replicas are Running, Ready and Available, we can consider termination. // We will wait for all predecessors to be Running and Ready prior to attempting a deletion. // We will terminate Pods in a monotonically decreasing order. // Note that we do not resurrect Pods in this interval. Also note that scaling will take precedence over // updates. for target := len(condemned) - 1; target &gt;= 0; target-- &#123; // wait for terminating pods to expire if isTerminating(condemned[target]) &#123; logger.V(4).Info("StatefulSet is waiting for Pod to Terminate prior to scale down", "statefulSet", klog.KObj(set), "pod", klog.KObj(condemned[target])) // block if we are in monotonic mode if monotonic &#123; return &amp;status, nil &#125; continue &#125; // if we are in monotonic mode and the condemned target is not the first unhealthy Pod block if !isRunningAndReady(condemned[target]) &amp;&amp; monotonic &amp;&amp; condemned[target] != firstUnhealthyPod &#123; logger.V(4).Info("StatefulSet is waiting for Pod to be Running and Ready prior to scale down", "statefulSet", klog.KObj(set), "pod", klog.KObj(firstUnhealthyPod)) return &amp;status, nil &#125; // if we are in monotonic mode and the condemned target is not the first unhealthy Pod, block. if !isRunningAndAvailable(condemned[target], set.Spec.MinReadySeconds) &amp;&amp; monotonic &amp;&amp; condemned[target] != firstUnhealthyPod &#123; logger.V(4).Info("StatefulSet is waiting for Pod to be Available prior to scale down", "statefulSet", klog.KObj(set), "pod", klog.KObj(firstUnhealthyPod)) return &amp;status, nil &#125; logger.V(2).Info("Pod of StatefulSet is terminating for scale down", "statefulSet", klog.KObj(set), "pod", klog.KObj(condemned[target])) if err := ssc.podControl.DeleteStatefulPod(set, condemned[target]); err != nil &#123; return &amp;status, err &#125; if getPodRevision(condemned[target]) == currentRevision.Name &#123; status.CurrentReplicas-- &#125; if getPodRevision(condemned[target]) == updateRevision.Name &#123; status.UpdatedReplicas-- &#125; if monotonic &#123; return &amp;status, nil &#125; &#125; // for the OnDelete strategy we short circuit. Pods will be updated when they are manually deleted. if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType &#123; return &amp;status, nil &#125; if utilfeature.DefaultFeatureGate.Enabled(features.MaxUnavailableStatefulSet) &#123; return updateStatefulSetAfterInvariantEstablished(ctx, ssc, set, replicas, updateRevision, status, ) &#125; // we compute the minimum ordinal of the target sequence for a destructive update based on the strategy. updateMin := 0 if set.Spec.UpdateStrategy.RollingUpdate != nil &#123; updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) &#125; // we terminate the Pod with the largest ordinal that does not match the update revision. for target := len(replicas) - 1; target &gt;= updateMin; target-- &#123; // delete the Pod if it is not already terminating and does not match the update revision. if getPodRevision(replicas[target]) != updateRevision.Name &amp;&amp; !isTerminating(replicas[target]) &#123; logger.V(2).Info("Pod of StatefulSet is terminating for update", "statefulSet", klog.KObj(set), "pod", klog.KObj(replicas[target])) if err := ssc.podControl.DeleteStatefulPod(set, replicas[target]); err != nil &#123; if !errors.IsNotFound(err) &#123; return &amp;status, err &#125; &#125; status.CurrentReplicas-- return &amp;status, err &#125; // wait for unhealthy Pods on update if !isHealthy(replicas[target]) &#123; logger.V(4).Info("StatefulSet is waiting for Pod to update", "statefulSet", klog.KObj(set), "pod", klog.KObj(replicas[target])) return &amp;status, nil &#125; &#125; return &amp;status, nil&#125; REF:1.pkg/controller/statefulset/stateful_set.go2.pkg/controller/statefulset/stateful_set_control.go vendor/k8s.io/api/apps/v1/types.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Terminating状态的pod是怎么被删除的]]></title>
    <url>%2F2023%2F06%2F27%2FTerminating%E7%8A%B6%E6%80%81%E7%9A%84pod%E6%98%AF%E6%80%8E%E4%B9%88%E8%A2%AB%E5%88%A0%E9%99%A4%E7%9A%84%2F</url>
    <content type="text"><![CDATA[一个Terminating状态(数据并没有直接从etcd删除，而是设置了DeletionTimestamp)的Pod是怎么删除的呢?我们知道删除一个资源对象是要调用APIServer接口从etcd中将数据删除。假如要调用接口那请求发起方又是哪个组件呢？ 创建一个Terminating状态的Podk apply -f b2.yaml,然后k delete pod b2你会得到一个处于Terminating状态的Pod1234567891011# b2.yamlapiVersion: v1kind: Podmetadata: name: b2 finalizers: - kubernetesspec: containers: - name: app image: hysyeah/my-curl:v1 删除Terminating状态的podk edit pod b2将finalizers移除，然后你会发现pod b2被立马删除了。你可以通过命令watch -n 1 k get pod或 watch -n 1 etcdctl get /registry/pods/default/b2 -w json来监听对应的资源是否删除。 关于etcdctl的使用可以查看 验证为了验证猜测，首先我把k8s集群中的kube-controller-manager给移除了。然后对Terminating状态的移除finalizers操作，发现pod被删除，可见这跟kube-controller-manager。然后猜测可能是kubelet发送的请求，首先在kubelet中并没有发现删除pod的代码，然后经过验证发现这与kubelet也没有关系。 最后观察k edit pod b2 --v=9看这条操作调用了什么接口。 源码分析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283// 当我们删除一个带有finalizer的pod时并不会马上将etcd中的数据删除// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.gofunc (e *Store) Delete(ctx context.Context, name string, deleteValidation rest.ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) &#123; ... // 这里会设置DeletionTimestamp的值 graceful, pendingGraceful, err := rest.BeforeDelete(e.DeleteStrategy, ctx, obj, options) if err != nil &#123; return nil, false, err &#125; // this means finalizers cannot be updated via DeleteOptions if a deletion is already pending if pendingGraceful &#123; out, err := e.finalizeDelete(ctx, obj, false, options) return out, false, err &#125; // check if obj has pending finalizers accessor, err := meta.Accessor(obj) if err != nil &#123; return nil, false, apierrors.NewInternalError(err) &#125; pendingFinalizers := len(accessor.GetFinalizers()) != 0 var ignoreNotFound // 用于判断资源是否能被立即删除 var deleteImmediately bool = true var lastExisting, out runtime.Object // Handle combinations of graceful deletion and finalization by issuing // the correct updates. shouldUpdateFinalizers, _ := deletionFinalizersForGarbageCollection(ctx, e, accessor, options) // TODO: remove the check, because we support no-op updates now. if graceful || pendingFinalizers || shouldUpdateFinalizers &#123; // / updateForGracefulDeletionAndFinalizers 函数用于为对象进行优雅删除和最终化的更新，// 它设置删除时间戳和优雅删除的宽限期秒数（graceful deletion），// 并更新最终器（finalizers）列表 // updateForGracefulDeletionAndFinalizers 通过设置DeletionTimestamp和grace perios seconds和更新finalizers列表 // 以实现优雅删除和终结 err, ignoreNotFound, deleteImmediately, out, lastExisting = e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, deleteValidation, obj) // Update the preconditions.ResourceVersion if set since we updated the object. if err == nil &amp;&amp; deleteImmediately &amp;&amp; preconditions.ResourceVersion != nil &#123; accessor, err = meta.Accessor(out) if err != nil &#123; return out, false, apierrors.NewInternalError(err) &#125; resourceVersion := accessor.GetResourceVersion() preconditions.ResourceVersion = &amp;resourceVersion &#125; &#125; // !deleteImmediately covers all cases where err != nil. We keep both to be future-proof. // 如果不能立即删除，直接返回。并不会删除etcd中的数据，pod状态变为Terminating if !deleteImmediately || err != nil &#123; return out, false, err &#125; // Going further in this function is not useful when we are // performing a dry-run request. Worse, it will actually // override "out" with the version of the object in database // that doesn't have the finalizer and deletiontimestamp set // (because the update above was dry-run too). If we already // have that version available, let's just return it now, // otherwise, we can call dry-run delete that will get us the // latest version of the object. if dryrun.IsDryRun(options.DryRun) &amp;&amp; out != nil &#123; return out, true, nil &#125; // delete immediately, or no graceful deletion supported klog.V(6).InfoS("Going to delete object from registry", "object", klog.KRef(genericapirequest.NamespaceValue(ctx), name)) out = e.NewFunc() if err := e.Storage.Delete(ctx, key, out, &amp;preconditions, storage.ValidateObjectFunc(deleteValidation), dryrun.IsDryRun(options.DryRun), nil); err != nil &#123; // Please refer to the place where we set ignoreNotFound for the reason // why we ignore the NotFound error . if storage.IsNotFound(err) &amp;&amp; ignoreNotFound &amp;&amp; lastExisting != nil &#123; // The lastExisting object may not be the last state of the object // before its deletion, but it's the best approximation. out, err := e.finalizeDelete(ctx, lastExisting, true, options) return out, true, err &#125; return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name) &#125; out, err = e.finalizeDelete(ctx, out, true, options) return out, true, err&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384// 在下面的函数中打个断点，执行edit操作之后通过监听etcd数据，发现并没有被删除// 可以知道删除逻辑就在这里了// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.gofunc (e *Store) Update(ctx context.Context, name string, objInfo rest.UpdatedObjectInfo, createValidation rest.ValidateObjectFunc, updateValidation rest.ValidateObjectUpdateFunc, forceAllowCreate bool, options *metav1.UpdateOptions) (runtime.Object, bool, error) &#123; key, err := e.KeyFunc(ctx, name) if err != nil &#123; return nil, false, err &#125; var ( creatingObj runtime.Object creating = false ) qualifiedResource := e.qualifiedResourceFromContext(ctx) storagePreconditions := &amp;storage.Preconditions&#123;&#125; if preconditions := objInfo.Preconditions(); preconditions != nil &#123; storagePreconditions.UID = preconditions.UID storagePreconditions.ResourceVersion = preconditions.ResourceVersion &#125; out := e.NewFunc() // deleteObj is only used in case a deletion is carried out var deleteObj runtime.Object err = e.Storage.GuaranteedUpdate(ctx, key, out, true, storagePreconditions, func(existing runtime.Object, res storage.ResponseMeta) (runtime.Object, *uint64, error) &#123; existingResourceVersion, err := e.Storage.Versioner().ObjectResourceVersion(existing) ...//省略部分代码 if err != nil &#123; // delete the object // 最终会进入到这里 if err == errEmptiedFinalizers &#123; // deleteWithoutFinalizers 会将数据从etcd中删除 return e.deleteWithoutFinalizers(ctx, name, key, deleteObj, storagePreconditions, newDeleteOptionsFromUpdateOptions(options)) &#125; if creating &#123; err = storeerr.InterpretCreateError(err, qualifiedResource, name) err = rest.CheckGeneratedNameError(ctx, e.CreateStrategy, err, creatingObj) &#125; else &#123; err = storeerr.InterpretUpdateError(err, qualifiedResource, name) &#125; return nil, false, err &#125; if creating &#123; if e.AfterCreate != nil &#123; e.AfterCreate(out, newCreateOptionsFromUpdateOptions(options)) &#125; &#125; else &#123; if e.AfterUpdate != nil &#123; e.AfterUpdate(out, options) &#125; &#125; if e.Decorator != nil &#123; e.Decorator(out) &#125; return out, creating, nil&#125;func (e *Store) deleteWithoutFinalizers(ctx context.Context, name, key string, obj runtime.Object, preconditions *storage.Preconditions, options *metav1.DeleteOptions) (runtime.Object, bool, error) &#123; out := e.NewFunc() klog.V(6).InfoS("Going to delete object from registry, triggered by update", "object", klog.KRef(genericapirequest.NamespaceValue(ctx), name)) // Using the rest.ValidateAllObjectFunc because the request is an UPDATE request and has already passed the admission for the UPDATE verb. if err := e.Storage.Delete(ctx, key, out, preconditions, rest.ValidateAllObjectFunc, dryrun.IsDryRun(options.DryRun), nil); err != nil &#123; // Deletion is racy, i.e., there could be multiple update // requests to remove all finalizers from the object, so we // ignore the NotFound error. if storage.IsNotFound(err) &#123; _, err := e.finalizeDelete(ctx, obj, true, options) // clients are expecting an updated object if a PUT succeeded, // but finalizeDelete returns a metav1.Status, so return // the object in the request instead. return obj, false, err &#125; return nil, false, storeerr.InterpretDeleteError(err, e.qualifiedResourceFromContext(ctx), name) &#125; _, err := e.finalizeDelete(ctx, out, true, options) // clients are expecting an updated object if a PUT succeeded, but // finalizeDelete returns a metav1.Status, so return the object in // the request instead. return obj, false, err&#125; 小结Terminating状态的Pod即不是由kubelet,也不是由kube-controller-manager发起请求删除的。而是当更新资源时(删除finalizers)调用Update接口会进入deleteWithoutFinalizers从而删除数据。 REF:1.staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcdctl的使用]]></title>
    <url>%2F2023%2F06%2F26%2Fetcdctl%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[etcdctl安装 访问etcd的GitHub仓库，链接为：https://github.com/etcd-io/etcd/releases 在该页面中，找到与您的操作系统和体系结构相匹配的最新版本的etcd二进制文件。例如，对于Linux系统，您可以下载以etcd-vX.Y.Z-linux-amd64.tar.gz为后缀的文件 下载所选版本的二进制文件，并将其解压缩到您的系统中 进入解压缩后的文件夹，并找到名为etcdctl的二进制文件 将etcdctl二进制文件移动到您的系统的可执行路径中，例如/usr/local/bin/ 确保etcdctl的可执行权限已设置，您可以使用chmod +x etcdctl命令进行设置 配置etcdctl环境变量将下面的信念添加到~/.bashrc,然后source ~/.zshrc将所有的文件改为当前用户有访问权限，如果访问要加sudo可能会有问题12345export ETCDCTL_API=3export ETCDCTL_ENDPOINTS=https://[etcd-endpoint]:2379export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crtexport ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crtexport ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key 常用的一些命令1234567891011# 检测endpoint健康状态etcdctl endpoint health# 返回所有以/registry开头的keyetcdctl get /registry --prefix --keys-only=true# 通过key获取对应的值并以json格式展示etcdctl get /registry/pods/default/b2 -w json# 监听指定键的变化etcdctl watch &lt;key&gt;]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之history]]></title>
    <url>%2F2023%2F06%2F25%2Fkube-controller%E4%B9%8Bhistory%2F</url>
    <content type="text"><![CDATA[ControllerRevisionControllerRevision 实现了一个不可变的状态数据快照。客户端负责序列化和反序列化包含其内部状态的对象一旦成功创建了 ControllerRevision，就不能对其进行更新,但是可以被删除。APIServer会拒绝所有试图修改 Data 字段的请求。ControllerRevision主要被 DaemonSet 和 StatefulSet 控制器用于更新和回滚。而且这个对象是beta的，在未来可能会发生变化，客户端不应依赖其稳定性。因此ControllerRevision主要在内部控制器内使用。 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170// staging/src/k8s.io/api/apps/v1/types.gotype ControllerRevision struct &#123; metav1.TypeMeta `json:",inline"` // Standard object's metadata. metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"` // Data is the serialized representation of the state. Data runtime.RawExtension `json:"data,omitempty" protobuf:"bytes,2,opt,name=data"` // Revision indicates the revision of the state represented by Data. Revision int64 `json:"revision" protobuf:"varint,3,opt,name=revision"`&#125;// pkg/controller/history/controller_history.gofunc NewControllerRevision(parent metav1.Object, parentKind schema.GroupVersionKind, templateLabels map[string]string, data runtime.RawExtension, revision int64, collisionCount *int32) (*apps.ControllerRevision, error) &#123; labelMap := make(map[string]string) for k, v := range templateLabels &#123; labelMap[k] = v &#125; cr := &amp;apps.ControllerRevision&#123; ObjectMeta: metav1.ObjectMeta&#123; Labels: labelMap, // 设置OwnerReferences OwnerReferences: []metav1.OwnerReference&#123;*metav1.NewControllerRef(parent, parentKind)&#125;, &#125;, Data: data, Revision: revision, &#125; // 返回cr.Data的hash hash := HashControllerRevision(cr, collisionCount) // 设置cr.Name cr.Name = ControllerRevisionName(parent.GetName(), hash) cr.Labels[ControllerRevisionHashLabel] = hash return cr, nil&#125;// pkg/controller/history/controller_history.gofunc ControllerRevisionName(prefix string, hash string) string &#123; if len(prefix) &gt; 223 &#123; prefix = prefix[:223] &#125; return fmt.Sprintf("%s-%s", prefix, hash)&#125;type realHistory struct &#123; client clientset.Interface lister appslisters.ControllerRevisionLister&#125;func NewHistory(client clientset.Interface, lister appslisters.ControllerRevisionLister) Interface &#123; return &amp;realHistory&#123;client, lister&#125;&#125;// 返回ControllerRevision列表func (rh *realHistory) ListControllerRevisions(parent metav1.Object, selector labels.Selector) ([]*apps.ControllerRevision, error) &#123; // List all revisions in the namespace that match the selector history, err := rh.lister.ControllerRevisions(parent.GetNamespace()).List(selector) if err != nil &#123; return nil, err &#125; var owned []*apps.ControllerRevision for i := range history &#123; ref := metav1.GetControllerOfNoCopy(history[i]) if ref == nil ControllerRevisions|| ref.UID == parent.GetUID() &#123; owned = append(owned, history[i]) &#125; &#125; return owned, err&#125;// 创建ControllerRevisionfunc (rh *realHistory) CreateControllerRevision(parent metav1.Object, revision *apps.ControllerRevision, collisionCount *int32) (*apps.ControllerRevision, error) &#123; if collisionCount ControllerRevisions== nil &#123; return nil, fmt.Errorf("collisionCount should not be nil") &#125; // Clone the input clone := revision.DeepCopy() // Continue to attempt to create the revision updating the name with a new hash on each iteration for &#123; hash := HashControllerRevision(revision, collisionCount) // Update the revisions name clone.Name = ControllerRevisionName(parent.GetName(), hash) ns := parent.GetNamespace() created, err := rh.client.AppsV1().ControllerRevisions(ns).Create(context.TODO(), clone, metav1.CreateOptions&#123;&#125;) if errors.IsAlreadyExists(err) &#123; exists, err := rh.client.AppsV1().ControllerRevisions(ns).Get(context.TODO(), clone.Name, metav1.GetOptions&#123;&#125;) if err != nil &#123; return nil, err &#125; if bytes.Equal(exists.Data.Raw, clone.Data.Raw) &#123; return exists, nil &#125; *collisionCount++ continue &#125; return created, err &#125;&#125;// 更新ControllerRevision,Data字段不能更新func (rh *realHistory) UpdateControllerRevision(revision *apps.ControllerRevision, newRevision int64) (*apps.ControllerRevision, error) &#123; clone := revision.DeepCopy() err := retry.RetryOnConflict(retry.DefaultBackoff, func() error &#123; if clone.Revision == newRevision &#123; return nil &#125; clone.Revision = newRevision updated, updateErr := rh.client.AppsV1().ControllerRevisions(clone.Namespace).Update(context.TODO(), clone, metav1.UpdateOptions&#123;&#125;) if updateErr == nil &#123; return nil &#125; if updated != nil &#123; clone = updated &#125; if updated, err := rh.lister.ControllerRevisions(clone.Namespace).Get(clone.Name); err == nil &#123; // make a copy so we don't mutate the shared cache clone = updated.DeepCopy() &#125; return updateErr &#125;) return clone, err&#125;// 删除ControllerRevisionfunc (rh *realHistory) DeleteControllerRevision(revision *apps.ControllerRevision) error &#123; return rh.client.AppsV1().ControllerRevisions(revision.Namespace).Delete(context.TODO(), revision.Name, metav1.DeleteOptions&#123;&#125;)&#125;// pkg/controller/statefulset/stateful_set_control.gofunc newRevision(set *apps.StatefulSet, revision int64, collisionCount *int32) (*apps.ControllerRevision, error) &#123; patch, err := getPatch(set) if err != nil &#123; return nil, err &#125; cr, err := history.NewControllerRevision(set, controllerKind, set.Spec.Template.Labels, runtime.RawExtension&#123;Raw: patch&#125;, revision, collisionCount) if err != nil &#123; return nil, err &#125; if cr.ObjectMeta.Annotations == nil &#123; cr.ObjectMeta.Annotations = make(map[string]string) &#125; for key, value := range set.Annotations &#123; cr.ObjectMeta.Annotations[key] = value &#125; return cr, nil&#125;func (ssc *defaultStatefulSetControl) getStatefulSetRevisions( set *apps.StatefulSet, revisions []*apps.ControllerRevision) (*apps.ControllerRevision, *apps.ControllerRevision, int32, error) &#123; ... updateRevision, err := newRevision(set, nextRevision(revisions), &amp;collisionCount) ...&#125; 小结严格来说ControllerRevision history controller并不是一个真正的控制器，只是提供了创建ControlllerRevison和对ControllerRevision操作的一些方法，可以给其它的控制器使用，比如StatefulSet。 REF:1.staging/src/k8s.io/api/apps/v1/types.go2.pkg/controller/history/controller_history.go3.pkg/controller/statefulset/stateful_set_control.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之ttlafterfinished]]></title>
    <url>%2F2023%2F06%2F24%2Fkube-controller%E4%B9%8Bttlafterfinished%2F</url>
    <content type="text"><![CDATA[TTLAfterFinishedTTLAfterFinished是一种自动清理已完成的 Job（包括 Complete 或 Failed 状态）的方法。其使用 TTL 机制，通过指定 Job的 .spec.ttlSecondsAfterFinished 字段。 当 TTLAfterFinished-Controller清理 Job 时，它将级联删除该 Job，即连同 Job 依赖的对象（例如 Pods）一起删除。请注意，当 Job 被删除时，将遵守其生命周期保证，例如 finalizer。 TTLAfterFinished-Controller 是负责监视 Job API 对象变化的组件。它通过监听 Job 的创建和更新事件，并将具有非空 .spec.ttlSecondsAfterFinished 字段的 Job 加入到队列中。TTLAfterFinished-Controller从队列中获取 Job，检查 Job 的 TTL 是否已过期。如果 Job 的 TTL 尚未过期，Worker将在预计 TTL 过期后将 Job 再次加入队列；如果 TTL 已过期，Worker将向 APIServer发送请求以相应地删除这些 Job。 这部分功能的实现与 Job 控制器分开，是为了分离关注点，并且可以扩展到处理其他可完成的资源类型。这种设计可以让 Job-Controller 只关注监视和管理 Job 的状态，而将具体的 TTL过期处理逻辑交给独立的组件来实现。这样可以提高代码的可维护性和可扩展性，并使代码结构更清晰。 123456789101112131415apiVersion: batch/v1kind: Jobmetadata: name: pi-with-ttlspec:# job pi-with-ttl将会在job完成后的100秒被删除# 如果ttlSecondsAfterFinished设置为0,将会在job完成后立即删除 ttlSecondsAfterFinished: 100 template: spec: containers: - name: pi image: perl:5.34.0 command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205// pkg/controller/ttlafterfinished/ttlafterfinished_controller.gotype Controller struct &#123; client clientset.Interface recorder record.EventRecorder // jLister can list/get Jobs from the shared informer's store jLister batchlisters.JobLister // jStoreSynced returns true if the Job store has been synced at least once. // Added as a member to the struct to allow injection for testing. jListerSynced cache.InformerSynced // Jobs that the controller will check its TTL and attempt to delete when the TTL expires. queue workqueue.RateLimitingInterface // The clock for tracking time clock clock.Clock&#125;// 初始化一个Controllerfunc New(ctx context.Context, jobInformer batchinformers.JobInformer, client clientset.Interface) *Controller &#123; eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartStructuredLogging(0) eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: client.CoreV1().Events("")&#125;) metrics.Register() tc := &amp;Controller&#123; client: client, recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "ttl-after-finished-controller"&#125;), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "ttl_jobs_to_delete"), &#125; logger := klog.FromContext(ctx) // Informer jobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; tc.addJob(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; tc.updateJob(logger, oldObj, newObj) &#125;, &#125;) tc.jLister = jobInformer.Lister() tc.jListerSynced = jobInformer.Informer().HasSynced tc.clock = clock.RealClock&#123;&#125; return tc&#125;func (tc *Controller) addJob(logger klog.Logger, obj interface&#123;&#125;) &#123; job := obj.(*batch.Job) logger.V(4).Info("Adding job", "job", klog.KObj(job)) // job没有被删除和j.Spec.TTLSecondsAfterFinished不为空和job已经完成 if job.DeletionTimestamp == nil &amp;&amp; needsCleanup(job) &#123; tc.enqueue(logger, job) &#125;&#125;func (tc *Controller) updateJob(logger klog.Logger, old, cur interface&#123;&#125;) &#123; job := cur.(*batch.Job) logger.V(4).Info("Updating job", "job", klog.KObj(job)) // job没有被删除和j.Spec.TTLSecondsAfterFinished不为空和job已经完成 if job.DeletionTimestamp == nil &amp;&amp; needsCleanup(job) &#123; tc.enqueue(logger, job) &#125;&#125;// TTLSecondsAfterFinished != nil &amp;&amp; Job已经完成func needsCleanup(j *batch.Job) bool &#123; return j.Spec.TTLSecondsAfterFinished != nil &amp;&amp; jobutil.IsJobFinished(j)&#125;// 启动Workerfunc (tc *Controller) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() defer tc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting TTL after finished controller") defer logger.Info("Shutting down TTL after finished controller") if !cache.WaitForNamedCacheSync("TTL after finished", ctx.Done(), tc.jListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, tc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (tc *Controller) worker(ctx context.Context) &#123; for tc.processNextWorkItem(ctx) &#123; &#125;&#125;func (tc *Controller) processNextWorkItem(ctx context.Context) bool &#123; key, quit := tc.queue.Get() if quit &#123; return false &#125; defer tc.queue.Done(key) // 调谐函数为processJob err := tc.processJob(ctx, key.(string)) tc.handleErr(err, key) return true&#125;// processJob 函数用于检查 Job 的状态和 TTL，并在 Job 完成并且其 TTL 过期后将其删除。// 如果 Job 尚未完成或者其 TTL 尚未过期，将在预计 TTL 过期后将 Job 重新添加到队列中。// 需要注意的是，该函数不应在相同的 key 下并发调用func (tc *Controller) processJob(ctx context.Context, key string) error &#123; // 从key中解析出namespace和资源名称 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // Ignore the Jobs that are already deleted or being deleted, or the ones that don't need clean up. // 获取对应的Job job, err := tc.jLister.Jobs(namespace).Get(name) logger := klog.FromContext(ctx) logger.V(4).Info("Checking if Job is ready for cleanup", "job", klog.KRef(namespace, name)) if errors.IsNotFound(err) &#123; return nil &#125; if err != nil &#123; return err &#125; if expiredAt, err := tc.processTTL(logger, job); err != nil &#123; return err &#125; else if expiredAt == nil &#123; return nil &#125; // The Job's TTL is assumed to have expired, but the Job TTL might be stale. // Before deleting the Job, do a final sanity check. // If TTL is modified before we do this check, we cannot be sure if the TTL truly expires. // The latest Job may have a different UID, but it's fine because the checks will be run again. // 在删除前再检查一个Job fresh, err := tc.client.BatchV1().Jobs(namespace).Get(ctx, name, metav1.GetOptions&#123;&#125;) if errors.IsNotFound(err) &#123; return nil &#125; if err != nil &#123; return err &#125; // Use the latest Job TTL to see if the TTL truly expires. // 用最新的job检查TTL是否真正的过期 expiredAt, err := tc.processTTL(logger, fresh) if err != nil &#123; return err &#125; else if expiredAt == nil &#123; return nil &#125; // Cascade deletes the Jobs if TTL truly expires. policy := metav1.DeletePropagationForeground options := metav1.DeleteOptions&#123; PropagationPolicy: &amp;policy, Preconditions: &amp;metav1.Preconditions&#123;UID: &amp;fresh.UID&#125;, &#125; logger.V(4).Info("Cleaning up Job", "job", klog.KObj(fresh)) // 删除对应的job if err := tc.client.BatchV1().Jobs(fresh.Namespace).Delete(ctx, fresh.Name, options); err != nil &#123; return err &#125; metrics.JobDeletionDurationSeconds.Observe(time.Since(*expiredAt).Seconds()) return nil&#125;// processTTL检查Job的TTL是否过期，将在预计 TTL 过期后将 Job 重新添加到队列中。func (tc *Controller) processTTL(logger klog.Logger, job *batch.Job) (expiredAt *time.Time, err error) &#123; // We don't care about the Jobs that are going to be deleted, or the ones that don't need clean up. // job已经删除或者不需要清理(TTL为空或者Job未完成) if job.DeletionTimestamp != nil || !needsCleanup(job) &#123; return nil, nil &#125; now := tc.clock.Now() t, e, err := timeLeft(logger, job, &amp;now) if err != nil &#123; return nil, err &#125; // TTL has expired if *t &lt;= 0 &#123; return e, nil &#125; tc.enqueueAfter(job, *t) return nil, nil&#125; REF:1.https://kubernetes.io/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically2.pkg/controller/ttlafterfinished/ttlafterfinished_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之job]]></title>
    <url>%2F2023%2F06%2F21%2Fkube-controller%E4%B9%8Bjob%2F</url>
    <content type="text"><![CDATA[Job在k8s中，Job是用于运行批处理或单次任务的资源。它代表了一个任务或一组并行任务，直到成功完成。Job通常用于数据处理、备份或其他需要执行一次或定期执行的任务CronJob。 以下是k8s中Job的一些关键特性和特点： 执行：Job确保任务成功完成后才将其标记为“完成”。每个任务都会执行直到完成，可以是一个单独的Pod或一组并行的Pod。 Pod创建：Job创建一个或多个Pod来执行任务。这些Pod由Job控制器自动管理。 任务并行性：Job可以配置为并行运行任务，可以通过指定并行Pod的数量或定义具有多个工作器的工作队列来实现。 重启策略：Job具有重启策略，用于确定任务失败或Pod终止时的处理方式。重启策略可以设置为“从不重启”、“仅在失败时重启”或“总是重启”。 任务完成条件：Job具有完成条件，定义了何时认为Job成功完成。完成条件可以基于成功完成的任务数量或用户指定的其他条件。 清理：Job可以配置为在任务完成后清理已完成的Pod。这样可以确保在Job完成后不会留下未释放的资源。 Job提供了在k8s中执行批处理和一次性任务的方式。它们是执行非交互式工作负载的基本构建块，可以与其他k8s资源（例如持久卷、配置映射和机密）结合使用，执行复杂的任务。 详细信息可查看官方文档 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370// pkg/controller/job/job_controller.govar ( // DefaultJobBackOff is the default backoff period. Exported for tests. DefaultJobBackOff = 10 * time.Second // MaxJobBackOff is the max backoff period. Exported for tests. MaxJobBackOff = 360 * time.Second // MaxUncountedPods is the maximum size the slices in // .status.uncountedTerminatedPods should have to keep their representation // roughly below 20 KB. Exported for tests MaxUncountedPods = 500 // MaxPodCreateDeletePerSync is the maximum number of pods that can be // created or deleted in a single sync call. Exported for tests. MaxPodCreateDeletePerSync = 500)type Controller struct &#123; kubeClient clientset.Interface podControl controller.PodControlInterface // To allow injection of the following for testing. updateStatusHandler func(ctx context.Context, job *batch.Job) (*batch.Job, error) patchJobHandler func(ctx context.Context, job *batch.Job, patch []byte) error syncHandler func(ctx context.Context, jobKey string) error // podStoreSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. // 判断是否至少出现一次全量同步 podStoreSynced cache.InformerSynced // jobStoreSynced returns true if the job store has been synced at least once. // Added as a member to the struct to allow injection for testing. jobStoreSynced cache.InformerSynced // A TTLCache of pod creates/deletes each rc expects to see expectations controller.ControllerExpectationsInterface // finalizerExpectations tracks the Pod UIDs for which the controller // expects to observe the tracking finalizer removed. finalizerExpectations *uidTrackingExpectations // A store of jobs jobLister batchv1listers.JobLister // A store of pods, populated by the podController podStore corelisters.PodLister // Jobs that need to be updated queue workqueue.RateLimitingInterface // Orphan deleted pods that still have a Job tracking finalizer to be removed orphanQueue workqueue.RateLimitingInterface broadcaster record.EventBroadcaster recorder record.EventRecorder podUpdateBatchPeriod time.Duration clock clock.WithTicker backoffRecordStore *backoffStore&#125;func NewController(podInformer coreinformers.PodInformer, jobInformer batchinformers.JobInformer, kubeClient clientset.Interface) *Controller &#123; return newControllerWithClock(podInformer, jobInformer, kubeClient, &amp;clock.RealClock&#123;&#125;)&#125;func newControllerWithClock(podInformer coreinformers.PodInformer, jobInformer batchinformers.JobInformer, kubeClient clientset.Interface, clock clock.WithTicker) *Controller &#123; eventBroadcaster := record.NewBroadcaster() jm := &amp;Controller&#123; kubeClient: kubeClient, podControl: controller.RealPodControl&#123; KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "job-controller"&#125;), &#125;, expectations: controller.NewControllerExpectations(), finalizerExpectations: newUIDTrackingExpectations(), queue: workqueue.NewRateLimitingQueueWithDelayingInterface(workqueue.NewDelayingQueueWithCustomClock(clock, "job"), workqueue.NewItemExponentialFailureRateLimiter(DefaultJobBackOff, MaxJobBackOff)), orphanQueue: workqueue.NewRateLimitingQueueWithDelayingInterface(workqueue.NewDelayingQueueWithCustomClock(clock, "job_orphan_pod"), workqueue.NewItemExponentialFailureRateLimiter(DefaultJobBackOff, MaxJobBackOff)), broadcaster: eventBroadcaster, recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "job-controller"&#125;), clock: clock, backoffRecordStore: newBackoffRecordStore(), &#125; if feature.DefaultFeatureGate.Enabled(features.JobReadyPods) &#123; jm.podUpdateBatchPeriod = podUpdateBatchPeriod &#125; jobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; jm.enqueueController(obj, true) &#125;, UpdateFunc: jm.updateJob, DeleteFunc: jm.deleteJob, &#125;) jm.jobLister = jobInformer.Lister() jm.jobStoreSynced = jobInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: jm.addPod, UpdateFunc: jm.updatePod, DeleteFunc: func(obj interface&#123;&#125;) &#123; jm.deletePod(obj, true) &#125;, &#125;) jm.podStore = podInformer.Lister() jm.podStoreSynced = podInformer.Informer().HasSynced // 向apiserver更新job的状态 jm.updateStatusHandler = jm.updateJobStatus // 设置对应的Handler函数 // 对job进行patch操作 jm.patchJobHandler = jm.patchJob // 调谐函数 jm.syncHandler = jm.syncJob metrics.Register() return jm&#125;// syncJob will sync the job with the given key if it has had its expectations fulfilled, meaning// it did not expect to see any more of its pods created or deleted. This function is not meant to be invoked// concurrently with the same key.func (jm *Controller) syncJob(ctx context.Context, key string) (rErr error) &#123; startTime := jm.clock.Now() defer func() &#123; klog.V(4).Infof("Finished syncing job %q (%v)", key, jm.clock.Since(startTime)) &#125;() ns, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; if len(ns) == 0 || len(name) == 0 &#123; return fmt.Errorf("invalid job key %q: either namespace or name is missing", key) &#125; // 获取对应的Job sharedJob, err := jm.jobLister.Jobs(ns).Get(name) if err != nil &#123; if apierrors.IsNotFound(err) &#123; // 如果Job不存在，说明Job已经被删除 // 删除对应的expectations klog.V(4).Infof("Job has been deleted: %v", key) jm.expectations.DeleteExpectations(key) jm.finalizerExpectations.deleteExpectations(key) err := jm.backoffRecordStore.removeBackoffRecord(key) if err != nil &#123; // re-syncing here as the record has to be removed for finished/deleted jobs return fmt.Errorf("error removing backoff record %w", err) &#125; return nil &#125; return err &#125; // make a copy so we don't mutate the shared cache job := *sharedJob.DeepCopy() // if job was finished previously, we don't want to redo the termination if IsJobFinished(&amp;job) &#123; // 从backoff缓存中删除 err := jm.backoffRecordStore.removeBackoffRecord(key) if err != nil &#123; // re-syncing here as the record has to be removed for finished/deleted jobs return fmt.Errorf("error removing backoff record %w", err) &#125; return nil &#125; // 目前有两种CompletionMode: 1. NonIndexed(default),2. Indexed // NonIndexed: 当完成的pod数量=.spec.completions表示此job已经完成，每个pod都是一样的 // Indexed: Job关联了一个completon index(0-.spec.completions-1),当每个index上都有 // 一个completed pod,表示此job已完成 // 当CompletionMod=“Indexed",必须指定.spec.completions,.spec.parallelism必须&lt;=100000 // pod名称$(job-name)-$(index)-$(random-string) // pod hostname:$(job-name)-$(index) if job.Spec.CompletionMode != nil &amp;&amp; *job.Spec.CompletionMode != batch.NonIndexedCompletion &amp;&amp; *job.Spec.CompletionMode != batch.IndexedCompletion &#123; jm.recorder.Event(&amp;job, v1.EventTypeWarning, "UnknownCompletionMode", "Skipped Job sync because completion mode is unknown") return nil &#125; // 获取CompletionMode completionMode := getCompletionMode(&amp;job) action := metrics.JobSyncActionReconciling defer func() &#123; result := "success" if rErr != nil &#123; result = "error" &#125; metrics.JobSyncDurationSeconds.WithLabelValues(completionMode, result, action).Observe(jm.clock.Since(startTime).Seconds()) metrics.JobSyncNum.WithLabelValues(completionMode, result, action).Inc() &#125;() // UncountedTerminatedPods 记录terminated状态但是job controller并没有记录的pod的UID if job.Status.UncountedTerminatedPods == nil &#123; job.Status.UncountedTerminatedPods = &amp;batch.UncountedTerminatedPods&#123;&#125; &#125; uncounted := newUncountedTerminatedPods(*job.Status.UncountedTerminatedPods) expectedRmFinalizers := jm.finalizerExpectations.getExpectedUIDs(key) // Check the expectations of the job before counting active pods, otherwise a new pod can sneak in // and update the expectations after we've retrieved active pods from the store. If a new pod enters // the store after we've checked the expectation, the job sync is just deferred till the next relist. satisfiedExpectations := jm.expectations.SatisfiedExpectations(key) pods, err := jm.getPodsForJob(ctx, &amp;job) if err != nil &#123; return err &#125; // activePod p.Status.Phase != v1.PodSucceeded &amp;&amp; p.Status.Phase!=v1.PodFailed &amp;&amp; p.DeletionTimestamp==nil activePods := controller.FilterActivePods(pods) active := int32(len(activePods)) // 返回未记录的最近成功和失败的pod newSucceededPods, newFailedPods := getNewFinishedPods(&amp;job, pods, uncounted, expectedRmFinalizers) // 更新成功的pod数量 succeeded := job.Status.Succeeded + int32(len(newSucceededPods)) + int32(len(uncounted.succeeded)) // 更新失败的pod数量 failed := job.Status.Failed + int32(len(newFailedPods)) + int32(len(uncounted.failed)) var ready *int32 if feature.DefaultFeatureGate.Enabled(features.JobReadyPods) &#123; ready = pointer.Int32(countReadyPods(activePods)) &#125; // Job first start. Set StartTime only if the job is not in the suspended state. if job.Status.StartTime == nil &amp;&amp; !jobSuspended(&amp;job) &#123; now := metav1.NewTime(jm.clock.Now()) job.Status.StartTime = &amp;now &#125; newBackoffInfo := jm.backoffRecordStore.newBackoffRecord(jm.clock, key, newSucceededPods, newFailedPods) var manageJobErr error var finishedCondition *batch.JobCondition jobHasNewFailure := failed &gt; job.Status.Failed // new failures happen when status does not reflect the failures and active // is different than parallelism, otherwise the previous controller loop // failed updating status so even if we pick up failure it is not a new one exceedsBackoffLimit := jobHasNewFailure &amp;&amp; (active != *job.Spec.Parallelism) &amp;&amp; (failed &gt; *job.Spec.BackoffLimit) if feature.DefaultFeatureGate.Enabled(features.JobPodFailurePolicy) &#123; if failureTargetCondition := findConditionByType(job.Status.Conditions, batch.JobFailureTarget); failureTargetCondition != nil &#123; finishedCondition = newFailedConditionForFailureTarget(failureTargetCondition, jm.clock.Now()) &#125; else if failJobMessage := getFailJobMessage(&amp;job, pods); failJobMessage != nil &#123; // Prepare the interim FailureTarget condition to record the failure message before the finalizers (allowing removal of the pods) are removed. finishedCondition = newCondition(batch.JobFailureTarget, v1.ConditionTrue, jobConditionReasonPodFailurePolicy, *failJobMessage, jm.clock.Now()) &#125; &#125; if finishedCondition == nil &#123; if exceedsBackoffLimit || pastBackoffLimitOnFailure(&amp;job, pods) &#123; // check if the number of pod restart exceeds backoff (for restart OnFailure only) // OR if the number of failed jobs increased since the last syncJob finishedCondition = newCondition(batch.JobFailed, v1.ConditionTrue, "BackoffLimitExceeded", "Job has reached the specified backoff limit", jm.clock.Now()) &#125; else if jm.pastActiveDeadline(&amp;job) &#123; finishedCondition = newCondition(batch.JobFailed, v1.ConditionTrue, "DeadlineExceeded", "Job was active longer than specified deadline", jm.clock.Now()) &#125; else if job.Spec.ActiveDeadlineSeconds != nil &amp;&amp; !jobSuspended(&amp;job) &#123; syncDuration := time.Duration(*job.Spec.ActiveDeadlineSeconds)*time.Second - jm.clock.Since(job.Status.StartTime.Time) klog.V(2).InfoS("Job has activeDeadlineSeconds configuration. Will sync this job again", "job", key, "nextSyncIn", syncDuration) jm.queue.AddAfter(key, syncDuration) &#125; &#125; var prevSucceededIndexes, succeededIndexes orderedIntervals // 如果是IndexedJob,统计成功的indexes if isIndexedJob(&amp;job) &#123; prevSucceededIndexes, succeededIndexes = calculateSucceededIndexes(&amp;job, pods) succeeded = int32(succeededIndexes.total()) &#125; suspendCondChanged := false // Remove active pods if Job failed. if finishedCondition != nil &#123; deleted, err := jm.deleteActivePods(ctx, &amp;job, activePods) if deleted != active || !satisfiedExpectations &#123; // Can't declare the Job as finished yet, as there might be remaining // pod finalizers or pods that are not in the informer's cache yet. finishedCondition = nil &#125; active -= deleted manageJobErr = err &#125; else &#123; manageJobCalled := false if satisfiedExpectations &amp;&amp; job.DeletionTimestamp == nil &#123; active, action, manageJobErr = jm.manageJob(ctx, &amp;job, activePods, succeeded, succeededIndexes, newBackoffInfo) manageJobCalled = true &#125; complete := false if job.Spec.Completions == nil &#123; // This type of job is complete when any pod exits with success. // Each pod is capable of // determining whether or not the entire Job is done. Subsequent pods are // not expected to fail, but if they do, the failure is ignored. Once any // pod succeeds, the controller waits for remaining pods to finish, and // then the job is complete. complete = succeeded &gt; 0 &amp;&amp; active == 0 &#125; else &#123; // Job specifies a number of completions. This type of job signals // success by having that number of successes. Since we do not // start more pods than there are remaining completions, there should // not be any remaining active pods once this count is reached. complete = succeeded &gt;= *job.Spec.Completions &amp;&amp; active == 0 &#125; if complete &#123; finishedCondition = newCondition(batch.JobComplete, v1.ConditionTrue, "", "", jm.clock.Now()) &#125; else if manageJobCalled &#123; // Update the conditions / emit events only if manageJob was called in // this syncJob. Otherwise wait for the right syncJob call to make // updates. if job.Spec.Suspend != nil &amp;&amp; *job.Spec.Suspend &#123; // Job can be in the suspended state only if it is NOT completed. var isUpdated bool job.Status.Conditions, isUpdated = ensureJobConditionStatus(job.Status.Conditions, batch.JobSuspended, v1.ConditionTrue, "JobSuspended", "Job suspended", jm.clock.Now()) if isUpdated &#123; suspendCondChanged = true jm.recorder.Event(&amp;job, v1.EventTypeNormal, "Suspended", "Job suspended") &#125; &#125; else &#123; // Job not suspended. var isUpdated bool job.Status.Conditions, isUpdated = ensureJobConditionStatus(job.Status.Conditions, batch.JobSuspended, v1.ConditionFalse, "JobResumed", "Job resumed", jm.clock.Now()) if isUpdated &#123; suspendCondChanged = true jm.recorder.Event(&amp;job, v1.EventTypeNormal, "Resumed", "Job resumed") // Resumed jobs will always reset StartTime to current time. This is // done because the ActiveDeadlineSeconds timer shouldn't go off // whilst the Job is still suspended and resetting StartTime is // consistent with resuming a Job created in the suspended state. // (ActiveDeadlineSeconds is interpreted as the number of seconds a // Job is continuously active.) now := metav1.NewTime(jm.clock.Now()) job.Status.StartTime = &amp;now &#125; &#125; &#125; &#125; needsStatusUpdate := suspendCondChanged || active != job.Status.Active || !equalReady(ready, job.Status.Ready) job.Status.Active = active job.Status.Ready = ready err = jm.trackJobStatusAndRemoveFinalizers(ctx, &amp;job, pods, prevSucceededIndexes, *uncounted, expectedRmFinalizers, finishedCondition, needsStatusUpdate, newBackoffInfo) if err != nil &#123; if apierrors.IsConflict(err) &#123; // we probably have a stale informer cache // so don't return an error to avoid backoff jm.enqueueController(&amp;job, false) return nil &#125; return fmt.Errorf("tracking status: %w", err) &#125; // 判断Job是否已经完成 jobFinished := IsJobFinished(&amp;job) if jobHasNewFailure &amp;&amp; !jobFinished &#123; // returning an error will re-enqueue Job after the backoff period return fmt.Errorf("failed pod(s) detected for job key %q", key) &#125; return manageJobErr&#125;// pkg/controller/controller_utils.gofunc IsJobFinished(j *batch.Job) bool &#123; for _, c := range j.Status.Conditions &#123; if (c.Type == batch.JobComplete || c.Type == batch.JobFailed) &amp;&amp; c.Status == v1.ConditionTrue &#123; return true &#125; &#125; return false&#125; REF:1.pkg/controller/job/job_controller.go2.pkg/controller/controller_utils.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s hasSynced]]></title>
    <url>%2F2023%2F06%2F20%2Fk8s-hasSynced%2F</url>
    <content type="text"><![CDATA[k8s cache在k8s中为减轻apiserver的压力,会使用缓存来存储k8s中的资源，这样每次访问就可以直接从缓存了获取数据(k8s通过watch，resync机制来保证本地缓存和数据的同步)。 当我们在使用informer机制时，必须等待资源同步到本地缓存中。所以如何等待和判断数据已经同步到了本地缓存呢？ 源码分析12345678factory := informers.NewSharedInformerFactory(clientset, time.Minute)podInformer := factory.Core().V1().Pods().Informer() ... go podInformer.Run(make(chan struct&#123;&#125;)) // client-go 中通过WaitForCacheSync来等待缓存同步 if !cache.WaitForCacheSync(make(chan struct&#123;&#125;), podInformer.HasSynced) &#123; return&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// staging/src/k8s.io/client-go/tools/cache/shared_informer.gofunc WaitForCacheSync(stopCh &lt;-chan struct&#123;&#125;, cacheSyncs ...InformerSynced) bool &#123; err := wait.PollImmediateUntil(syncedPollPeriod, func() (bool, error) &#123; for _, syncFunc := range cacheSyncs &#123; // 这里的syncFunc就是podInformer.HasSynced if !syncFunc() &#123; return false, nil &#125; &#125; return true, nil &#125;, stopCh) if err != nil &#123; klog.V(2).Infof("stop requested") return false &#125; klog.V(4).Infof("caches populated") return true&#125;func (s *sharedIndexInformer) HasSynced() bool &#123; s.startedLock.Lock() defer s.startedLock.Unlock() if s.controller == nil &#123; return false &#125; return s.controller.HasSynced()&#125;// staging/src/k8s.io/client-go/tools/cache/controller.gofunc (c *controller) HasSynced() bool &#123; return c.config.Queue.HasSynced()&#125;// staging/src/k8s.io/client-go/tools/cache/delta_fifo.go// 最后是调用DeltaFIFO中HasSynced方法进行判断func (f *DeltaFIFO) HasSynced() bool &#123; f.lock.Lock() defer f.lock.Unlock() // 为什么这两个条件就能判断是否进行过一次fulllist呢？ // 接着往下看 return f.populated &amp;&amp; f.initialPopulationCount == 0&#125;// staging/src/k8s.io/apimachinery/pkg/util/wait/poll.go// PollImmediateUntil执行conditionFunc 直到函数返回true,或者error或者关闭了stopChfunc PollImmediateUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct&#123;&#125;) error &#123; done, err := condition() if err != nil &#123; return err &#125; if done &#123; return nil &#125; select &#123; case &lt;-stopCh: return ErrWaitTimeout default: return PollUntil(interval, condition, stopCh) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237type DeltaFIFO struct &#123; // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update/AddIfNotPresent was called first. // 调用了Repllace()函数，或者Delete/Add/Update/AddIfNotPresent， populated = true populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() // 调用Replace()插入数据的长度 initialPopulationCount int&#125;func (s *sharedIndexInformer) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions&#123; KnownObjects: s.indexer, EmitDeltaTypeReplaced: true, &#125;) cfg := &amp;Config&#123; Queue: fifo, ListerWatcher: s.listerWatcher, ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, Process: s.HandleDeltas, WatchErrorHandler: s.watchErrorHandler, &#125; func() &#123; s.startedLock.Lock() defer s.startedLock.Unlock() s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true &#125;() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct&#123;&#125;) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() &#123; s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners &#125;() s.controller.Run(stopCh)&#125;func (c *controller) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() go func() &#123; &lt;-stopCh c.config.Queue.Close() &#125;() r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.WatchListPageSize = c.config.WatchListPageSize r.clock = c.clock if c.config.WatchErrorHandler != nil &#123; r.watchErrorHandler = c.config.WatchErrorHandler &#125; c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group wg.StartWithChannel(stopCh, r.Run) wait.Until(c.processLoop, time.Second, stopCh) wg.Wait()&#125;func (r *Reflector) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; klog.V(2).Infof("Starting reflector %s (%s) from %s", r.expectedTypeName, r.resyncPeriod, r.name) wait.BackoffUntil(func() &#123; if err := r.ListAndWatch(stopCh); err != nil &#123; r.watchErrorHandler(r, err) &#125; &#125;, r.backoffManager, true, stopCh) klog.V(2).Infof("Stopping reflector %s (%s) from %s", r.expectedTypeName, r.resyncPeriod, r.name)&#125;func (r *Reflector) ListAndWatch(stopCh &lt;-chan struct&#123;&#125;) error &#123; ... // syncWith会调用Delta中的Replace方法 if err := r.syncWith(items, resourceVersion); err != nil &#123; return fmt.Errorf("unable to sync list result: %v", err) &#125; ...&#125;func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error &#123; found := make([]interface&#123;&#125;, 0, len(items)) for _, item := range items &#123; found = append(found, item) &#125; return r.store.Replace(found, resourceVersion)&#125;func (f *DeltaFIFO) Replace(list []interface&#123;&#125;, resourceVersion string) error &#123; f.lock.Lock() defer f.lock.Unlock() keys := make(sets.String, len(list)) // keep backwards compat for old clients action := Sync if f.emitDeltaTypeReplaced &#123; action = Replaced &#125; // Add Sync/Replaced action for each new item. for _, item := range list &#123; key, err := f.KeyOf(item) if err != nil &#123; return KeyError&#123;item, err&#125; &#125; keys.Insert(key) if err := f.queueActionLocked(action, item); err != nil &#123; return fmt.Errorf("couldn't enqueue object: %v", err) &#125; &#125; if f.knownObjects == nil &#123; // Do deletion detection against our own list. queuedDeletions := 0 for k, oldItem := range f.items &#123; if keys.Has(k) &#123; continue &#125; // Delete pre-existing items not in the new list. // This could happen if watch deletion event was missed while // disconnected from apiserver. var deletedObj interface&#123;&#125; if n := oldItem.Newest(); n != nil &#123; deletedObj = n.Object &#125; queuedDeletions++ if err := f.queueActionLocked(Deleted, DeletedFinalStateUnknown&#123;k, deletedObj&#125;); err != nil &#123; return err &#125; &#125; if !f.populated &#123; // 如果在这之前没有调用Add/Update/Delete/AddIfNotPresent,则设置populated = true f.populated = true // While there shouldn't be any queued deletions in the initial // population of the queue, it's better to be on the safe side. // 设置initialPopulationCount, 在进行Pop操作的时候会对initialPopulationCount-1 f.initialPopulationCount = keys.Len() + queuedDeletions &#125; return nil &#125; // Detect deletions not already in the queue. knownKeys := f.knownObjects.ListKeys() queuedDeletions := 0 for _, k := range knownKeys &#123; if keys.Has(k) &#123; continue &#125; deletedObj, exists, err := f.knownObjects.GetByKey(k) if err != nil &#123; deletedObj = nil klog.Errorf("Unexpected error %v during lookup of key %v, placing DeleteFinalStateUnknown marker without object", err, k) &#125; else if !exists &#123; deletedObj = nil klog.Infof("Key %v does not exist in known objects store, placing DeleteFinalStateUnknown marker without object", k) &#125; queuedDeletions++ if err := f.queueActionLocked(Deleted, DeletedFinalStateUnknown&#123;k, deletedObj&#125;); err != nil &#123; return err &#125; &#125; if !f.populated &#123; f.populated = true f.initialPopulationCount = keys.Len() + queuedDeletions &#125; return nil&#125;func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface&#123;&#125;, error) &#123; f.lock.Lock() defer f.lock.Unlock() for &#123; for len(f.queue) == 0 &#123; // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.closed &#123; return nil, ErrFIFOClosed &#125; f.cond.Wait() &#125; id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount &gt; 0 &#123; f.initialPopulationCount-- &#125; item, ok := f.items[id] if !ok &#123; // This should never happen klog.Errorf("Inconceivable! %q was in f.queue but not f.items; ignoring.", id) continue &#125; delete(f.items, id) err := process(item) if e, ok := err.(ErrRequeue); ok &#123; f.addIfNotPresent(id, item) err = e.Err &#125; // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err &#125;&#125; 小结综上f.populated &amp;&amp; f.initialPopulationCount == 0 表示至少进行过一次全量List REF:1.staging/src/k8s.io/client-go/tools/cache/shared_informer.go2.staging/src/k8s.io/client-go/tools/cache/controller.go3.staging/src/k8s.io/client-go/tools/cache/delta_fifo.go4.staging/src/k8s.io/apimachinery/pkg/util/wait/poll.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s Pod Qos]]></title>
    <url>%2F2023%2F06%2F18%2Fk8s-Pod-Qos%2F</url>
    <content type="text"><![CDATA[QOSQos(Quality of Service),k8s会根据Pod容器中指定的资源约束为每个Pod设置QoS类。k8s依赖这种分类来决定当Node上没有足够可用资源时要驱逐哪些 Pod。 QoS 类k8s 对你运行的 Pod 进行分类，并将每个 Pod 分配到特定的 QoS类中。 k8s 使用这种分类来影响不同 Pod 被处理的方式。k8s 基于 Pod 中容器的资源请求进行分类， 同时确定这些请求如何与资源限制相关。 这称为服务质量 (QoS) 类。 k8s 基于每个 Pod 中容器的资源请求和限制为 Pod 设置 QoS 。k8s 使用 QoS 类来决定从遇到节点压力的 Node 中驱逐哪些 Pod。可选的 QoS 类有 Guaranteed、Burstable 和 BestEffort。 当一个 Node 耗尽资源时，k8s 将首先驱逐在该 Node 上运行的 BestEffort Pod， 然后是 Burstable Pod，最后是 Guaranteed Pod。当这种驱逐是由于资源压力时， 只有超出资源请求的 Pod 才是被驱逐的候选对象。 GuaranteedGuaranteed Pod 具有最严格的资源限制，并且最不可能面临驱逐。 在这些 Pod 超过其自身的限制或者没有可以从 Node 抢占的低优先级 Pod 之前， 这些 Pod 保证不会被杀死。这些 Pod 不可以获得超出其指定 limit 的资源。这些 Pod 也可以使用 static CPU 管理策略来使用独占的 CPU。 Pod 被赋予 Guaranteed QoS 类的几个依据： Pod 中的每个容器必须有内存 limit 和内存 request。 对于 Pod 中的每个容器，内存 limit 必须等于内存 request。 Pod 中的每个容器必须有 CPU limit 和 CPU request。 对于 Pod 中的每个容器，CPU limit 必须等于 CPU request。 BurstableBurstable Pod 有一些基于 request 的资源下限保证，但不需要特定的 limit。 如果未指定 limit，则默认为其 limit 等于 Node 容量，这允许 Pod 在资源可用时灵活地增加其资源。 在由于 Node 资源压力导致 Pod 被驱逐的情况下，只有在所有 BestEffort Pod 被驱逐后 这些 Pod 才会被驱逐。因为 Burstable Pod 可以包括没有资源 limit 或资源 request 的容器， 所以 Burstable Pod 可以尝试使用任意数量的节点资源。 Pod被赋予 Burstable QoS 类的几个依据： Pod 不满足针对 QoS 类 Guaranteed 的判据。 Pod 中至少一个容器有内存或 CPU 的 request 或 limit。 BestEffortBestEffort QoS 类中的 Pod 可以使用未专门分配给其他 QoS 类中的 Pod 的节点资源。 例如若你有一个节点有 16 核 CPU 可供 kubelet 使用，并且你将 4 核 CPU 分配给一个 Guaranteed Pod， 那么 BestEffort QoS 类中的 Pod 可以尝试任意使用剩余的 12 核 CPU。 如果节点遇到资源压力，kubelet 将优先驱逐 BestEffort Pod。 如果 Pod 不满足 Guaranteed 或 Burstable 的判据，则它的 QoS 类为 BestEffort。 换言之，只有当 Pod 中的所有容器没有内存 limit 或内存 request，也没有 CPU limit 或 CPU request 时，Pod 才是 BestEffort。Pod 中的容器可以请求（除 CPU 或内存之外的） 其他资源并且仍然被归类为 BestEffort。 以上来自k8s文档 Qos与cgroup的关系创建不同Qos的pod使用下面的yaml文件创建对应的`pod12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Podmetadata: name: guaranteedspec: containers: - name: app image: hysyeah/my-curl:v1 resources: requests: memory: "64Mi" cpu: "250m" limits: memory: "64Mi" cpu: "250m"--apiVersion: v1kind: Podmetadata: name: burstablespec: containers: - name: app image: hysyeah/my-curl:v1 resources: requests: memory: "64Mi" cpu: "250m"---apiVersion: v1kind: Podmetadata: name: besteffortspec: containers: - name: app image: hysyeah/my-curl:v1 当我们apply这个yaml文件后，Pod会被调度到对应的Node,kubelet会将PodSpec中的内容传递给CRI,CRI再将这此内容翻译成更底层的OCI JSON Spec描述容器创建的信息。 crictl inspect &lt;containerID&gt;查看对应的资源信息123456789101112131415161718192021// 0e5962a09e2d3的部分内容// 包括了资源的限制信息及cgroupsPath"linux": &#123; "resources": &#123; "devices": [ &#123; "allow": false, "access": "rwm" &#125; ], "memory": &#123; "limit": 67108864 // 64M &#125;, "cpu": &#123; "shares": 256, "quota": 25000, "period": 100000 &#125; &#125;, "cgroupsPath": "/kubepods/pod21ef5ccc-a09d-4754-aa1e-a65820d572cc/0e5962a09e2d3840e4f6ae1a3750136ad578c3df026a8dd64b175f8e33570823",&#125; 创建完成后你可以查看对应pod的Qos。k describe pod &lt;pod-name&gt; 查看系统中的cgroups 节点系统信息 12345678✗ uname -aLinux hysyeah 5.15.0-72-generic #79~20.04.1-Ubuntu SMP Thu Apr 20 22:12:07 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux# cgroups版本# 对于 cgroup v2，输出为 cgroup2fs# 对于 cgroup v1，输出为 tmpfs➜ stat -fc %T /sys/fs/cgroup/tmpfs 查看容器相关信息crictl ps,对应的PodID和ContainerID下面我们会使用到 cgroups的目录为/sys/fs/cgroup,我们只关注cpu和memory。 12➜ cgroup lsblkio cpu cpuacct cpu,cpuacct cpuset devices freezer hugetlb memory misc net_cls net_cls,net_prio net_prio perf_event pids rdma systemd unified 可以看到cpu和memory目录下都有一个名为kubepods的目录，k8s相关的cgroups就是放在这个目录下的。以memory/kubepods为例：其中Qos为burstable和besteffort的Pod的cgroup信息分别放在目录burstable,besteffort下。Qos为guaranteed的pod的cgroup信息放在]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之resourceclaim]]></title>
    <url>%2F2023%2F06%2F18%2Fkube-controller%E4%B9%8Bresourceclaim%2F</url>
    <content type="text"><![CDATA[动态资源分配动态资源分配是一个用于在 Pod 之间和 Pod 内部容器之间请求和共享资源的新 API。 它是对为通用资源所提供的持久卷 API 的泛化。第三方资源驱动程序负责跟踪和分配资源。 不同类型的资源支持用任意参数进行定义和初始化. resource.k8s.io/v1alpha2 API 组提供四种新类型： ResourceClass定义由哪个资源驱动程序处理某种资源，并为其提供通用参数。 集群管理员在安装资源驱动程序时创建 ResourceClass。ResourceClaim定义工作负载所需的特定资源实例。 由用户创建（手动管理生命周期，可以在不同的 Pod 之间共享）， 或者由控制平面基于 ResourceClaimTemplate 为特定 Pod 创建 （自动管理生命周期，通常仅由一个 Pod 使用）。ResourceClaimTemplate定义用于创建 ResourceClaim 的 spec 和一些元数据。 部署工作负载时由用户创建。PodSchedulingContext供控制平面和资源驱动程序内部使用， 在需要为 Pod 分配 ResourceClaim 时协调 Pod 调度。ResourceClass 和 ResourceClaim 的参数存储在单独的对象中， 通常使用安装资源驱动程序时创建的 CRD 所定义的类型。 core/v1 的 PodSpec 在新的 resourceClaims 字段中定义 Pod 所需的 ResourceClaim。 该列表中的条目引用 ResourceClaim 或 ResourceClaimTemplate。 当引用 ResourceClaim 时，使用此 PodSpec 的所有 Pod （例如 Deployment 或 StatefulSet 中的 Pod）共享相同的 ResourceClaim 实例。 引用 ResourceClaimTemplate 时，每个 Pod 都有自己的实例。 容器资源的 resources.claims 列表定义容器可以访问的资源实例， 从而可以实现在一个或多个容器之间共享资源。 下面是一个虚构的资源驱动程序的示例。 该示例将为此 Pod 创建两个 ResourceClaim 对象，每个容器都可以访问其中一个。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849piVersion: resource.k8s.io/v1alpha2kind: ResourceClassname: resource.example.comdriverName: resource-driver.example.com---apiVersion: cats.resource.example.com/v1kind: ClaimParametersname: large-black-cat-claim-parametersspec: color: black size: large---apiVersion: resource.k8s.io/v1alpha2kind: ResourceClaimTemplatemetadata: name: large-black-cat-claim-templatespec: spec: resourceClassName: resource.example.com parametersRef: apiGroup: cats.resource.example.com kind: ClaimParameters name: large-black-cat-claim-parameters–--apiVersion: v1kind: Podmetadata: name: pod-with-catsspec: containers: - name: container0 image: ubuntu:20.04 command: ["sleep", "9999"] resources: claims: - name: cat-0 - name: container1 image: ubuntu:20.04 command: ["sleep", "9999"] resources: claims: - name: cat-1 resourceClaims: - name: cat-0 source: resourceClaimTemplateName: large-black-cat-claim-template - name: cat-1 source: resourceClaimTemplateName: large-black-cat-claim-template resourceClaimresourceClaim Controller的作用就是根据pod spec中的ResourceClaimTemplates创建对应的ResourceClaims。 resourceClaim源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276/// pkg/controller/resourceclaim/controller.gotype Controller struct &#123; // kubeClient is the kube API client used to communicate with the API // server. kubeClient clientset.Interface // claimLister is the shared ResourceClaim lister used to fetch and store ResourceClaim // objects from the API server. It is shared with other controllers and // therefore the ResourceClaim objects in its store should be treated as immutable. claimLister resourcev1alpha2listers.ResourceClaimLister claimsSynced cache.InformerSynced // podLister is the shared Pod lister used to fetch Pod // objects from the API server. It is shared with other controllers and // therefore the Pod objects in its store should be treated as immutable. podLister v1listers.PodLister podSynced cache.InformerSynced // templateLister is the shared ResourceClaimTemplate lister used to // fetch template objects from the API server. It is shared with other // controllers and therefore the objects in its store should be treated // as immutable. templateLister resourcev1alpha2listers.ResourceClaimTemplateLister templatesSynced cache.InformerSynced // podIndexer has the common PodResourceClaim indexer indexer installed To // limit iteration over pods to those of interest. podIndexer cache.Indexer // recorder is used to record events in the API server recorder record.EventRecorder queue workqueue.RateLimitingInterface // The deletedObjects cache keeps track of Pods for which we know that // they have existed and have been removed. For those we can be sure // that a ReservedFor entry needs to be removed. deletedObjects *uidCache&#125;// 创建resourceClaim Controller// resourceClaim为pod和resourceClaim添加事件监听func NewController( kubeClient clientset.Interface, podInformer v1informers.PodInformer, claimInformer resourcev1alpha2informers.ResourceClaimInformer, templateInformer resourcev1alpha2informers.ResourceClaimTemplateInformer) (*Controller, error) &#123; ec := &amp;Controller&#123; kubeClient: kubeClient, podLister: podInformer.Lister(), podIndexer: podInformer.Informer().GetIndexer(), podSynced: podInformer.Informer().HasSynced, claimLister: claimInformer.Lister(), claimsSynced: claimInformer.Informer().HasSynced, templateLister: templateInformer.Lister(), templatesSynced: templateInformer.Informer().HasSynced, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "resource_claim"), deletedObjects: newUIDCache(maxUIDCacheEntries), &#125; metrics.RegisterMetrics() if _, err := podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; ec.enqueuePod(obj, false) &#125;, UpdateFunc: func(old, updated interface&#123;&#125;) &#123; ec.enqueuePod(updated, false) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; ec.enqueuePod(obj, true) &#125;, &#125;); err != nil &#123; return nil, err &#125; if _, err := claimInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: ec.onResourceClaimAddOrUpdate, UpdateFunc: func(old, updated interface&#123;&#125;) &#123; ec.onResourceClaimAddOrUpdate(updated) &#125;, DeleteFunc: ec.onResourceClaimDelete, &#125;); err != nil &#123; return nil, err &#125; if err := ec.podIndexer.AddIndexers(cache.Indexers&#123;podResourceClaimIndex: podResourceClaimIndexFunc&#125;); err != nil &#123; return nil, fmt.Errorf("could not initialize ResourceClaim controller: %w", err) &#125; return ec, nil&#125;func (ec *Controller) Run(ctx context.Context, workers int) &#123; defer runtime.HandleCrash() defer ec.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting ephemeral volume controller") defer logger.Info("Shutting down ephemeral volume controller") eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(klog.Infof) eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: ec.kubeClient.CoreV1().Events("")&#125;) ec.recorder = eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "resource_claim"&#125;) defer eventBroadcaster.Shutdown() if !cache.WaitForNamedCacheSync("ephemeral", ctx.Done(), ec.podSynced, ec.claimsSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, ec.runWorker, time.Second) &#125; &lt;-ctx.Done()&#125;func (ec *Controller) runWorker(ctx context.Context) &#123; for ec.processNextWorkItem(ctx) &#123; &#125;&#125;func (ec *Controller) processNextWorkItem(ctx context.Context) bool &#123; key, shutdown := ec.queue.Get() if shutdown &#123; return false &#125; defer ec.queue.Done(key) err := ec.syncHandler(ctx, key.(string)) if err == nil &#123; ec.queue.Forget(key) return true &#125; runtime.HandleError(fmt.Errorf("%v failed with: %v", key, err)) ec.queue.AddRateLimited(key) return true&#125;// syncHandler is invoked for each work item which might need to be processed.// If an error is returned from this function, the item will be requeued.func (ec *Controller) syncHandler(ctx context.Context, key string) error &#123; sep := strings.Index(key, ":") if sep &lt; 0 &#123; return fmt.Errorf("unexpected key: %s", key) &#125; prefix, object := key[0:sep+1], key[sep+1:] namespace, name, err := cache.SplitMetaNamespaceKey(object) if err != nil &#123; return err &#125; // 通过prefix判断是pod事件还是resourceClaim事件，从而调用不同的处理函数 switch prefix &#123; case podKeyPrefix: return ec.syncPod(ctx, namespace, name) case claimKeyPrefix: return ec.syncClaim(ctx, namespace, name) default: return fmt.Errorf("unexpected key prefix: %s", prefix) &#125;&#125;func (ec *Controller) syncPod(ctx context.Context, namespace, name string) error &#123; logger := klog.LoggerWithValues(klog.FromContext(ctx), "pod", klog.KRef(namespace, name)) ctx = klog.NewContext(ctx, logger) pod, err := ec.podLister.Pods(namespace).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; logger.V(5).Info("nothing to do for pod, it is gone") return nil &#125; return err &#125; // Ignore pods which are already getting deleted. if pod.DeletionTimestamp != nil &#123; logger.V(5).Info("nothing to do for pod, it is marked for deletion") return nil &#125; for _, podClaim := range pod.Spec.ResourceClaims &#123; if err := ec.handleClaim(ctx, pod, podClaim); err != nil &#123; if ec.recorder != nil &#123; ec.recorder.Event(pod, v1.EventTypeWarning, "FailedResourceClaimCreation", fmt.Sprintf("PodResourceClaim %s: %v", podClaim.Name, err)) &#125; return fmt.Errorf("pod %s/%s, PodResourceClaim %s: %v", namespace, name, podClaim.Name, err) &#125; &#125; return nil&#125;func (ec *Controller) syncClaim(ctx context.Context, namespace, name string) error &#123; logger := klog.LoggerWithValues(klog.FromContext(ctx), "PVC", klog.KRef(namespace, name)) ctx = klog.NewContext(ctx, logger) claim, err := ec.claimLister.ResourceClaims(namespace).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; logger.V(5).Info("nothing to do for claim, it is gone") return nil &#125; return err &#125; // Check if the ReservedFor entries are all still valid. valid := make([]resourcev1alpha2.ResourceClaimConsumerReference, 0, len(claim.Status.ReservedFor)) for _, reservedFor := range claim.Status.ReservedFor &#123; if reservedFor.APIGroup == "" &amp;&amp; reservedFor.Resource == "pods" &#123; // A pod falls into one of three categories: // - we have it in our cache -&gt; don't remove it until we are told that it got removed // - we don't have it in our cache anymore, but we have seen it before -&gt; it was deleted, remove it // - not in our cache, not seen -&gt; double-check with API server before removal keepEntry := true // Tracking deleted pods in the LRU cache is an // optimization. Without this cache, the code would // have to do the API call below for every deleted pod // to ensure that the pod really doesn't exist. With // the cache, most of the time the pod will be recorded // as deleted and the API call can be avoided. if ec.deletedObjects.Has(reservedFor.UID) &#123; // We know that the pod was deleted. This is // easy to check and thus is done first. keepEntry = false &#125; else &#123; pod, err := ec.podLister.Pods(claim.Namespace).Get(reservedFor.Name) if err != nil &amp;&amp; !errors.IsNotFound(err) &#123; return err &#125; if pod == nil &#123; // We might not have it in our informer cache // yet. Removing the pod while the scheduler is // scheduling it would be bad. We have to be // absolutely sure and thus have to check with // the API server. pod, err := ec.kubeClient.CoreV1().Pods(claim.Namespace).Get(ctx, reservedFor.Name, metav1.GetOptions&#123;&#125;) if err != nil &amp;&amp; !errors.IsNotFound(err) &#123; return err &#125; if pod == nil || pod.UID != reservedFor.UID &#123; keepEntry = false &#125; &#125; else if pod.UID != reservedFor.UID &#123; // Pod exists, but is a different incarnation under the same name. keepEntry = false &#125; &#125; if keepEntry &#123; valid = append(valid, reservedFor) &#125; continue &#125; // TODO: support generic object lookup return fmt.Errorf("unsupported ReservedFor entry: %v", reservedFor) &#125; if len(valid) &lt; len(claim.Status.ReservedFor) &#123; // TODO (#113700): patch claim := claim.DeepCopy() claim.Status.ReservedFor = valid _, err := ec.kubeClient.ResourceV1alpha2().ResourceClaims(claim.Namespace).UpdateStatus(ctx, claim, metav1.UpdateOptions&#123;&#125;) if err != nil &#123; return err &#125; &#125; return nil&#125; REF:1.https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/dynamic-resource-allocation/2.pkg/controller/resourceclaim/controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之ttl]]></title>
    <url>%2F2023%2F06%2F17%2Fkube-controller%E4%B9%8Bttl%2F</url>
    <content type="text"><![CDATA[在k8s中TTLController负责根据集群大小在Node上设置ttl annotations。TTL annotations告诉kubelet在重新请求APISerever之前可以缓存资源对象(比如: secrets,configmap)多长时间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266// pkg/controller/ttl/ttl_controller.go// Controller sets ttl annotations on nodes, based on cluster size.type Controller struct &#123; kubeClient clientset.Interface // 缓存集群节点信息 nodeStore listers.NodeLister // Nodes that need to be synced. queue workqueue.RateLimitingInterface // Returns true if all underlying informers are synced. hasSynced func() bool lock sync.RWMutex // 集群节点大小 nodeCount int // 期望的TTL desiredTTLSeconds int // 表示目前所处的集群规模大小 // 0, 1, 2, 3, 4 // boundaryStep --&gt; 节点范围 --&gt; desiredTTLSeconds // 0 --&gt; [0,100] 0 // 1 --&gt; [90, 500] 15 // 2 --&gt; [450, 100] 30 // 3 --&gt; [900, 2000] 60 // 4 --&gt; [1800, ~] 300 boundaryStep int&#125;// NewTTLController creates a new TTLControllerfunc NewTTLController(ctx context.Context, nodeInformer informers.NodeInformer, kubeClient clientset.Interface) *Controller &#123; ttlc := &amp;Controller&#123; kubeClient: kubeClient, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "ttlcontroller"), &#125; logger := klog.FromContext(ctx) nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; ttlc.addNode(logger, obj) &#125;, UpdateFunc: func(old, newObj interface&#123;&#125;) &#123; ttlc.updateNode(logger, old, newObj) &#125;, DeleteFunc: ttlc.deleteNode, &#125;) ttlc.nodeStore = listers.NewNodeLister(nodeInformer.Informer().GetIndexer()) ttlc.hasSynced = nodeInformer.Informer().HasSynced return ttlc&#125;type ttlBoundary struct &#123; sizeMin int sizeMax int ttlSeconds int&#125;// 定义了不同节点范围的集群下的TTLvar ( ttlBoundaries = []ttlBoundary&#123; &#123;sizeMin: 0, sizeMax: 100, ttlSeconds: 0&#125;, &#123;sizeMin: 90, sizeMax: 500, ttlSeconds: 15&#125;, &#123;sizeMin: 450, sizeMax: 1000, ttlSeconds: 30&#125;, &#123;sizeMin: 900, sizeMax: 2000, ttlSeconds: 60&#125;, &#123;sizeMin: 1800, sizeMax: math.MaxInt32, ttlSeconds: 300&#125;, &#125;)// Run begins watching and syncing.func (ttlc *Controller) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() defer ttlc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting TTL controller") defer logger.Info("Shutting down TTL controller") if !cache.WaitForNamedCacheSync("TTL", ctx.Done(), ttlc.hasSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, ttlc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (ttlc *Controller) addNode(logger klog.Logger, obj interface&#123;&#125;) &#123; node, ok := obj.(*v1.Node) if !ok &#123; utilruntime.HandleError(fmt.Errorf("unexpected object type: %v", obj)) return &#125; func() &#123; ttlc.lock.Lock() defer ttlc.lock.Unlock() // 集群节点数加1 ttlc.nodeCount++ if ttlc.nodeCount &gt; ttlBoundaries[ttlc.boundaryStep].sizeMax &#123; // 超过当前规模的最大值，则加1进入下一规模，ttlSeconds相应的也会增加 ttlc.boundaryStep++ ttlc.desiredTTLSeconds = ttlBoundaries[ttlc.boundaryStep].ttlSeconds &#125; &#125;() // 入队操作 ttlc.enqueueNode(logger, node)&#125;func (ttlc *Controller) updateNode(logger klog.Logger, _, newObj interface&#123;&#125;) &#123; node, ok := newObj.(*v1.Node) if !ok &#123; utilruntime.HandleError(fmt.Errorf("unexpected object type: %v", newObj)) return &#125; // Processing all updates of nodes guarantees that we will update // the ttl annotation, when cluster size changes. // We are relying on the fact that Kubelet is updating node status // every 10s (or generally every X seconds), which means that whenever // required, its ttl annotation should be updated within that period. ttlc.enqueueNode(logger, node)&#125;func (ttlc *Controller) deleteNode(obj interface&#123;&#125;) &#123; _, ok := obj.(*v1.Node) if !ok &#123; tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok &#123; utilruntime.HandleError(fmt.Errorf("unexpected object type: %v", obj)) return &#125; _, ok = tombstone.Obj.(*v1.Node) if !ok &#123; utilruntime.HandleError(fmt.Errorf("unexpected object types: %v", obj)) return &#125; &#125; func() &#123; ttlc.lock.Lock() defer ttlc.lock.Unlock() // 节点减1 ttlc.nodeCount-- if ttlc.nodeCount &lt; ttlBoundaries[ttlc.boundaryStep].sizeMin &#123; // 如果小于当前规模的最小值，则减1进入回到上一规模，ttlSeconds相应的也会减小 ttlc.boundaryStep-- ttlc.desiredTTLSeconds = ttlBoundaries[ttlc.boundaryStep].ttlSeconds &#125; &#125;() // 这里我们不再对节点进行处理，因为节点已经不存在了&#125;// 入队操作func (ttlc *Controller) enqueueNode(logger klog.Logger, node *v1.Node) &#123; key, err := controller.KeyFunc(node) if err != nil &#123; logger.Error(nil, "Couldn't get key for object", "object", klog.KObj(node)) return &#125; ttlc.queue.Add(key)&#125;func (ttlc *Controller) worker(ctx context.Context) &#123; for ttlc.processItem(ctx) &#123; &#125;&#125;func (ttlc *Controller) processItem(ctx context.Context) bool &#123; key, quit := ttlc.queue.Get() if quit &#123; return false &#125; defer ttlc.queue.Done(key) // 这里就是调谐函数 err := ttlc.updateNodeIfNeeded(ctx, key.(string)) if err == nil &#123; ttlc.queue.Forget(key) return true &#125; ttlc.queue.AddRateLimited(key) utilruntime.HandleError(err) return true&#125;func (ttlc *Controller) getDesiredTTLSeconds() int &#123; ttlc.lock.RLock() defer ttlc.lock.RUnlock() return ttlc.desiredTTLSeconds&#125;func getIntFromAnnotation(ctx context.Context, node *v1.Node, annotationKey string) (int, bool) &#123; if node.Annotations == nil &#123; return 0, false &#125; annotationValue, ok := node.Annotations[annotationKey] if !ok &#123; return 0, false &#125; intValue, err := strconv.Atoi(annotationValue) if err != nil &#123; logger := klog.FromContext(ctx) logger.Info("Could not convert the value with annotation key for the node", "annotationValue", annotationValue, "annotationKey", annotationKey, "node", klog.KObj(node)) return 0, false &#125; return intValue, true&#125;func setIntAnnotation(node *v1.Node, annotationKey string, value int) &#123; if node.Annotations == nil &#123; node.Annotations = make(map[string]string) &#125; node.Annotations[annotationKey] = strconv.Itoa(value)&#125;func (ttlc *Controller) patchNodeWithAnnotation(ctx context.Context, node *v1.Node, annotationKey string, value int) error &#123; oldData, err := json.Marshal(node) if err != nil &#123; return err &#125; setIntAnnotation(node, annotationKey, value) newData, err := json.Marshal(node) if err != nil &#123; return err &#125; patchBytes, err := strategicpatch.CreateTwoWayMergePatch(oldData, newData, &amp;v1.Node&#123;&#125;) if err != nil &#123; return err &#125; _, err = ttlc.kubeClient.CoreV1().Nodes().Patch(ctx, node.Name, types.StrategicMergePatchType, patchBytes, metav1.PatchOptions&#123;&#125;) logger := klog.FromContext(ctx) if err != nil &#123; logger.V(2).Info("Failed to change ttl annotation for node", "node", klog.KObj(node), "err", err) return err &#125; logger.V(2).Info("Changed ttl annotation", "node", klog.KObj(node), "TTL", time.Duration(value)*time.Second) return nil&#125;func (ttlc *Controller) updateNodeIfNeeded(ctx context.Context, key string) error &#123; node, err := ttlc.nodeStore.Get(key) if err != nil &#123; if apierrors.IsNotFound(err) &#123; return nil &#125; return err &#125; // 预期的TTL desiredTTL := ttlc.getDesiredTTLSeconds() // 当前的TTL currentTTL, ok := getIntFromAnnotation(ctx, node, v1.ObjectTTLAnnotationKey) if ok &amp;&amp; currentTTL == desiredTTL &#123; return nil &#125; // 更新TTL return ttlc.patchNodeWithAnnotation(ctx, node.DeepCopy(), v1.ObjectTTLAnnotationKey, desiredTTL)&#125; REF:1.pkg/controller/ttl/ttl_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之daemonset]]></title>
    <url>%2F2023%2F06%2F16%2Fkube-controller%E4%B9%8Bdaemonset%2F</url>
    <content type="text"><![CDATA[Kubernetes中的DaemonSet Controller是负责管理DaemonSet资源的控制器，确保在集群中的每个节点上都运行指定数量的Pod副本。DaemonSet Controller是一种类型的控制器，用于在每个节点上运行一个Pod副本，以便在整个集群中覆盖所有节点。DaemonSet通常用于运行守护进程、日志收集器、监控代理等任务。 通过DaemonSet Controller，Kubernetes能够实现在整个集群中自动管理和部署Pod副本的能力，确保每个节点都运行所需的应用程序或服务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334// pkg/controller/daemon/daemon_controller.goconst ( // BurstReplicas is a rate limiter for booting pods on a lot of pods. // The value of 250 is chosen b/c values that are too high can cause registry DoS issues. BurstReplicas = 250 // StatusUpdateRetries limits the number of retries if sending a status update to API server fails. StatusUpdateRetries = 1 // BackoffGCInterval is the time that has to pass before next iteration of backoff GC is run BackoffGCInterval = 1 * time.Minute)// 定义daemonset事件原因const ( // SelectingAllReason 表示daemonset没有设置Selector SelectingAllReason = "SelectingAll" // FailedPlacementReason 表示pod不能调度到对应的节点 FailedPlacementReason = "FailedPlacement" // FailedDaemonPodReason pod的状态是"Failed" FailedDaemonPodReason = "FailedDaemonPod" // SucceededDaemonPodReason 成功拉取对应的pod SucceededDaemonPodReason = "SucceededDaemonPod")type DaemonSetsController struct &#123; kubeClient clientset.Interface eventBroadcaster record.EventBroadcaster eventRecorder record.EventRecorder podControl controller.PodControlInterface crControl controller.ControllerRevisionControlInterface // An dsc is temporarily suspended after creating/deleting these many replicas. // It resumes normal action after observing the watch events for them. // burstReplicas 默认值为 250 即每个 syncLoop 中创建或者删除的 pod 数最多为 250 个 burstReplicas int // To allow injection of syncDaemonSet for testing. syncHandler func(ctx context.Context, dsKey string) error // used for unit testing enqueueDaemonSet func(ds *apps.DaemonSet) // A TTLCache of pod creates/deletes each ds expects to see expectations controller.ControllerExpectationsInterface // dsLister can list/get daemonsets from the shared informer's store dsLister appslisters.DaemonSetLister // dsStoreSynced returns true if the daemonset store has been synced at least once. // Added as a member to the struct to allow injection for testing. dsStoreSynced cache.InformerSynced // historyLister get list/get history from the shared informers's store historyLister appslisters.ControllerRevisionLister // historyStoreSynced returns true if the history store has been synced at least once. // Added as a member to the struct to allow injection for testing. historyStoreSynced cache.InformerSynced // podLister get list/get pods from the shared informers's store podLister corelisters.PodLister // podStoreSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. podStoreSynced cache.InformerSynced // nodeLister can list/get nodes from the shared informer's store nodeLister corelisters.NodeLister // nodeStoreSynced returns true if the node store has been synced at least once. // Added as a member to the struct to allow injection for testing. nodeStoreSynced cache.InformerSynced // DaemonSet keys that need to be synced. queue workqueue.RateLimitingInterface failedPodsBackoff *flowcontrol.Backoff&#125;// 创建DaemonSetControllerfunc NewDaemonSetsController( ctx context.Context, daemonSetInformer appsinformers.DaemonSetInformer, historyInformer appsinformers.ControllerRevisionInformer, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, kubeClient clientset.Interface, failedPodsBackoff *flowcontrol.Backoff,) (*DaemonSetsController, error) &#123; eventBroadcaster := record.NewBroadcaster() logger := klog.FromContext(ctx) dsc := &amp;DaemonSetsController&#123; kubeClient: kubeClient, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "daemonset-controller"&#125;), podControl: controller.RealPodControl&#123; KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "daemonset-controller"&#125;), &#125;, crControl: controller.RealControllerRevisionControl&#123; KubeClient: kubeClient, &#125;, burstReplicas: BurstReplicas, expectations: controller.NewControllerExpectations(), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "daemonset"), &#125; // daemonSetInformer 监听daemonSet变化 daemonSetInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; dsc.addDaemonset(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dsc.updateDaemonset(logger, oldObj, newObj) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; dsc.deleteDaemonset(logger, obj) &#125;, &#125;) dsc.dsLister = daemonSetInformer.Lister() dsc.dsStoreSynced = daemonSetInformer.Informer().HasSynced // historyInformer 监听history变化 historyInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; dsc.addHistory(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dsc.updateHistory(logger, oldObj, newObj) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; dsc.deleteHistory(logger, obj) &#125;, &#125;) dsc.historyLister = historyInformer.Lister() dsc.historyStoreSynced = historyInformer.Informer().HasSynced // 监听pod事件creation/deletion. 我们监听pod事件的原因在于我们不想创建或删除更多的pod直到满足expectations. podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; dsc.addPod(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dsc.updatePod(logger, oldObj, newObj) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; dsc.deletePod(logger, obj) &#125;, &#125;) dsc.podLister = podInformer.Lister() dsc.podStoreSynced = podInformer.Informer().HasSynced // nodeInformer 监听node事件 nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; dsc.addNode(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dsc.updateNode(logger, oldObj, newObj) &#125;, &#125;, ) dsc.nodeStoreSynced = nodeInformer.Informer().HasSynced dsc.nodeLister = nodeInformer.Lister() // 设置调谐函数 dsc.syncHandler = dsc.syncDaemonSet dsc.enqueueDaemonSet = dsc.enqueue dsc.failedPodsBackoff = failedPodsBackoff return dsc, nil&#125;func (dsc *DaemonSetsController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() dsc.eventBroadcaster.StartStructuredLogging(0) dsc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: dsc.kubeClient.CoreV1().Events("")&#125;) defer dsc.eventBroadcaster.Shutdown() defer dsc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting daemon sets controller") defer logger.Info("Shutting down daemon sets controller") if !cache.WaitForNamedCacheSync("daemon sets", ctx.Done(), dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, dsc.runWorker, time.Second) &#125; go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, ctx.Done()) &lt;-ctx.Done()&#125;func (dsc *DaemonSetsController) runWorker(ctx context.Context) &#123; for dsc.processNextWorkItem(ctx) &#123; &#125;&#125;// processNextWorkItem deals with one key off the queue. It returns false when it's time to quit.func (dsc *DaemonSetsController) processNextWorkItem(ctx context.Context) bool &#123; dsKey, quit := dsc.queue.Get() if quit &#123; return false &#125; defer dsc.queue.Done(dsKey) err := dsc.syncHandler(ctx, dsKey.(string)) if err == nil &#123; dsc.queue.Forget(dsKey) return true &#125; utilruntime.HandleError(fmt.Errorf("%v failed with : %v", dsKey, err)) dsc.queue.AddRateLimited(dsKey) return true&#125;// 调谐函数func (dsc *DaemonSetsController) syncDaemonSet(ctx context.Context, key string) error &#123; logger := klog.FromContext(ctx) startTime := dsc.failedPodsBackoff.Clock.Now() defer func() &#123; logger.V(4).Info("Finished syncing daemon set", "daemonset", key, "time", dsc.failedPodsBackoff.Clock.Now().Sub(startTime)) &#125;() // 根据workqueue中的数据解析出namespace和资源名称 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 获取对应的资源daemonset ds, err := dsc.dsLister.DaemonSets(namespace).Get(name) if apierrors.IsNotFound(err) &#123; logger.V(3).Info("Daemon set has been deleted", "daemonset", key) dsc.expectations.DeleteExpectations(key) return nil &#125; if err != nil &#123; return fmt.Errorf("unable to retrieve ds %v from store: %v", key, err) &#125; // 获取所有的node节点 nodeList, err := dsc.nodeLister.List(labels.Everything()) if err != nil &#123; return fmt.Errorf("couldn't get list of nodes when syncing daemon set %#v: %v", ds, err) &#125; everything := metav1.LabelSelector&#123;&#125; // 判断daemonset有没有设置Selector, 如果没有设置则产生事件然后return if reflect.DeepEqual(ds.Spec.Selector, &amp;everything) &#123; dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, "This daemon set is selecting all pods. A non-empty selector is required.") return nil &#125; // Don't process a daemon set until all its creations and deletions have been processed. // For example if daemon set foo asked for 3 new daemon pods in the previous call to manage, // then we do not want to call manage on foo until the daemon pods have been created. dsKey, err := controller.KeyFunc(ds) if err != nil &#123; return fmt.Errorf("couldn't get key for object %#v: %v", ds, err) &#125; // If the DaemonSet is being deleted (either by foreground deletion or // orphan deletion), we cannot be sure if the DaemonSet history objects // it owned still exist -- those history objects can either be deleted // or orphaned. Garbage collector doesn't guarantee that it will delete // DaemonSet pods before deleting DaemonSet history objects, because // DaemonSet history doesn't own DaemonSet pods. We cannot reliably // calculate the status of a DaemonSet being deleted. Therefore, return // here without updating status for the DaemonSet being deleted. if ds.DeletionTimestamp != nil &#123; return nil &#125; // Construct histories of the DaemonSet, and get the hash of current history cur, old, err := dsc.constructHistory(ctx, ds) if err != nil &#123; return fmt.Errorf("failed to construct revisions of DaemonSet: %v", err) &#125; hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] // 通过expectations机制判断daemonset有没有处理完 if !dsc.expectations.SatisfiedExpectations(dsKey) &#123; // 没处理完只更新状态 // Only update status. Don't raise observedGeneration since controller didn't process object of that generation. return dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, false) &#125; err = dsc.updateDaemonSet(ctx, ds, nodeList, hash, dsKey, old) statusErr := dsc.updateDaemonSetStatus(ctx, ds, nodeList, hash, true) switch &#123; case err != nil &amp;&amp; statusErr != nil: // If there was an error, and we failed to update status, // log it and return the original error. logger.Error(statusErr, "Failed to update status", "daemonSet", klog.KObj(ds)) return err case err != nil: return err case statusErr != nil: return statusErr &#125; return nil&#125;func (dsc *DaemonSetsController) updateDaemonSet(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash, key string, old []*apps.ControllerRevision) error &#123; err := dsc.manage(ctx, ds, nodeList, hash) if err != nil &#123; return err &#125; // Process rolling updates if we're ready. // 满足expectations if dsc.expectations.SatisfiedExpectations(key) &#123; switch ds.Spec.UpdateStrategy.Type &#123; case apps.OnDeleteDaemonSetStrategyType: case apps.RollingUpdateDaemonSetStrategyType: err = dsc.rollingUpdate(ctx, ds, nodeList, hash) &#125; if err != nil &#123; return err &#125; &#125; err = dsc.cleanupHistory(ctx, ds, old) if err != nil &#123; return fmt.Errorf("failed to clean up revisions of DaemonSet: %w", err) &#125; return nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172// pkg/controller/daemon/update.gofunc (dsc *DaemonSetsController) rollingUpdate(ctx context.Context, ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error &#123; logger := klog.FromContext(ctx) // 返回一个字典，key为nodeName,值为列表，元素是对应node上需要创建的pod nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ctx, ds) if err != nil &#123; return fmt.Errorf("couldn't get node to daemon pod mapping for daemon set %q: %v", ds.Name, err) &#125; // maxSurge允许的超出期望副本数的最大数量 // maxUnavailable允许的不可用副本的最大数量 maxSurge, maxUnavailable, err := dsc.updatedDesiredNodeCounts(ctx, ds, nodeList, nodeToDaemonPods) if err != nil &#123; return fmt.Errorf("couldn't get unavailable numbers: %v", err) &#125; now := dsc.failedPodsBackoff.Clock.Now() // When not surging, we delete just enough pods to stay under the maxUnavailable limit, if any // are necessary, and let the core loop create new instances on those nodes. // // Assumptions: // * Expect manage loop to allow no more than one pod per node // * Expect manage loop will create new pods // * Expect manage loop will handle failed pods // * Deleted pods do not count as unavailable so that updates make progress when nodes are down // Invariants: // * The number of new pods that are unavailable must be less than maxUnavailable // * A node with an available old pod is a candidate for deletion if it does not violate other invariants if maxSurge == 0 &#123; var numUnavailable int var allowedReplacementPods []string var candidatePodsToDelete []string for nodeName, pods := range nodeToDaemonPods &#123; //findUpdatedPodsOnNode函数检查给定节点上的非删除Pod，如果旧Pod和新Pod至多存在一个，则返回true；如果存在多个Pod，则返回false。 // 在这种情况下，我们可以跳过处理特定节点，并让管理循环在下一次循环中处理多余的Pod。 newPod, oldPod, ok := findUpdatedPodsOnNode(ds, pods, hash) if !ok &#123; // let the manage loop clean up this node, and treat it as an unavailable node logger.V(3).Info("DaemonSet has excess pods on node, skipping to allow the core loop to process", "daemonset", klog.KObj(ds), "node", klog.KRef("", nodeName)) numUnavailable++ continue &#125; switch &#123; case oldPod == nil &amp;&amp; newPod == nil, oldPod != nil &amp;&amp; newPod != nil: // the manage loop will handle creating or deleting the appropriate pod, consider this unavailable numUnavailable++ case newPod != nil: // this pod is up to date, check its availability if !podutil.IsPodAvailable(newPod, ds.Spec.MinReadySeconds, metav1.Time&#123;Time: now&#125;) &#123; // an unavailable new pod is counted against maxUnavailable numUnavailable++ &#125; default: // this pod is old, it is an update candidate switch &#123; // 此pod状态不是Available，则加入allowedReplacementPods列表 case !podutil.IsPodAvailable(oldPod, ds.Spec.MinReadySeconds, metav1.Time&#123;Time: now&#125;): // the old pod isn't available, so it needs to be replaced logger.V(5).Info("DaemonSet pod on node is out of date and not available, allowing replacement", "daemonset", klog.KObj(ds), "pod", klog.KObj(oldPod), "node", klog.KRef("", nodeName)) // record the replacement if allowedReplacementPods == nil &#123; allowedReplacementPods = make([]string, 0, len(nodeToDaemonPods)) &#125; allowedReplacementPods = append(allowedReplacementPods, oldPod.Name) case numUnavailable &gt;= maxUnavailable: // no point considering any other candidates continue default: logger.V(5).Info("DaemonSet pod on node is out of date, this is a candidate to replace", "daemonset", klog.KObj(ds), "pod", klog.KObj(oldPod), "node", klog.KRef("", nodeName)) // record the candidate if candidatePodsToDelete == nil &#123; candidatePodsToDelete = make([]string, 0, maxUnavailable) &#125; // 记录需要被删除的pod candidatePodsToDelete = append(candidatePodsToDelete, oldPod.Name) &#125; &#125; &#125; // use any of the candidates we can, including the allowedReplacemnntPods logger.V(5).Info("DaemonSet allowing replacements", "daemonset", klog.KObj(ds), "replacements", len(allowedReplacementPods), "maxUnavailable", maxUnavailable, "numUnavailable", numUnavailable, "candidates", len(candidatePodsToDelete)) remainingUnavailable := maxUnavailable - numUnavailable if remainingUnavailable &lt; 0 &#123; remainingUnavailable = 0 &#125; if max := len(candidatePodsToDelete); remainingUnavailable &gt; max &#123; remainingUnavailable = max &#125; oldPodsToDelete := append(allowedReplacementPods, candidatePodsToDelete[:remainingUnavailable]...) return dsc.syncNodes(ctx, ds, oldPodsToDelete, nil, hash) &#125; // When surging, we create new pods whenever an old pod is unavailable, and we can create up // to maxSurge extra pods // // Assumptions: // * Expect manage loop to allow no more than two pods per node, one old, one new // * Expect manage loop will create new pods if there are no pods on node // * Expect manage loop will handle failed pods // * Deleted pods do not count as unavailable so that updates make progress when nodes are down // Invariants: // * A node with an unavailable old pod is a candidate for immediate new pod creation // * An old available pod is deleted if a new pod is available // * No more than maxSurge new pods are created for old available pods at any one time // var oldPodsToDelete []string var candidateNewNodes []string var allowedNewNodes []string var numSurge int for nodeName, pods := range nodeToDaemonPods &#123; newPod, oldPod, ok := findUpdatedPodsOnNode(ds, pods, hash) if !ok &#123; // let the manage loop clean up this node, and treat it as a surge node logger.V(3).Info("DaemonSet has excess pods on node, skipping to allow the core loop to process", "daemonset", klog.KObj(ds), "node", klog.KRef("", nodeName)) numSurge++ continue &#125; switch &#123; case oldPod == nil: // we don't need to do anything to this node, the manage loop will handle it case newPod == nil: // this is a surge candidate switch &#123; case !podutil.IsPodAvailable(oldPod, ds.Spec.MinReadySeconds, metav1.Time&#123;Time: now&#125;): // the old pod isn't available, allow it to become a replacement logger.V(5).Info("Pod on node is out of date and not available, allowing replacement", "daemonset", klog.KObj(ds), "pod", klog.KObj(oldPod), "node", klog.KRef("", nodeName)) // record the replacement if allowedNewNodes == nil &#123; allowedNewNodes = make([]string, 0, len(nodeToDaemonPods)) &#125; allowedNewNodes = append(allowedNewNodes, nodeName) case numSurge &gt;= maxSurge: // no point considering any other candidates continue default: logger.V(5).Info("DaemonSet pod on node is out of date, this is a surge candidate", "daemonset", klog.KObj(ds), "pod", klog.KObj(oldPod), "node", klog.KRef("", nodeName)) // record the candidate if candidateNewNodes == nil &#123; candidateNewNodes = make([]string, 0, maxSurge) &#125; candidateNewNodes = append(candidateNewNodes, nodeName) &#125; default: // we have already surged onto this node, determine our state if !podutil.IsPodAvailable(newPod, ds.Spec.MinReadySeconds, metav1.Time&#123;Time: now&#125;) &#123; // we're waiting to go available here numSurge++ continue &#125; // we're available, delete the old pod logger.V(5).Info("DaemonSet pod on node is available, remove old pod", "daemonset", klog.KObj(ds), "newPod", klog.KObj(newPod), "node", nodeName, "oldPod", klog.KObj(oldPod)) oldPodsToDelete = append(oldPodsToDelete, oldPod.Name) &#125; &#125; // use any of the candidates we can, including the allowedNewNodes logger.V(5).Info("DaemonSet allowing replacements", "daemonset", klog.KObj(ds), "replacements", len(allowedNewNodes), "maxSurge", maxSurge, "numSurge", numSurge, "candidates", len(candidateNewNodes)) remainingSurge := maxSurge - numSurge if remainingSurge &lt; 0 &#123; remainingSurge = 0 &#125; if max := len(candidateNewNodes); remainingSurge &gt; max &#123; remainingSurge = max &#125; newNodesToCreate := append(allowedNewNodes, candidateNewNodes[:remainingSurge]...) return dsc.syncNodes(ctx, ds, oldPodsToDelete, newNodesToCreate, hash)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118// pkg/controller/daemon/daemon_controller.go// 在节点上删除或创建对应的podfunc (dsc *DaemonSetsController) syncNodes(ctx context.Context, ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error &#123; // We need to set expectations before creating/deleting pods to avoid race conditions. // 在创建/删除pod前我们需要设置expections以避免竞态情况 logger := klog.FromContext(ctx) dsKey, err := controller.KeyFunc(ds) if err != nil &#123; return fmt.Errorf("couldn't get key for object %#v: %v", ds, err) &#125; createDiff := len(nodesNeedingDaemonPods) deleteDiff := len(podsToDelete) // 设置最大创建数 if createDiff &gt; dsc.burstReplicas &#123; createDiff = dsc.burstReplicas &#125; // 设置最大删除数 if deleteDiff &gt; dsc.burstReplicas &#123; deleteDiff = dsc.burstReplicas &#125; // 设置期望值 dsc.expectations.SetExpectations(dsKey, createDiff, deleteDiff) // error channel to communicate back failures. make the buffer big enough to avoid any blocking errCh := make(chan error, createDiff+deleteDiff) logger.V(4).Info("Nodes needing daemon pods for daemon set, creating", "daemonset", klog.KObj(ds), "needCount", nodesNeedingDaemonPods, "createCount", createDiff) createWait := sync.WaitGroup&#123;&#125; // If the returned error is not nil we have a parse error. // The controller handles this via the hash. generation, err := util.GetTemplateGeneration(ds) if err != nil &#123; generation = nil &#125; template := util.CreatePodTemplate(ds.Spec.Template, generation, hash) // Batch the pod creates. Batch sizes start at SlowStartInitialBatchSize // and double with each successful iteration in a kind of "slow start". // This handles attempts to start large numbers of pods that would // likely all fail with the same error. For example a project with a // low quota that attempts to create a large number of pods will be // prevented from spamming the API service with the pod create requests // after one of its pods fails. Conveniently, this also prevents the // event spam that those failures would generate. batchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff &gt; pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize &#123; errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i &lt; pos+batchSize; i++ &#123; go func(ix int) &#123; defer createWait.Done() podTemplate := template.DeepCopy() // The pod's NodeAffinity will be updated to make sure the Pod is bound // to the target node by default scheduler. It is safe to do so because there // should be no conflicting node affinity with the target node. podTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity( podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix]) err := dsc.podControl.CreatePods(ctx, ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) if err != nil &#123; if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) &#123; // If the namespace is being torn down, we can safely ignore // this error since all subsequent creations will fail. return &#125; &#125; if err != nil &#123; logger.V(2).Info("Failed creation, decrementing expectations for daemon set", "daemonset", klog.KObj(ds)) dsc.expectations.CreationObserved(dsKey) errCh &lt;- err utilruntime.HandleError(err) &#125; &#125;(i) &#125; createWait.Wait() // any skipped pods that we never attempted to start shouldn't be expected. skippedPods := createDiff - (batchSize + pos) if errorCount &lt; len(errCh) &amp;&amp; skippedPods &gt; 0 &#123; logger.V(2).Info("Slow-start failure. Skipping creation pods, decrementing expectations for daemon set", "skippedPods", skippedPods, "daemonset", klog.KObj(ds)) dsc.expectations.LowerExpectations(dsKey, skippedPods, 0) // The skipped pods will be retried later. The next controller resync will // retry the slow start process. break &#125; &#125; logger.V(4).Info("Pods to delete for daemon set, deleting", "daemonset", klog.KObj(ds), "toDeleteCount", podsToDelete, "deleteCount", deleteDiff) deleteWait := sync.WaitGroup&#123;&#125; deleteWait.Add(deleteDiff) for i := 0; i &lt; deleteDiff; i++ &#123; go func(ix int) &#123; defer deleteWait.Done() if err := dsc.podControl.DeletePod(ctx, ds.Namespace, podsToDelete[ix], ds); err != nil &#123; dsc.expectations.DeletionObserved(dsKey) if !apierrors.IsNotFound(err) &#123; logger.V(2).Info("Failed deletion, decremented expectations for daemon set", "daemonset", klog.KObj(ds)) errCh &lt;- err utilruntime.HandleError(err) &#125; &#125; &#125;(i) &#125; deleteWait.Wait() // collect errors if any for proper reporting/retry logic in the controller errors := []error&#123;&#125; close(errCh) for err := range errCh &#123; errors = append(errors, err) &#125; return utilerrors.NewAggregate(errors)&#125; REF:1.pkg/controller/daemon/daemon_controller.go2.pkg/controller/daemon/update.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之resource-quota]]></title>
    <url>%2F2023%2F06%2F15%2Fkube-controller%E4%B9%8Bresource-quota%2F</url>
    <content type="text"><![CDATA[在 Kubernetes 中，ResourceQuota（资源配额）是一种机制，用于对命名空间中的资源使用进行限制和控制。它可以帮助管理员在 Kubernetes 集群中实施资源管理策略，确保不同的命名空间或用户不会滥用资源或超出预定的限制。 ResourceQuota 控制器是 Kubernetes 中的一部分，它负责监控和执行 ResourceQuota 对象的策略。控制器会定期检查每个命名空间的资源使用情况，并与 ResourceQuota 对象中定义的限制进行比较。如果某个命名空间的资源使用超出了限制，控制器将采取相应的措施来限制该命名空间的资源使用。 通过 ResourceQuota，管理员可以限制每个命名空间的 CPU、内存、存储等资源的使用量。它还可以限制 Pod、服务、配置映射等对象的数量。通过定义适当的 ResourceQuota对象，管理员可以确保资源在集群中得到合理的分配和使用。 在 Kubernetes 源码中，ResourceQuota 控制器的实现可以在 pkg/controller/resourcequota 目录下找到。该目录中的文件包含了 ResourceQuota 控制器的业务逻辑和处理逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207// pkg/controller/resourcequota/resource_quota_controller.go// Controller is responsible for tracking quota usage status in the systemtype Controller struct &#123; // Must have authority to list all resources in the system, and update quota status rqClient corev1client.ResourceQuotasGetter // A lister/getter of resource quota objects rqLister corelisters.ResourceQuotaLister // A list of functions that return true when their caches have synced informerSyncedFuncs []cache.InformerSynced // ResourceQuota objects that need to be synchronized queue workqueue.RateLimitingInterface // missingUsageQueue holds objects that are missing the initial usage information missingUsageQueue workqueue.RateLimitingInterface // To allow injection of syncUsage for testing. syncHandler func(ctx context.Context, key string) error // function that controls full recalculation of quota usage resyncPeriod controller.ResyncPeriodFunc // knows how to calculate usage registry quota.Registry // knows how to monitor all the resources tracked by quota and trigger replenishment quotaMonitor *QuotaMonitor // controls the workers that process quotas // this lock is acquired to control write access to the monitors and ensures that all // monitors are synced before the controller can process quotas. workerLock sync.RWMutex&#125;// NewController creates a quota controller with specified optionsfunc NewController(ctx context.Context, options *ControllerOptions) (*Controller, error) &#123; // build the resource quota controller rq := &amp;Controller&#123; rqClient: options.QuotaClient, rqLister: options.ResourceQuotaInformer.Lister(), informerSyncedFuncs: []cache.InformerSynced&#123;options.ResourceQuotaInformer.Informer().HasSynced&#125;, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "resourcequota_primary"), missingUsageQueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "resourcequota_priority"), resyncPeriod: options.ResyncPeriod, registry: options.Registry, &#125; // set the synchronization handler rq.syncHandler = rq.syncResourceQuotaFromKey logger := klog.FromContext(ctx) options.ResourceQuotaInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; rq.addQuota(logger, obj) &#125;, UpdateFunc: func(old, cur interface&#123;&#125;) &#123; // 我们只 oldResourceQuota := old.(*v1.ResourceQuota) curResourceQuota := cur.(*v1.ResourceQuota) if quota.Equals(oldResourceQuota.Spec.Hard, curResourceQuota.Spec.Hard) &#123; return &#125; rq.addQuota(logger, curResourceQuota) &#125;, // This will enter the sync loop and no-op, because the controller has been deleted from the store. // Note that deleting a controller immediately after scaling it to 0 will not work. The recommended // way of achieving this is by performing a `stop` operation on the controller. // 进入sync loop但是不做任何操作，因为controller已经被删除了 DeleteFunc: func(obj interface&#123;&#125;) &#123; rq.enqueueResourceQuota(logger, obj) &#125;, &#125;, rq.resyncPeriod(), ) if options.DiscoveryFunc != nil &#123; qm := &amp;QuotaMonitor&#123; informersStarted: options.InformersStarted, informerFactory: options.InformerFactory, ignoredResources: options.IgnoredResourcesFunc(), resourceChanges: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "resource_quota_controller_resource_changes"), resyncPeriod: options.ReplenishmentResyncPeriod, replenishmentFunc: rq.replenishQuota, registry: rq.registry, updateFilter: options.UpdateFilter, &#125; rq.quotaMonitor = qm // do initial quota monitor setup. If we have a discovery failure here, it's ok. We'll discover more resources when a later sync happens. // 用于获取可以进行配额限制的资源列表 resources, err := GetQuotableResources(options.DiscoveryFunc) if discovery.IsGroupDiscoveryFailedError(err) &#123; utilruntime.HandleError(fmt.Errorf("initial discovery check failure, continuing and counting on future sync update: %v", err)) &#125; else if err != nil &#123; return nil, err &#125; if err = qm.SyncMonitors(ctx, resources); err != nil &#123; utilruntime.HandleError(fmt.Errorf("initial monitor sync has error: %v", err)) &#125; // only start quota once all informers synced rq.informerSyncedFuncs = append(rq.informerSyncedFuncs, func() bool &#123; return qm.IsSynced(ctx) &#125;) &#125; return rq, nil&#125;type ControllerOptions struct &#123; // Must have authority to list all quotas, and update quota status QuotaClient corev1client.ResourceQuotasGetter // Shared informer for resource quotas ResourceQuotaInformer coreinformers.ResourceQuotaInformer // Controls full recalculation of quota usage ResyncPeriod controller.ResyncPeriodFunc // Maintains evaluators that know how to calculate usage for group resource Registry quota.Registry // Discover list of supported resources on the server. DiscoveryFunc NamespacedResourcesFunc // A function that returns the list of resources to ignore IgnoredResourcesFunc func() map[schema.GroupResource]struct&#123;&#125; // InformersStarted knows if informers were started. InformersStarted &lt;-chan struct&#123;&#125; // InformerFactory interfaces with informers. InformerFactory informerfactory.InformerFactory // Controls full resync of objects monitored for replenishment. ReplenishmentResyncPeriod controller.ResyncPeriodFunc // Filters update events so we only enqueue the ones where we know quota will change UpdateFilter UpdateFilter&#125;// syncHandlerfunc (rq *Controller) syncResourceQuotaFromKey(ctx context.Context, key string) (err error) &#123; startTime := time.Now() logger := klog.FromContext(ctx) logger = klog.LoggerWithValues(logger, "key", key) defer func() &#123; logger.V(4).Info("Finished syncing resource quota", "key", key, "duration", time.Since(startTime)) &#125;() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; resourceQuota, err := rq.rqLister.ResourceQuotas(namespace).Get(name) if errors.IsNotFound(err) &#123; logger.Info("Resource quota has been deleted", "key", key) return nil &#125; if err != nil &#123; logger.Error(err, "Unable to retrieve resource quota from store", "key", key) return err &#125; return rq.syncResourceQuota(ctx, resourceQuota)&#125;// syncResourceQuota runs a complete sync of resource quota status across all known kinds// 计算资源使用率并向api-serever上报func (rq *Controller) syncResourceQuota(ctx context.Context, resourceQuota *v1.ResourceQuota) (err error) &#123; // quota is dirty if any part of spec hard limits differs from the status hard limits statusLimitsDirty := !apiequality.Semantic.DeepEqual(resourceQuota.Spec.Hard, resourceQuota.Status.Hard) // dirty tracks if the usage status differs from the previous sync, // if so, we send a new usage with latest status // if this is our first sync, it will be dirty by default, since we need track usage dirty := statusLimitsDirty || resourceQuota.Status.Hard == nil || resourceQuota.Status.Used == nil used := v1.ResourceList&#123;&#125; if resourceQuota.Status.Used != nil &#123; used = quota.Add(v1.ResourceList&#123;&#125;, resourceQuota.Status.Used) &#125; hardLimits := quota.Add(v1.ResourceList&#123;&#125;, resourceQuota.Spec.Hard) var errs []error newUsage, err := quota.CalculateUsage(resourceQuota.Namespace, resourceQuota.Spec.Scopes, hardLimits, rq.registry, resourceQuota.Spec.ScopeSelector) if err != nil &#123; // if err is non-nil, remember it to return, but continue updating status with any resources in newUsage errs = append(errs, err) &#125; for key, value := range newUsage &#123; used[key] = value &#125; // ensure set of used values match those that have hard constraints hardResources := quota.ResourceNames(hardLimits) used = quota.Mask(used, hardResources) // Create a usage object that is based on the quota resource version that will handle updates // by default, we preserve the past usage observation, and set hard to the current spec usage := resourceQuota.DeepCopy() usage.Status = v1.ResourceQuotaStatus&#123; Hard: hardLimits, Used: used, &#125; dirty = dirty || !quota.Equals(usage.Status.Used, resourceQuota.Status.Used) // there was a change observed by this controller that requires we update quota if dirty &#123; _, err = rq.rqClient.ResourceQuotas(usage.Namespace).UpdateStatus(ctx, usage, metav1.UpdateOptions&#123;&#125;) if err != nil &#123; errs = append(errs, err) &#125; &#125; return utilerrors.NewAggregate(errs)&#125; REF: 1.resource-quotas2.quota-api-object3.admission_control_resource_quota4.pkg/controller/resourcequota/resource_quota_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s controller-expectation]]></title>
    <url>%2F2023%2F06%2F15%2Fk8s-controller-expectation%2F</url>
    <content type="text"><![CDATA[Controller Expectation在Kubernetes中，Controller Expectation（控制器期望机制）是一种用于协调多个控制器对共享资源的操作的机制。它通过维护每个控制器对资源的期望状态，以确保只有一个控制器能够成功执行操作。 该机制的实现是通过Expectations对象来管理的。Expectations对象是一个保存了资源键（Resource Key）和期望计数（Expectation Count）之间映射关系的数据结构。每个控制器都可以通过Expectations对象来声明它对某个资源键的期望计数。 当控制器需要执行对资源的操作时，它会先检查Expectations对象中对应资源键的期望计数是否满足要求。如果满足，则该控制器可以执行操作，并将期望计数减一。如果不满足，则跳过对应的操作 通过使用Controller Expectation机制，可以有效地协调多个控制器并发对共享资源进行操作，避免了冲突和竞争条件的发生。这种机制能够确保每个控制器在执行操作之前都满足了其期望的前置条件，从而保证了系统的一致性和正确性。 Controller Expectation实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152// pkg/controller/controller_utils.go// ControllerExpectations: &#123;// controller1: expects 2 adds in 2 minutes// controller2: expects 2 dels in 2 minutes// controller3: expects -1 adds in 2 minutes =&gt; 表示已经达到期望状态// &#125;//// 实现：// ControlleeExpectation: 通过原子计数来追踪controllee的creation/deletion// ControllerExpectationsStore: TTLStore + 每个controller设置ControlleeExpectation//// 一个controller不会再执行sync直到它的expections状态为fulfilled或expire// 没有设置expections的Controller将被唤醒以处理每个匹配的控制对象// 定义了需要实现的方法type ControllerExpectationsInterface interface &#123; GetExpectations(controllerKey string) (*ControlleeExpectations, bool, error) SatisfiedExpectations(controllerKey string) bool DeleteExpectations(controllerKey string) SetExpectations(controllerKey string, add, del int) error ExpectCreations(controllerKey string, adds int) error ExpectDeletions(controllerKey string, dels int) error CreationObserved(controllerKey string) DeletionObserved(controllerKey string) RaiseExpectations(controllerKey string, add, del int) LowerExpectations(controllerKey string, add, del int)&#125;// ControllerExpectations is a cache mapping controllers to what they expect to see before being woken up for a sync.type ControllerExpectations struct &#123; cache.Store&#125;func (r *ControllerExpectations) GetExpectations(controllerKey string) (*ControlleeExpectations, bool, error) &#123; exp, exists, err := r.GetByKey(controllerKey) if err == nil &amp;&amp; exists &#123; return exp.(*ControlleeExpectations), true, nil &#125; return nil, false, err&#125;// DeleteExpectations deletes the expectations of the given controller from the TTLStore.func (r *ControllerExpectations) DeleteExpectations(controllerKey string) &#123; if exp, exists, err := r.GetByKey(controllerKey); err == nil &amp;&amp; exists &#123; if err := r.Delete(exp); err != nil &#123; klog.V(2).Infof("Error deleting expectations for controller %v: %v", controllerKey, err) &#125; &#125;&#125;func (r *ControllerExpectations) SatisfiedExpectations(controllerKey string) bool &#123; if exp, exists, err := r.GetExpectations(controllerKey); exists &#123; if exp.Fulfilled() &#123; klog.V(4).Infof("Controller expectations fulfilled %#v", exp) return true &#125; else if exp.isExpired() &#123; klog.V(4).Infof("Controller expectations expired %#v", exp) return true &#125; else &#123; klog.V(4).Infof("Controller still waiting on expectations %#v", exp) return false &#125; &#125; else if err != nil &#123; klog.V(2).Infof("Error encountered while checking expectations %#v, forcing sync", err) &#125; else &#123; // When a new controller is created, it doesn't have expectations. // When it doesn't see expected watch events for &gt; TTL, the expectations expire. // - In this case it wakes up, creates/deletes controllees, and sets expectations again. // When it has satisfied expectations and no controllees need to be created/destroyed &gt; TTL, the expectations expire. // - In this case it continues without setting expectations till it needs to create/delete controllees. klog.V(4).Infof("Controller %v either never recorded expectations, or the ttl expired.", controllerKey) &#125; // Trigger a sync if we either encountered and error (which shouldn't happen since we're // getting from local store) or this controller hasn't established expectations. return true&#125;func (exp *ControlleeExpectations) isExpired() bool &#123; return clock.RealClock&#123;&#125;.Since(exp.timestamp) &gt; ExpectationsTimeout&#125;// SetExpectations registers new expectations for the given controller. Forgets existing expectations.func (r *ControllerExpectations) SetExpectations(controllerKey string, add, del int) error &#123; exp := &amp;ControlleeExpectations&#123;add: int64(add), del: int64(del), key: controllerKey, timestamp: clock.RealClock&#123;&#125;.Now()&#125; klog.V(4).Infof("Setting expectations %#v", exp) return r.Add(exp)&#125;func (r *ControllerExpectations) ExpectCreations(controllerKey string, adds int) error &#123; return r.SetExpectations(controllerKey, adds, 0)&#125;func (r *ControllerExpectations) ExpectDeletions(controllerKey string, dels int) error &#123; return r.SetExpectations(controllerKey, 0, dels)&#125;// Decrements the expectation counts of the given controller.func (r *ControllerExpectations) LowerExpectations(controllerKey string, add, del int) &#123; if exp, exists, err := r.GetExpectations(controllerKey); err == nil &amp;&amp; exists &#123; exp.Add(int64(-add), int64(-del)) // The expectations might've been modified since the update on the previous line. klog.V(4).Infof("Lowered expectations %#v", exp) &#125;&#125;// Increments the expectation counts of the given controller.func (r *ControllerExpectations) RaiseExpectations(controllerKey string, add, del int) &#123; if exp, exists, err := r.GetExpectations(controllerKey); err == nil &amp;&amp; exists &#123; exp.Add(int64(add), int64(del)) // The expectations might've been modified since the update on the previous line. klog.V(4).Infof("Raised expectations %#v", exp) &#125;&#125;// CreationObserved atomically decrements the `add` expectation count of the given controller.func (r *ControllerExpectations) CreationObserved(controllerKey string) &#123; r.LowerExpectations(controllerKey, 1, 0)&#125;// DeletionObserved atomically decrements the `del` expectation count of the given controller.func (r *ControllerExpectations) DeletionObserved(controllerKey string) &#123; r.LowerExpectations(controllerKey, 0, 1)&#125;// ControlleeExpectations track controllee creates/deletes.type ControlleeExpectations struct &#123; // See: https://golang.org/pkg/sync/atomic/ for more information add int64 del int64 key string timestamp time.Time&#125;// Add increments the add and del counters.func (e *ControlleeExpectations) Add(add, del int64) &#123; atomic.AddInt64(&amp;e.add, add) atomic.AddInt64(&amp;e.del, del)&#125;// Fulfilled returns true if this expectation has been fulfilled.func (e *ControlleeExpectations) Fulfilled() bool &#123; // TODO: think about why this line being atomic doesn't matter return atomic.LoadInt64(&amp;e.add) &lt;= 0 &amp;&amp; atomic.LoadInt64(&amp;e.del) &lt;= 0&#125;// GetExpectations returns the add and del expectations of the controllee.func (e *ControlleeExpectations) GetExpectations() (int64, int64) &#123; return atomic.LoadInt64(&amp;e.add), atomic.LoadInt64(&amp;e.del)&#125; expection在replicaSetController中的使用1234567891011121314151617181920212223242526272829// pkg/controller/replicaset/replica_set.gofunc NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.PodControlInterface, eventBroadcaster record.EventBroadcaster) *ReplicaSetController &#123; rsc := &amp;ReplicaSetController&#123; GroupVersionKind: gvk, kubeClient: kubeClient, podControl: podControl, eventBroadcaster: eventBroadcaster, burstReplicas: burstReplicas, // 设置expectations expectations: controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), queueName), &#125; ...&#125;// pkg/controller/controller_utils.gofunc NewUIDTrackingControllerExpectations(ce ControllerExpectationsInterface) *UIDTrackingControllerExpectations &#123; return &amp;UIDTrackingControllerExpectations&#123;ControllerExpectationsInterface: ce, uidStore: cache.NewStore(UIDSetKeyFunc)&#125;&#125;// 实现了ControllerExpectationsInterface接口type UIDTrackingControllerExpectations struct &#123; ControllerExpectationsInterface uidStoreLock sync.Mutex // Store used for the UIDs associated with any expectation tracked via the // ControllerExpectationsInterface. uidStore cache.Store&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166// pkg/controller/replicaset/replica_set.gofunc (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error &#123; ... rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if apierrors.IsNotFound(err) &#123; klog.FromContext(ctx).V(4).Info("deleted", "kind", rsc.Kind, "key", key) // 删除对应的key rsc.expectations.DeleteExpectations(key) return nil &#125; ... rsNeedsSync := rsc.expectations.SatisfiedExpectations(key) ... var manageReplicasErr error // 如果rsNeedsSync不为true,说明有Controller正在执行对应调谐操作 // 这时当时Controller则不会执行manageReplicas,也就是真正的调谐操作 if rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil &#123; manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs) &#125; ...&#125;func (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error &#123; ... if diff &lt; 0 &#123; diff *= -1 if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; // TODO: Track UIDs of creates just like deletes. The problem currently // is we'd need to wait on the result of a create to record the pod's // UID, which would require locking *across* the create, which will turn // into a performance bottleneck. We should generate a UID for the pod // beforehand and store it via ExpectCreations. // 添加期望数diff rsc.expectations.ExpectCreations(rsKey, diff) ... if skippedPods := diff - successfulCreations; skippedPods &gt; 0 &#123; klog.FromContext(ctx).V(2).Info("Slow-start failure. Skipping creation of pods, decrementing expectations", "podsSkipped", skippedPods, "kind", rsc.Kind, "replicaSet", klog.KObj(rs)) for i := 0; i &lt; skippedPods; i++ &#123; // Decrement the expected number of creates because the informer won't observe this pod // 忽略的pod说明不是期望的，这里要减去 rsc.expectations.CreationObserved(rsKey) &#125; &#125; &#125; else if diff &gt; 0 &#123; // 说明需要缩容 if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; klog.FromContext(ctx).V(2).Info("Too many replicas", "replicaSet", klog.KObj(rs), "need", *(rs.Spec.Replicas), "deleting", diff) relatedPods, err := rsc.getIndirectlyRelatedPods(klog.FromContext(ctx), rs) utilruntime.HandleError(err) // Choose which Pods to delete, preferring those in earlier phases of startup. podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff) // Snapshot the UIDs (ns/name) of the pods we're expecting to see // deleted, so we know to record their expectations exactly once either // when we see it as an update of the deletion timestamp, or as a delete. // Note that if the labels on a pod/rs change in a way that the pod gets // orphaned, the rs will only wake up after the expectations have // expired even if other pods are deleted. rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete)) errCh := make(chan error, diff) var wg sync.WaitGroup wg.Add(diff) for _, pod := range podsToDelete &#123; go func(targetPod *v1.Pod) &#123; defer wg.Done() if err := rsc.podControl.DeletePod(ctx, rs.Namespace, targetPod.Name, rs); err != nil &#123; // Decrement the expected number of deletes because the informer won't observe this deletion podKey := controller.PodKey(targetPod) // 删除了一个pod, expectations上也要减去 rsc.expectations.DeletionObserved(rsKey, podKey) if !apierrors.IsNotFound(err) &#123; klog.FromContext(ctx).V(2).Info("Failed to delete pod, decremented expectations", "pod", podKey, "kind", rsc.Kind, "replicaSet", klog.KObj(rs)) errCh &lt;- err &#125; &#125; &#125;(pod) &#125; wg.Wait() select &#123; case err := &lt;-errCh: // all errors have been reported before and they're likely to be the same, so we'll only return the first one we hit. if err != nil &#123; return err &#125; default: &#125; &#125;&#125;func (rsc *ReplicaSetController) deleteRS(obj interface&#123;&#125;) &#123; rs, ok := obj.(*apps.ReplicaSet) ... // Delete expectations for the ReplicaSet so if we create a new one with the same name it starts clean rsc.expectations.DeleteExpectations(key) rsc.queue.Add(key)&#125;// When a pod is created, enqueue the replica set that manages it and update its expectations.func (rsc *ReplicaSetController) addPod(obj interface&#123;&#125;) &#123; pod := obj.(*v1.Pod) if pod.DeletionTimestamp != nil &#123; // on a restart of the controller manager, it's possible a new pod shows up in a state that // is already pending deletion. Prevent the pod from being a creation observation. rsc.deletePod(pod) return &#125; // If it has a ControllerRef, that's all that matters. if controllerRef := metav1.GetControllerOf(pod); controllerRef != nil &#123; rs := rsc.resolveControllerRef(pod.Namespace, controllerRef) if rs == nil &#123; return &#125; rsKey, err := controller.KeyFunc(rs) if err != nil &#123; return &#125; klog.V(4).Infof("Pod %s created: %#v.", pod.Name, pod) // pod创建成功，需要减去pod的期望数 rsc.expectations.CreationObserved(rsKey) rsc.queue.Add(rsKey) return &#125; ...&#125;// When a pod is deleted, enqueue the replica set that manages the pod and update its expectations.// obj could be an *v1.Pod, or a DeletionFinalStateUnknown marker item.func (rsc *ReplicaSetController) deletePod(obj interface&#123;&#125;) &#123; pod, ok := obj.(*v1.Pod) ... controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil &#123; // No controller should care about orphans being deleted. return &#125; rs := rsc.resolveControllerRef(pod.Namespace, controllerRef) if rs == nil &#123; return &#125; rsKey, err := controller.KeyFunc(rs) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("couldn't get key for object %#v: %v", rs, err)) return &#125; klog.V(4).Infof("Pod %s/%s deleted through %v, timestamp %+v: %#v.", pod.Namespace, pod.Name, utilruntime.GetCaller(), pod.DeletionTimestamp, pod) // 同样当删除了一个pod,期望删除的pod数也更新 rsc.expectations.DeletionObserved(rsKey, controller.PodKey(pod)) rsc.queue.Add(rsKey)&#125; REF:1.pkg/controller/controller_utils.go2.pkg/controller/replicaset/replica_set.go3.pkg/controller/controller_utils.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码中的一些实现]]></title>
    <url>%2F2023%2F06%2F15%2Fk8s%E6%BA%90%E7%A0%81%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文主要记录k8s中源码中一些函数实现。持续更新。 函数批量调用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// pkg/controller/replicaset/replica_set.go// slowStartBatch 尝试调用fn总共count次，刚开始以较小的并发batchSize调用fn函数(用于检查是否发生错误)// 如果未发生错误则增加batchSize// 将函数进行分批调用，起始值为initialBatchSize和count之间的最小值，在每一批的函数调用是并发执行的// 如果一次批调用都成功了，一下次批调用将会对batchSize翻倍。// 如果在一次批调用过程中发生了错误，会等待当前批次的调用执行完。但余下的批次将不会执行。// 如果initialBatchSize为1，则batchSize变化为// 1, 2, 4, 8, 16...// 返回成功调用fn的次数func slowStartBatch(count int, initialBatchSize int, fn func() error) (int, error) &#123; // remaining统计fn剩余调用的次数 remaining := count successes := 0 for batchSize := integer.IntMin(remaining, initialBatchSize); batchSize &gt; 0; batchSize = integer.IntMin(2*batchSize, remaining) &#123; // errCh用于统计调用失败的次数 errCh := make(chan error, batchSize) var wg sync.WaitGroup wg.Add(batchSize) for i := 0; i &lt; batchSize; i++ &#123; go func() &#123; defer wg.Done() if err := fn(); err != nil &#123; errCh &lt;- err &#125; &#125;() &#125; wg.Wait() curSuccesses := batchSize - len(errCh) successes += curSuccesses if len(errCh) &gt; 0 &#123; return successes, &lt;-errCh &#125; // 更新remaining remaining -= batchSize &#125; return successes, nil&#125;// vendor/k8s.io/utils/integer/integer.go// IntMin returns the minimum of the paramsfunc IntMin(a, b int) int &#123; if b &lt; a &#123; return b &#125; return a&#125; FindTailLineStartIndex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// pkg/util/tail/tail.go// 返回倒数第n行的起始字节数// If n &lt; 0, 返回文件起始位置// If n &gt;=0, 返回倒数第n行的起始字节数// 如果最后一行没有换行符将不会统计func FindTailLineStartIndex(f io.ReadSeeker, n int64) (int64, error) &#123; if n &lt; 0 &#123; return 0, nil &#125; // 获取文件大小 size, err := f.Seek(0, io.SeekEnd) if err != nil &#123; return 0, err &#125; var left, cnt int64 buf := make([]byte, blockSize) for right := size; right &gt; 0 &amp;&amp; cnt &lt;= n; right -= blockSize &#123; left = right - blockSize if left &lt; 0 &#123; left = 0 buf = make([]byte, right) &#125; if _, err := f.Seek(left, io.SeekStart); err != nil &#123; return 0, err &#125; if _, err := f.Read(buf); err != nil &#123; return 0, err &#125; // 统计行数 cnt += int64(bytes.Count(buf, eol)) &#125; // 假设文件为"hello\nworld\n",则size=12 // cnt = 2, 此时left为0 // 假设n=1,则cnt&gt;n进入循环 for ; cnt &gt; n; cnt-- &#123; // idx=6 idx := bytes.Index(buf, eol) + 1 buf = buf[idx:] left += int64(idx) &#125; // left=6 return left, nil&#125;const ( blockSize = 1024)var ( // eol is the end-of-line sign in the log. eol = []byte&#123;'\n'&#125;) 并行拉取镜像最大并行镜像拉取数量：特性状态： Kubernetes v1.27 [alpha]当 serializeImagePulls 被设置为 false 时，kubelet 默认对同时拉取的最大镜像数量没有限制。 如果你想限制并行镜像拉取的数量，可以在 kubelet 配置中设置字段 maxParallelImagePulls。 当 maxParallelImagePulls 设置为 n 时，只能同时拉取 n 个镜像， 超过 n 的任何镜像都必须等到至少一个正在进行拉取的镜像拉取完成后，才能拉取。 当启用并行镜像拉取时，限制并行镜像拉取的数量可以防止镜像拉取消耗过多的网络带宽或磁盘 I/O。 你可以将 maxParallelImagePulls 设置为大于或等于 1 的正数。 如果将 maxParallelImagePulls 设置为大于等于 2，则必须将 serializeImagePulls 设置为 false。 kubelet 在无效的 maxParallelImagePulls 设置下会启动失败。 1234567891011121314151617181920212223242526272829303132333435363738394041424344// pkg/kubelet/images/puller.gotype parallelImagePuller struct &#123; imageService kubecontainer.ImageService tokens chan struct&#123;&#125;&#125;func newParallelImagePuller(imageService kubecontainer.ImageService, maxParallelImagePulls *int32) imagePuller &#123; if maxParallelImagePulls == nil || *maxParallelImagePulls &lt; 1 &#123; return &amp;parallelImagePuller&#123;imageService, nil&#125; &#125; // tokens的大小为maxParallelImagePulls // maxParallelImagePulls并发的goroutine数 return &amp;parallelImagePuller&#123;imageService, make(chan struct&#123;&#125;, *maxParallelImagePulls)&#125;&#125;func (pip *parallelImagePuller) pullImage(ctx context.Context, spec kubecontainer.ImageSpec, pullSecrets []v1.Secret, pullChan chan&lt;- pullResult, podSandboxConfig *runtimeapi.PodSandboxConfig) &#123; go func() &#123; if pip.tokens != nil &#123; // 每启动一个goroutine拉取镜像，往tokens写入struct&#123;&#125;&#123;&#125; // 如果tokens已经满了，则会阻塞 pip.tokens &lt;- struct&#123;&#125;&#123;&#125; // 镜像拉取成功后，释放空间 defer func() &#123; &lt;-pip.tokens &#125;() &#125; startTime := time.Now() imageRef, err := pip.imageService.PullImage(ctx, spec, pullSecrets, podSandboxConfig) pullChan &lt;- pullResult&#123; imageRef: imageRef, err: err, pullDuration: time.Since(startTime), &#125; &#125;()&#125;func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) &#123; ... pullChan := make(chan pullResult) // 调用pullImage并行拉取镜像 m.puller.pullImage(ctx, spec, pullSecrets, pullChan, podSandboxConfig) // 串行获取镜像拉取结果 imagePullResult := &lt;-pullChan ...&#125; 串行镜像拉取123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// pkg/kubelet/images/puller.go// 可以进入排队的最大镜像拉取请求数const maxImagePullRequests = 10type serialImagePuller struct &#123; imageService kubecontainer.ImageService pullRequests chan *imagePullRequest&#125;func newSerialImagePuller(imageService kubecontainer.ImageService) imagePuller &#123; imagePuller := &amp;serialImagePuller&#123;imageService, make(chan *imagePullRequest, maxImagePullRequests)&#125; // 启动一个不会退出的goroutine, 不断的从pullRequests中取出信息，然后拉取镜像 go wait.Until(imagePuller.processImagePullRequests, time.Second, wait.NeverStop) return imagePuller&#125;type imagePullRequest struct &#123; ctx context.Context spec kubecontainer.ImageSpec pullSecrets []v1.Secret pullChan chan&lt;- pullResult podSandboxConfig *runtimeapi.PodSandboxConfig&#125;func (sip *serialImagePuller) pullImage(ctx context.Context, spec kubecontainer.ImageSpec, pullSecrets []v1.Secret, pullChan chan&lt;- pullResult, podSandboxConfig *runtimeapi.PodSandboxConfig) &#123; // 这里只是把镜像拉取请求发送到pullRequests,实现的拉取镜像动作是在processImagePullRequests sip.pullRequests &lt;- &amp;imagePullRequest&#123; ctx: ctx, spec: spec, pullSecrets: pullSecrets, pullChan: pullChan, podSandboxConfig: podSandboxConfig, &#125;&#125;func (sip *serialImagePuller) processImagePullRequests() &#123; // 遍历pullRequests,串行拉取镜像 for pullRequest := range sip.pullRequests &#123; startTime := time.Now() imageRef, err := sip.imageService.PullImage(pullRequest.ctx, pullRequest.spec, pullRequest.pullSecrets, pullRequest.podSandboxConfig) pullRequest.pullChan &lt;- pullResult&#123; imageRef: imageRef, err: err, pullDuration: time.Since(startTime), &#125; &#125;&#125; REF:1.pkg/controller/replicaset/replica_set.go2.vendor/k8s.io/utils/integer/integer.go3.pkg/util/tail/tail.go4.pkg/kubelet/images/puller.go5.https://kubernetes.io/zh-cn/docs/concepts/containers/images/#maximum-parallel-image-pulls6.pkg/kubelet/images/image_manager.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之hpa]]></title>
    <url>%2F2023%2F06%2F14%2Fkube-controller%E4%B9%8Bhpa%2F</url>
    <content type="text"><![CDATA[HPAHPA全称HorizontalPodAutoscaler,pod水平自动扩缩。在 Kubernetes 中，HorizontalPodAutoscaler 自动更新工作负载资源 （例如 Deployment 或者 StatefulSet）， 目的是自动扩缩工作负载以满足需求。 水平扩缩意味着对增加的负载的响应是部署更多的Pod。 这与 “垂直（Vertical）” 扩缩不同，对于 Kubernetes， 垂直扩缩意味着将更多资源（例如：内存或 CPU）分配给已经为工作负载运行的 Pod。 如果负载减少，并且 Pod 的数量高于配置的最小值， HorizontalPodAutoscaler 会指示工作负载资源（Deployment、StatefulSet 或其他类似资源）缩减。 水平 Pod 自动扩缩不适用于无法扩缩的对象（例如：DaemonSet。） HorizontalPodAutoscaler 被实现为 Kubernetes API 资源和控制器。 在 Kubernetes 控制平面内运行的水平 Pod 自动扩缩控制器会定期调整其目标（例如：Deployment）的所需规模，以匹配观察到的指标， 例如，平均 CPU 利用率、平均内存利用率或你指定的任何其他自定义指标。 HorizontalPodAutoscaler从Metrics Server中获得对应的资源指标，然后根据这些指标计算扩缩容比例。HorizontalPodAutoscaler 控制器访问支持扩缩的相应工作负载资源（例如：Deployment,StatefulSet,ReplicaSet) 这些资源每个都有一个名为 scale 的子资源，该接口允许你动态设置副本的数量。 算法细节Pod 水平自动扩缩控制器根据当前指标和期望指标来计算扩缩比例。1期望副本数 = ceil[当前副本数 * (当前指标 / 期望指标)] 例如，如果当前指标值为 200m，而期望值为 100m，则副本数将加倍， 因为 200.0 / 100.0 == 2.0 如果当前值为 50m，则副本数将减半， 因为 50.0 / 100.0 == 0.5。如果比率足够接近 1.0（在全局可配置的容差范围内，默认为 0.1）， 则控制平面会跳过扩缩操作。 详细信息可查看文档 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501// pkg/controller/podautoscaler/horizontal.go// HorizontalController, 可以看到HorizontalController// 包含了hpaLister,podLister,type HorizontalController struct &#123; scaleNamespacer scaleclient.ScalesGetter hpaNamespacer autoscalingclient.HorizontalPodAutoscalersGetter mapper apimeta.RESTMapper replicaCalc *ReplicaCalculator eventRecorder record.EventRecorder downscaleStabilisationWindow time.Duration monitor monitor.Monitor // hpaLister is able to list/get HPAs from the shared cache from the informer passed in to // NewHorizontalController. hpaLister autoscalinglisters.HorizontalPodAutoscalerLister hpaListerSynced cache.InformerSynced // podLister is able to list/get Pods from the shared cache from the informer passed in to // NewHorizontalController. podLister corelisters.PodLister podListerSynced cache.InformerSynced // Controllers that need to be synced queue workqueue.RateLimitingInterface // Latest unstabilized recommendations for each autoscaler. recommendations map[string][]timestampedRecommendation recommendationsLock sync.Mutex // Latest autoscaler events scaleUpEvents map[string][]timestampedScaleEvent scaleUpEventsLock sync.RWMutex scaleDownEvents map[string][]timestampedScaleEvent scaleDownEventsLock sync.RWMutex // Storage of HPAs and their selectors. hpaSelectors *selectors.BiMultimap hpaSelectorsMux sync.Mutex // feature gates containerResourceMetricsEnabled bool&#125;// 创建HorizontalControllerfunc NewHorizontalController( evtNamespacer v1core.EventsGetter, scaleNamespacer scaleclient.ScalesGetter, hpaNamespacer autoscalingclient.HorizontalPodAutoscalersGetter, mapper apimeta.RESTMapper, metricsClient metricsclient.MetricsClient, hpaInformer autoscalinginformers.HorizontalPodAutoscalerInformer, podInformer coreinformers.PodInformer, resyncPeriod time.Duration, downscaleStabilisationWindow time.Duration, tolerance float64, cpuInitializationPeriod, delayOfInitialReadinessStatus time.Duration, containerResourceMetricsEnabled bool,) *HorizontalController &#123; broadcaster := record.NewBroadcaster() broadcaster.StartStructuredLogging(0) broadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: evtNamespacer.Events("")&#125;) recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "horizontal-pod-autoscaler"&#125;) hpaController := &amp;HorizontalController&#123; eventRecorder: recorder, scaleNamespacer: scaleNamespacer, hpaNamespacer: hpaNamespacer, downscaleStabilisationWindow: downscaleStabilisationWindow, monitor: monitor.New(), queue: workqueue.NewNamedRateLimitingQueue(NewDefaultHPARateLimiter(resyncPeriod), "horizontalpodautoscaler"), mapper: mapper, recommendations: map[string][]timestampedRecommendation&#123;&#125;, recommendationsLock: sync.Mutex&#123;&#125;, scaleUpEvents: map[string][]timestampedScaleEvent&#123;&#125;, scaleUpEventsLock: sync.RWMutex&#123;&#125;, scaleDownEvents: map[string][]timestampedScaleEvent&#123;&#125;, scaleDownEventsLock: sync.RWMutex&#123;&#125;, hpaSelectors: selectors.NewBiMultimap(), containerResourceMetricsEnabled: containerResourceMetricsEnabled, &#125; // 监听了资源hpa变化，并进行入队操作 hpaInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: hpaController.enqueueHPA, UpdateFunc: hpaController.updateHPA, DeleteFunc: hpaController.deleteHPA, &#125;, resyncPeriod, ) hpaController.hpaLister = hpaInformer.Lister() hpaController.hpaListerSynced = hpaInformer.Informer().HasSynced hpaController.podLister = podInformer.Lister() hpaController.podListerSynced = podInformer.Informer().HasSynced // 用于计算副本数,ReplicaCalculator实现了几个方法用于副本的计算 replicaCalc := NewReplicaCalculator( metricsClient, hpaController.podLister, tolerance, cpuInitializationPeriod, delayOfInitialReadinessStatus, ) hpaController.replicaCalc = replicaCalc monitor.Register() return hpaController&#125;func (a *HorizontalController) processNextWorkItem(ctx context.Context) bool &#123; key, quit := a.queue.Get() if quit &#123; return false &#125; defer a.queue.Done(key) deleted, err := a.reconcileKey(ctx, key.(string)) if err != nil &#123; utilruntime.HandleError(err) &#125; // Add request processing HPA to queue with resyncPeriod delay. // Requests are always added to queue with resyncPeriod delay. If there's already request // for the HPA in the queue then a new request is always dropped. Requests spend resyncPeriod // in queue so HPAs are processed every resyncPeriod. // Request is added here just in case last resync didn't insert request into the queue. This // happens quite often because there is race condition between adding request after resyncPeriod // and removing them from queue. Request can be added by resync before previous request is // removed from queue. If we didn't add request here then in this case one request would be dropped // and HPA would processed after 2 x resyncPeriod. if !deleted &#123; a.queue.AddRateLimited(key) &#125; return true&#125;func (a *HorizontalController) reconcileKey(ctx context.Context, key string) (deleted bool, err error) &#123; namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return true, err &#125; logger := klog.FromContext(ctx) // 获取hpa hpa, err := a.hpaLister.HorizontalPodAutoscalers(namespace).Get(name) if k8serrors.IsNotFound(err) &#123; logger.Info("Horizontal Pod Autoscaler has been deleted", "HPA", klog.KRef(namespace, name)) a.recommendationsLock.Lock() delete(a.recommendations, key) a.recommendationsLock.Unlock() a.scaleUpEventsLock.Lock() delete(a.scaleUpEvents, key) a.scaleUpEventsLock.Unlock() a.scaleDownEventsLock.Lock() delete(a.scaleDownEvents, key) a.scaleDownEventsLock.Unlock() return true, nil &#125; if err != nil &#123; return false, err &#125; return false, a.reconcileAutoscaler(ctx, hpa, key)&#125;// 真正进行调谐的函数func (a *HorizontalController) reconcileAutoscaler(ctx context.Context, hpaShared *autoscalingv2.HorizontalPodAutoscaler, key string) (retErr error) &#123; // actionLabel is used to report which actions this reconciliation has taken. actionLabel := monitor.ActionLabelNone start := time.Now() defer func() &#123; errorLabel := monitor.ErrorLabelNone if retErr != nil &#123; // In case of error, set "internal" as default. errorLabel = monitor.ErrorLabelInternal &#125; if errors.Is(retErr, errSpec) &#123; errorLabel = monitor.ErrorLabelSpec &#125; a.monitor.ObserveReconciliationResult(actionLabel, errorLabel, time.Since(start)) &#125;() // make a copy so that we never mutate the shared informer cache (conversion can mutate the object) hpa := hpaShared.DeepCopy() hpaStatusOriginal := hpa.Status.DeepCopy() reference := fmt.Sprintf("%s/%s/%s", hpa.Spec.ScaleTargetRef.Kind, hpa.Namespace, hpa.Spec.ScaleTargetRef.Name) targetGV, err := schema.ParseGroupVersion(hpa.Spec.ScaleTargetRef.APIVersion) if err != nil &#123; a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err) if err := a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa); err != nil &#123; utilruntime.HandleError(err) &#125; return fmt.Errorf("invalid API version in scale target reference: %v%w", err, errSpec) &#125; targetGK := schema.GroupKind&#123; Group: targetGV.Group, Kind: hpa.Spec.ScaleTargetRef.Kind, &#125; // RESTMappings 返回提供的组和资源类型的所有资源映射 mappings, err := a.mapper.RESTMappings(targetGK) if err != nil &#123; a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err) if err := a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa); err != nil &#123; utilruntime.HandleError(err) &#125; return fmt.Errorf("unable to determine resource for scale target reference: %v", err) &#125; // 获取hpa引用的资源的子资源scale scale, targetGR, err := a.scaleForResourceMappings(ctx, hpa.Namespace, hpa.Spec.ScaleTargetRef.Name, mappings) if err != nil &#123; a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedGetScale", err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedGetScale", "the HPA controller was unable to get the target's current scale: %v", err) if err := a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa); err != nil &#123; utilruntime.HandleError(err) &#125; return fmt.Errorf("failed to query scale subresource for %s: %v", reference, err) &#125; setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionTrue, "SucceededGetScale", "the HPA controller was able to get the target's current scale") // 当前复本数 currentReplicas := scale.Spec.Replicas a.recordInitialRecommendation(currentReplicas, key) var ( metricStatuses []autoscalingv2.MetricStatus metricDesiredReplicas int32 metricName string ) // 期望的副本数 desiredReplicas := int32(0) rescaleReason := "" var minReplicas int32 // 设置MinReplicas if hpa.Spec.MinReplicas != nil &#123; minReplicas = *hpa.Spec.MinReplicas &#125; else &#123; // Default value minReplicas = 1 &#125; rescale := true logger := klog.FromContext(ctx) // scale的副本数为0和minReplicas!=0,不进行Autoscaling(为什么还需要minReplicas这个条件) if scale.Spec.Replicas == 0 &amp;&amp; minReplicas != 0 &#123; // Autoscaling is disabled for this resource desiredReplicas = 0 // 不进行扩缩容操作 rescale = false setCondition(hpa, autoscalingv2.ScalingActive, v1.ConditionFalse, "ScalingDisabled", "scaling is disabled since the replica count of the target is zero") &#125; else if currentReplicas &gt; hpa.Spec.MaxReplicas &#123; // 如果当前副本数大于hpa MaxReplicas, 则期望副本数等于hpa.Spec.MaxReplicas rescaleReason = "Current number of replicas above Spec.MaxReplicas" desiredReplicas = hpa.Spec.MaxReplicas &#125; else if currentReplicas &lt; minReplicas &#123; // 如果当前副本数小于hpa MinReplicas, 则期望副本数等于hpa.Spec.MinReplicas rescaleReason = "Current number of replicas below Spec.MinReplicas" desiredReplicas = minReplicas &#125; else &#123; var metricTimestamp time.Time // 根据资源指标计算metricDesiredReplicas // hpa的算法就在computeReplicasForMetrics里 metricDesiredReplicas, metricName, metricStatuses, metricTimestamp, err = a.computeReplicasForMetrics(ctx, hpa, scale, hpa.Spec.Metrics) // computeReplicasForMetrics may return both non-zero metricDesiredReplicas and an error. // That means some metrics still work and HPA should perform scaling based on them. if err != nil &amp;&amp; metricDesiredReplicas == -1 &#123; a.setCurrentReplicasInStatus(hpa, currentReplicas) if err := a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa); err != nil &#123; utilruntime.HandleError(err) &#125; a.eventRecorder.Event(hpa, v1.EventTypeWarning, "FailedComputeMetricsReplicas", err.Error()) return fmt.Errorf("failed to compute desired number of replicas based on listed metrics for %s: %v", reference, err) &#125; if err != nil &#123; // We proceed to scaling, but return this error from reconcileAutoscaler() finally. retErr = err &#125; logger.V(4).Info("Proposing desired replicas", "desiredReplicas", metricDesiredReplicas, "metric", metricName, "timestamp", metricTimestamp, "scaleTarget", reference) rescaleMetric := "" if metricDesiredReplicas &gt; desiredReplicas &#123; desiredReplicas = metricDesiredReplicas rescaleMetric = metricName &#125; if desiredReplicas &gt; currentReplicas &#123; rescaleReason = fmt.Sprintf("%s above target", rescaleMetric) &#125; if desiredReplicas &lt; currentReplicas &#123; rescaleReason = "All metrics below target" &#125; if hpa.Spec.Behavior == nil &#123; desiredReplicas = a.normalizeDesiredReplicas(hpa, key, currentReplicas, desiredReplicas, minReplicas) &#125; else &#123; desiredReplicas = a.normalizeDesiredReplicasWithBehaviors(hpa, key, currentReplicas, desiredReplicas, minReplicas) &#125; rescale = desiredReplicas != currentReplicas &#125; if rescale &#123; // 设置scale.Spec.Replicas scale.Spec.Replicas = desiredReplicas // 调用接口更新scale _, err = a.scaleNamespacer.Scales(hpa.Namespace).Update(ctx, targetGR, scale, metav1.UpdateOptions&#123;&#125;) if err != nil &#123; a.eventRecorder.Eventf(hpa, v1.EventTypeWarning, "FailedRescale", "New size: %d; reason: %s; error: %v", desiredReplicas, rescaleReason, err.Error()) setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionFalse, "FailedUpdateScale", "the HPA controller was unable to update the target scale: %v", err) a.setCurrentReplicasInStatus(hpa, currentReplicas) if err := a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa); err != nil &#123; utilruntime.HandleError(err) &#125; return fmt.Errorf("failed to rescale %s: %v", reference, err) &#125; setCondition(hpa, autoscalingv2.AbleToScale, v1.ConditionTrue, "SucceededRescale", "the HPA controller was able to update the target scale to %d", desiredReplicas) a.eventRecorder.Eventf(hpa, v1.EventTypeNormal, "SuccessfulRescale", "New size: %d; reason: %s", desiredReplicas, rescaleReason) a.storeScaleEvent(hpa.Spec.Behavior, key, currentReplicas, desiredReplicas) logger.Info("Successfully rescaled", "HPA", klog.KObj(hpa), "currentReplicas", currentReplicas, "desiredReplicas", desiredReplicas, "reason", rescaleReason) if desiredReplicas &gt; currentReplicas &#123; actionLabel = monitor.ActionLabelScaleUp &#125; else &#123; actionLabel = monitor.ActionLabelScaleDown &#125; &#125; else &#123; logger.V(4).Info("Decided not to scale", "scaleTarget", reference, "desiredReplicas", desiredReplicas, "lastScaleTime", hpa.Status.LastScaleTime) desiredReplicas = currentReplicas &#125; a.setStatus(hpa, currentReplicas, desiredReplicas, metricStatuses, rescale) err = a.updateStatusIfNeeded(ctx, hpaStatusOriginal, hpa) if err != nil &#123; // we can overwrite retErr in this case because it's an internal error. return err &#125; return retErr&#125;// 根据HPA中的metric spec计算期望副本数func (a *HorizontalController) computeReplicasForMetrics(ctx context.Context, hpa *autoscalingv2.HorizontalPodAutoscaler, scale *autoscalingv1.Scale, metricSpecs []autoscalingv2.MetricSpec) (replicas int32, metric string, statuses []autoscalingv2.MetricStatus, timestamp time.Time, err error) &#123; selector, err := a.validateAndParseSelector(hpa, scale.Status.Selector) if err != nil &#123; return -1, "", nil, time.Time&#123;&#125;, err &#125; specReplicas := scale.Spec.Replicas statusReplicas := scale.Status.Replicas statuses = make([]autoscalingv2.MetricStatus, len(metricSpecs)) invalidMetricsCount := 0 var invalidMetricError error var invalidMetricCondition autoscalingv2.HorizontalPodAutoscalerCondition // 遍历metricSpecs,分别计算期望副本数，取最大值 for i, metricSpec := range metricSpecs &#123; replicaCountProposal, metricNameProposal, timestampProposal, condition, err := a.computeReplicasForMetric(ctx, hpa, metricSpec, specReplicas, statusReplicas, selector, &amp;statuses[i]) if err != nil &#123; if invalidMetricsCount &lt;= 0 &#123; invalidMetricCondition = condition invalidMetricError = err &#125; invalidMetricsCount++ continue &#125; if replicas == 0 || replicaCountProposal &gt; replicas &#123; timestamp = timestampProposal replicas = replicaCountProposal metric = metricNameProposal &#125; &#125; if invalidMetricError != nil &#123; invalidMetricError = fmt.Errorf("invalid metrics (%v invalid out of %v), first error is: %v", invalidMetricsCount, len(metricSpecs), invalidMetricError) &#125; // If all metrics are invalid or some are invalid and we would scale down, // return an error and set the condition of the hpa based on the first invalid metric. // Otherwise set the condition as scaling active as we're going to scale if invalidMetricsCount &gt;= len(metricSpecs) || (invalidMetricsCount &gt; 0 &amp;&amp; replicas &lt; specReplicas) &#123; setCondition(hpa, invalidMetricCondition.Type, invalidMetricCondition.Status, invalidMetricCondition.Reason, invalidMetricCondition.Message) return -1, "", statuses, time.Time&#123;&#125;, invalidMetricError &#125; setCondition(hpa, autoscalingv2.ScalingActive, v1.ConditionTrue, "ValidMetricFound", "the HPA was able to successfully calculate a replica count from %s", metric) return replicas, metric, statuses, timestamp, invalidMetricError&#125;func (a *HorizontalController) computeReplicasForMetric(ctx context.Context, hpa *autoscalingv2.HorizontalPodAutoscaler, spec autoscalingv2.MetricSpec, specReplicas, statusReplicas int32, selector labels.Selector, status *autoscalingv2.MetricStatus) (replicaCountProposal int32, metricNameProposal string, timestampProposal time.Time, condition autoscalingv2.HorizontalPodAutoscalerCondition, err error) &#123; // actionLabel is used to report which actions this reconciliation has taken. start := time.Now() defer func() &#123; actionLabel := monitor.ActionLabelNone switch &#123; case replicaCountProposal &gt; hpa.Status.CurrentReplicas: actionLabel = monitor.ActionLabelScaleUp case replicaCountProposal &lt; hpa.Status.CurrentReplicas: actionLabel = monitor.ActionLabelScaleDown &#125; errorLabel := monitor.ErrorLabelNone if err != nil &#123; // In case of error, set "internal" as default. errorLabel = monitor.ErrorLabelInternal actionLabel = monitor.ActionLabelNone &#125; if errors.Is(err, errSpec) &#123; errorLabel = monitor.ErrorLabelSpec &#125; a.monitor.ObserveMetricComputationResult(actionLabel, errorLabel, time.Since(start), spec.Type) &#125;() // 根据不同的类型分别计算期望副本数 switch spec.Type &#123; case autoscalingv2.ObjectMetricSourceType: metricSelector, err := metav1.LabelSelectorAsSelector(spec.Object.Metric.Selector) if err != nil &#123; condition := a.getUnableComputeReplicaCountCondition(hpa, "FailedGetObjectMetric", err) return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get object metric value: %v", err) &#125; replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForObjectMetric(specReplicas, statusReplicas, spec, hpa, selector, status, metricSelector) if err != nil &#123; return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get object metric value: %v", err) &#125; case autoscalingv2.PodsMetricSourceType: metricSelector, err := metav1.LabelSelectorAsSelector(spec.Pods.Metric.Selector) if err != nil &#123; condition := a.getUnableComputeReplicaCountCondition(hpa, "FailedGetPodsMetric", err) return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get pods metric value: %v", err) &#125; replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForPodsMetric(specReplicas, spec, hpa, selector, status, metricSelector) if err != nil &#123; return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get pods metric value: %v", err) &#125; case autoscalingv2.ResourceMetricSourceType: replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForResourceMetric(ctx, specReplicas, spec, hpa, selector, status) if err != nil &#123; return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get %s resource metric value: %v", spec.Resource.Name, err) &#125; case autoscalingv2.ContainerResourceMetricSourceType: if !a.containerResourceMetricsEnabled &#123; // If the container resource metrics feature is disabled but the object has the one, // that means the user enabled the feature once, // created some HPAs with the container resource metrics, and disabled it finally. return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("ContainerResource metric type is not supported: disabled by the feature gate") &#125; replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForContainerResourceMetric(ctx, specReplicas, spec, hpa, selector, status) if err != nil &#123; return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get %s container metric value: %v", spec.ContainerResource.Container, err) &#125; case autoscalingv2.ExternalMetricSourceType: replicaCountProposal, timestampProposal, metricNameProposal, condition, err = a.computeStatusForExternalMetric(specReplicas, statusReplicas, spec, hpa, selector, status) if err != nil &#123; return 0, "", time.Time&#123;&#125;, condition, fmt.Errorf("failed to get %s external metric value: %v", spec.External.Metric.Name, err) &#125; default: // It shouldn't reach here as invalid metric source type is filtered out in the api-server's validation. err = fmt.Errorf("unknown metric source type %q%w", string(spec.Type), errSpec) condition := a.getUnableComputeReplicaCountCondition(hpa, "InvalidMetricSourceType", err) return 0, "", time.Time&#123;&#125;, condition, err &#125; return replicaCountProposal, metricNameProposal, timestampProposal, autoscalingv2.HorizontalPodAutoscalerCondition&#123;&#125;, nil&#125; 1234567891011121314151617181920212223242526272829// Metric类型// staging/src/k8s.io/api/autoscaling/v2/types.goconst ( // ObjectMetricSourceType is a metric describing a kubernetes object // (for example, hits-per-second on an Ingress object). ObjectMetricSourceType MetricSourceType = "Object" // PodsMetricSourceType is a metric describing each pod in the current scale // target (for example, transactions-processed-per-second). The values // will be averaged together before being compared to the target value. PodsMetricSourceType MetricSourceType = "Pods" // ResourceMetricSourceType is a resource metric known to Kubernetes, as // specified in requests and limits, describing each pod in the current // scale target (e.g. CPU or memory). Such metrics are built in to // Kubernetes, and have special scaling options on top of those available // to normal per-pod metrics (the "pods" source). ResourceMetricSourceType MetricSourceType = "Resource" // ContainerResourceMetricSourceType is a resource metric known to Kubernetes, as // specified in requests and limits, describing a single container in each pod in the current // scale target (e.g. CPU or memory). Such metrics are built in to // Kubernetes, and have special scaling options on top of those available // to normal per-pod metrics (the "pods" source). ContainerResourceMetricSourceType MetricSourceType = "ContainerResource" // ExternalMetricSourceType is a global metric that is not associated // with any Kubernetes object. It allows autoscaling based on information // coming from components running outside of cluster // (for example length of queue in cloud messaging service, or // QPS from loadbalancer running outside of cluster). ExternalMetricSourceType MetricSourceType = "External") REF:1.horizontal-pod-autoscale/2.编写自定义指标适配器3.staging/src/k8s.io/api/autoscaling/v2/types.go4.pkg/controller/podautoscaler/horizontal.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识k8sgpt]]></title>
    <url>%2F2023%2F06%2F01%2F%E5%88%9D%E8%AF%86k8sgpt%2F</url>
    <content type="text"><![CDATA[k8sgpt是一个扫描k8s集群和诊断集群问题的强有力工具。k8sgpt的分析都是基于SRE经验，使用k8sgpt能快速对集群进行问题扫描且快速提取相关信息并且可以使用AI增加自身的能力。 K8SGPT的作用是帮助用户扫描和诊断Kubernetes集群中的问题。它可以执行以下任务： 故障排除：K8SGPT使用自然语言处理技术分析Kubernetes集群的状态和配置，帮助用户快速识别和解决故障。 安全性分析：K8SGPT能够检测和分析Kubernetes集群的安全性问题，例如暴露的敏感信息、权限配置错误等，并提供相应的建议和修复措施。 性能优化：K8SGPT可以评估Kubernetes集群的性能，并提供性能优化建议，帮助用户提升集群的吞吐量、响应时间等关键指标。 最佳实践验证：K8SGPT基于SRE经验，可以检查Kubernetes集群是否符合最佳实践和行业标准，并提供改进建议。 如何安装安装教程 简明教程你可以在官方网站上直接体验不用安装 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 分析k8s集群状态，默认会检查这些资源# Deployment, ReplicaSet, PersistentVolumeClaim, Service, Ingress, StatefulSet, Pod, CronJob, Nodek8sgpt analyze# --explain选项将会将错误信息发送给AI后端(默认是OpenAI)以获得一些帮助(会提示你怎么解决问题)k8sgpt analyze --explain# 使用FakeAI providerk8sgpt analyze -b noopai --explain# 数据安全性，为了提高安全性k8sgpt提供了匿名化功能，在把数据发送给AI后端之前会对一些敏感数据进行处理。当AI后端返回数据之后，k8sgpt再把数据替换成真实的数据返回给客户端# 匿名化不支持Eventk8sgpt analyze -b noopai --explain --anonymize# 对指定的命名空间进行扫描k8sgpt analyze --namespace kubee-system# 只想扫描指定的资源# 只扫描Ingress, Servicek8sgpt analyze --filter Ingress,Service# filters列表，分为Active 和Unused➜ k8sgpt filters listActive: &gt; Pod&gt; Deployment&gt; ReplicaSet&gt; PersistentVolumeClaim&gt; Service&gt; Ingress&gt; StatefulSet&gt; CronJob&gt; NodeUnused: &gt; HorizontalPodAutoScaler&gt; PodDisruptionBudget&gt; NetworkPolicy# 添加新的filtersk8sgpt filters add PodDisruptionBudget# 从Active中的filters移除k8sgpt filters remove PodDisruptionBudget# 与其它工具集成## 支持的工具列表➜ k8sgpt integrations listActive:Unused: &gt; trivy# 激活trivyk8sgpt integration activate trivy# tryk8sgpt analyze -b noopai --filter VulnerabilityReport REF:1.https://docs.k8sgpt.ai/2.https://docs.k8sgpt.ai/tutorials/playground/3.https://github.com/k8sgpt-ai/k8sgpt/blob/main/CONTRIBUTING.md]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>k8sgpt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之ServiceAccounts]]></title>
    <url>%2F2023%2F05%2F24%2Fkube-controller%E4%B9%8BServiceAccounts%2F</url>
    <content type="text"><![CDATA[ServiceAccountsController 是 Kubernetes 中的一个控制器，负责管理和维护 Service Account 资源。 Service Account 是 Kubernetes 中用于身份验证和授权的一种机制。它与 Pod 关联，为 Pod 提供一个身份标识。通过 Service Account，Pod 可以与 API 服务器进行身份验证，并根据其与 Service Account 关联的角色和权限来访问集群中的资源。 ServiceAccountsController 的主要职责包括以下几个方面： 创建Service Account：当创建Namespace 时，ServiceAccountsController 会检查该Namespace下是否存在”default” ServiceAccounts,如果不存在则创建。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142func DefaultServiceAccountsControllerOptions() ServiceAccountsControllerOptions &#123; return ServiceAccountsControllerOptions&#123; ServiceAccounts: []v1.ServiceAccount&#123; &#123;ObjectMeta: metav1.ObjectMeta&#123;Name: "default"&#125;&#125;, &#125;, &#125;&#125;// pkg/controller/serviceaccount/serviceaccounts_controller.gotype ServiceAccountsController struct &#123; client clientset.Interface serviceAccountsToEnsure []v1.ServiceAccount // To allow injection for testing. syncHandler func(ctx context.Context, key string) error saLister corelisters.ServiceAccountLister saListerSynced cache.InformerSynced nsLister corelisters.NamespaceLister nsListerSynced cache.InformerSynced queue workqueue.RateLimitingInterface&#125;// NewServiceAccountsController 监听两种资源对象// ServiceAccounts 监听删除事件,将key入队// Namespaces 监听创建和更新事件,将key入队func NewServiceAccountsController(saInformer coreinformers.ServiceAccountInformer, nsInformer coreinformers.NamespaceInformer, cl clientset.Interface, options ServiceAccountsControllerOptions) (*ServiceAccountsController, error) &#123; e := &amp;ServiceAccountsController&#123; client: cl, // serviceAccountsToEnsure, 有一个名为"default"的serviceAccount serviceAccountsToEnsure: options.ServiceAccounts, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "serviceaccount"), &#125; saHandler, _ := saInformer.Informer().AddEventHandlerWithResyncPeriod(cache.ResourceEventHandlerFuncs&#123; DeleteFunc: e.serviceAccountDeleted, &#125;, options.ServiceAccountResync) e.saLister = saInformer.Lister() e.saListerSynced = saHandler.HasSynced nsHandler, _ := nsInformer.Informer().AddEventHandlerWithResyncPeriod(cache.ResourceEventHandlerFuncs&#123; AddFunc: e.namespaceAdded, UpdateFunc: e.namespaceUpdated, &#125;, options.NamespaceResync) e.nsLister = nsInformer.Lister() e.nsListerSynced = nsHandler.HasSynced // syncHandler为syncNamespace e.syncHandler = e.syncNamespace return e, nil&#125;func (c *ServiceAccountsController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() defer c.queue.ShutDown() klog.FromContext(ctx).Info("Starting service account controller") defer klog.FromContext(ctx).Info("Shutting down service account controller") if !cache.WaitForNamedCacheSync("service account", ctx.Done(), c.saListerSynced, c.nsListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, c.runWorker, time.Second) &#125; &lt;-ctx.Done()&#125;func (c *ServiceAccountsController) runWorker(ctx context.Context) &#123; for c.processNextWorkItem(ctx) &#123; &#125;&#125;// processNextWorkItem deals with one key off the queue. It returns false when it's time to quit.func (c *ServiceAccountsController) processNextWorkItem(ctx context.Context) bool &#123; key, quit := c.queue.Get() if quit &#123; return false &#125; defer c.queue.Done(key) err := c.syncHandler(ctx, key.(string)) if err == nil &#123; c.queue.Forget(key) return true &#125; utilruntime.HandleError(fmt.Errorf("%v failed with : %v", key, err)) c.queue.AddRateLimited(key) return true&#125;func (c *ServiceAccountsController) syncNamespace(ctx context.Context, key string) error &#123; startTime := time.Now() defer func() &#123; klog.FromContext(ctx).V(4).Info("Finished syncing namespace", "namespace", key, "duration", time.Since(startTime)) &#125;() ns, err := c.nsLister.Get(key) if apierrors.IsNotFound(err) &#123; return nil &#125; if err != nil &#123; return err &#125; // Namespace状态不是Active,不做任何事 if ns.Status.Phase != v1.NamespaceActive &#123; return nil &#125; createFailures := []error&#123;&#125; for _, sa := range c.serviceAccountsToEnsure &#123; // 判断对应的命名空间下有没有名为"default"的ServiceAccounts switch _, err := c.saLister.ServiceAccounts(ns.Name).Get(sa.Name); &#123; case err == nil: continue // 如果err是NotFound,则会在下面的逻辑中创建新的ServiceAccounts case apierrors.IsNotFound(err): case err != nil: return err &#125; // this is only safe because we never read it and we always write it // TODO eliminate this once the fake client can handle creation without NS sa.Namespace = ns.Name if _, err := c.client.CoreV1().ServiceAccounts(ns.Name).Create(ctx, &amp;sa, metav1.CreateOptions&#123;&#125;); err != nil &amp;&amp; !apierrors.IsAlreadyExists(err) &#123; // we can safely ignore terminating namespace errors if !apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) &#123; createFailures = append(createFailures, err) &#125; &#125; &#125; return utilerrors.Flatten(utilerrors.NewAggregate(createFailures))&#125; 启动入口 1234567891011121314// cmd/kube-controller-manager/app/core.gofunc startServiceAccountController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) &#123; sac, err := serviceaccountcontroller.NewServiceAccountsController( controllerContext.InformerFactory.Core().V1().ServiceAccounts(), controllerContext.InformerFactory.Core().V1().Namespaces(), controllerContext.ClientBuilder.ClientOrDie("service-account-controller"), serviceaccountcontroller.DefaultServiceAccountsControllerOptions(), ) if err != nil &#123; return nil, true, fmt.Errorf("error creating ServiceAccount controller: %v", err) &#125; go sac.Run(ctx, 1) return nil, true, nil&#125; 当 Pod 使用 ServiceAccount 运行时，Kubernetes 会自动为该 ServiceAccount 创建一个与之关联的 Token。这个 Token 通常存储在 Pod 的文件系统中的 /var/run/secrets/kubernetes.io/serviceaccount/token 文件中。Pod 中的容器可以通过读取该文件来获取与 ServiceAccount 相关联的 Token。 Token 是一种用于身份验证的凭据，它可以用于与 Kubernetes API 服务器进行身份验证和授权操作。Pod 中的容器可以使用 Token 来与 Kubernetes API 服务器交互，例如获取或修改资源对象。 通过 ServiceAccount 和 Token 的组合，Kubernetes 提供了一种安全的身份验证和授权机制。Pod 可以使用与其关联的 ServiceAccount 的 Token 来证明其身份，并在需要访问受保护资源时进行授权。这种机制确保了集群中的各个组件和应用程序具有适当的访问权限，并提供了更细粒度的权限控制。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232type TokensControllerOptions struct &#123; // TokenGenerator is the generator to use to create new tokens // GenerateToken生成一个令牌，用于标识给定的ServiceAccount TokenGenerator serviceaccount.TokenGenerator // ServiceAccountResync is the time.Duration at which to fully re-list service accounts. // If zero, re-list will be delayed as long as possible ServiceAccountResync time.Duration // SecretResync is the time.Duration at which to fully re-list secrets. // If zero, re-list will be delayed as long as possible SecretResync time.Duration // This CA will be added in the secrets of service accounts RootCA []byte // MaxRetries controls the maximum number of times a particular key is retried before giving up // If zero, a default max is used MaxRetries int&#125;// pkg/controller/serviceaccount/tokens_controller.gofunc NewTokensController(serviceAccounts informers.ServiceAccountInformer, secrets informers.SecretInformer, cl clientset.Interface, options TokensControllerOptions) (*TokensController, error) &#123; maxRetries := options.MaxRetries if maxRetries == 0 &#123; maxRetries = 10 &#125; e := &amp;TokensController&#123; client: cl, token: options.TokenGenerator, rootCA: options.RootCA, syncServiceAccountQueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "serviceaccount_tokens_service"), syncSecretQueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "serviceaccount_tokens_secret"), maxRetries: maxRetries, &#125; e.serviceAccounts = serviceAccounts.Lister() e.serviceAccountSynced = serviceAccounts.Informer().HasSynced serviceAccounts.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; // 加入syncServiceAccountQueue AddFunc: e.queueServiceAccountSync, UpdateFunc: e.queueServiceAccountUpdateSync, DeleteFunc: e.queueServiceAccountSync, &#125;, options.ServiceAccountResync, ) secretCache := secrets.Informer().GetIndexer() e.updatedSecrets = cache.NewIntegerResourceVersionMutationCache(secretCache, secretCache, 60*time.Second, true) e.secretSynced = secrets.Informer().HasSynced secrets.Informer().AddEventHandlerWithResyncPeriod( cache.FilteringResourceEventHandler&#123; FilterFunc: func(obj interface&#123;&#125;) bool &#123; switch t := obj.(type) &#123; case *v1.Secret: return t.Type == v1.SecretTypeServiceAccountToken default: utilruntime.HandleError(fmt.Errorf("object passed to %T that is not expected: %T", e, obj)) return false &#125; &#125;, // 加入syncSecretQueue Handler: cache.ResourceEventHandlerFuncs&#123; AddFunc: e.queueSecretSync, UpdateFunc: e.queueSecretUpdateSync, DeleteFunc: e.queueSecretSync, &#125;, &#125;, options.SecretResync, ) return e, nil&#125;func (e *TokensController) queueServiceAccountSync(obj interface&#123;&#125;) &#123; if serviceAccount, ok := obj.(*v1.ServiceAccount); ok &#123; e.syncServiceAccountQueue.Add(makeServiceAccountKey(serviceAccount)) &#125;&#125;func (e *TokensController) queueServiceAccountUpdateSync(oldObj interface&#123;&#125;, newObj interface&#123;&#125;) &#123; if serviceAccount, ok := newObj.(*v1.ServiceAccount); ok &#123; e.syncServiceAccountQueue.Add(makeServiceAccountKey(serviceAccount)) &#125;&#125;func (e *TokensController) queueSecretSync(obj interface&#123;&#125;) &#123; if secret, ok := obj.(*v1.Secret); ok &#123; e.syncSecretQueue.Add(makeSecretQueueKey(secret)) &#125;&#125;func (e *TokensController) queueSecretUpdateSync(oldObj interface&#123;&#125;, newObj interface&#123;&#125;) &#123; if secret, ok := newObj.(*v1.Secret); ok &#123; e.syncSecretQueue.Add(makeSecretQueueKey(secret)) &#125;&#125;// Run runs controller blocks until stopCh is closedfunc (e *TokensController) Run(ctx context.Context, workers int) &#123; // Shut down queues defer utilruntime.HandleCrash() defer e.syncServiceAccountQueue.ShutDown() defer e.syncSecretQueue.ShutDown() if !cache.WaitForNamedCacheSync("tokens", ctx.Done(), e.serviceAccountSynced, e.secretSynced) &#123; return &#125; logger := klog.FromContext(ctx) logger.V(5).Info("Starting workers") for i := 0; i &lt; workers; i++ &#123; // 启动了两个sync方法,syncServiceAccount,syncSecret go wait.UntilWithContext(ctx, e.syncServiceAccount, 0) go wait.UntilWithContext(ctx, e.syncSecret, 0) &#125; &lt;-ctx.Done() logger.V(1).Info("Shutting down")&#125;func (e *TokensController) syncServiceAccount(ctx context.Context) &#123; logger := klog.FromContext(ctx) key, quit := e.syncServiceAccountQueue.Get() if quit &#123; return &#125; defer e.syncServiceAccountQueue.Done(key) retry := false defer func() &#123; e.retryOrForget(logger, e.syncServiceAccountQueue, key, retry) &#125;() // 获取serviceAccountQueueKey // type serviceAccountQueueKey struct &#123; // namespace string // name string // uid types.UID // &#125; saInfo, err := parseServiceAccountKey(key) if err != nil &#123; logger.Error(err, "Parsing service account key") return &#125; sa, err := e.getServiceAccount(saInfo.namespace, saInfo.name, saInfo.uid, false) switch &#123; case err != nil: logger.Error(err, "Getting service account") retry = true case sa == nil: // serviceaccount已经不存在，删除对应的token logger.V(4).Info("Service account deleted, removing tokens", "namespace", saInfo.namespace, "serviceaccount", saInfo.name) sa = &amp;v1.ServiceAccount&#123;ObjectMeta: metav1.ObjectMeta&#123;Namespace: saInfo.namespace, Name: saInfo.name, UID: saInfo.uid&#125;&#125; retry, err = e.deleteTokens(sa) if err != nil &#123; logger.Error(err, "Error deleting serviceaccount tokens", "namespace", saInfo.namespace, "serviceaccount", saInfo.name) &#125; &#125;&#125;func (e *TokensController) syncSecret(ctx context.Context) &#123; key, quit := e.syncSecretQueue.Get() if quit &#123; return &#125; defer e.syncSecretQueue.Done(key) logger := klog.FromContext(ctx) // Track whether or not we should retry this sync retry := false defer func() &#123; e.retryOrForget(logger, e.syncSecretQueue, key, retry) &#125;() // type secretQueueKey struct &#123; // namespace string // name string // uid types.UID // saName string // // optional, will be blank when syncing tokens missing the service account uid annotation // saUID types.UID // &#125; secretInfo, err := parseSecretQueueKey(key) if err != nil &#123; logger.Error(err, "Parsing secret queue key") return &#125; // 获取对应的Secret secret, err := e.getSecret(secretInfo.namespace, secretInfo.name, secretInfo.uid, false) switch &#123; case err != nil: logger.Error(err, "Getting secret") retry = true case secret == nil: // If the service account exists if sa, saErr := e.getServiceAccount(secretInfo.namespace, secretInfo.saName, secretInfo.saUID, false); saErr == nil &amp;&amp; sa != nil &#123; // secret no longer exists, so delete references to this secret from the service account // serviceaccount存在,secret不存在，删除对secret的引用 if err := clientretry.RetryOnConflict(RemoveTokenBackoff, func() error &#123; return e.removeSecretReference(secretInfo.namespace, secretInfo.saName, secretInfo.saUID, secretInfo.name) &#125;); err != nil &#123; logger.Error(err, "Removing secret reference") &#125; &#125; default: // Ensure service account exists // secret不为nil的情况 sa, saErr := e.getServiceAccount(secretInfo.namespace, secretInfo.saName, secretInfo.saUID, true) switch &#123; case saErr != nil: logger.Error(saErr, "Getting service account") retry = true case sa == nil: // Delete token // 如果serviceaccount为nil, 则删除对应的token logger.V(4).Info("Service account does not exist, deleting token", "secret", klog.KRef(secretInfo.namespace, secretInfo.name)) if retriable, err := e.deleteToken(secretInfo.namespace, secretInfo.name, secretInfo.uid); err != nil &#123; logger.Error(err, "Deleting serviceaccount token", "secret", klog.KRef(secretInfo.namespace, secretInfo.name), "serviceAccount", klog.KRef(secretInfo.namespace, secretInfo.saName)) retry = retriable &#125; default: // 更新token if retriable, err := e.generateTokenIfNeeded(logger, sa, secret); err != nil &#123; logger.Error(err, "Populating serviceaccount token", "secret", klog.KRef(secretInfo.namespace, secretInfo.name), "serviceAccount", klog.KRef(secretInfo.namespace, secretInfo.saName)) retry = retriable &#125; &#125; &#125;&#125; REF:1.pkg/controller/serviceaccount/serviceaccounts_controller.go2.cmd/kube-controller-manager/app/core.go3.pkg/controller/serviceaccount/tokens_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之rbac]]></title>
    <url>%2F2023%2F05%2F23%2Fk8s%E4%B9%8Brbac%2F</url>
    <content type="text"><![CDATA[Kubernetes中的RBAC（Role-Based Access Control）是一种授权机制，用于管理和控制对Kubernetes资源的访问权限。RBAC允许管理员定义角色、角色绑定和集群角色绑定来精确控制用户、服务账号或组的权限。 RBAC的核心概念包括以下几个要素： Role（角色）：定义了一组权限，可以授予给特定的命名空间内的用户或服务账号。Role是命名空间级别的授权对象，它只对指定命名空间内的资源起作用。 ClusterRole（集群角色）：类似于Role，但作用于整个集群范围内的资源。ClusterRole可以跨越多个命名空间。 RoleBinding（角色绑定）：将一个Role绑定到用户、组或服务账号上，以赋予其相应的权限。RoleBinding是命名空间级别的绑定，它将角色授权应用于指定命名空间的主体。 ClusterRoleBinding（集群角色绑定）：类似于RoleBinding，但作用于整个集群范围内的资源。 通过RBAC，管理员可以创建和管理角色和绑定，将权限分配给用户或服务账号，从而实现对Kubernetes资源的细粒度访问控制。RBAC提供了灵活的权限管理机制，可以根据需要授予或限制特定操作的执行，以保护集群的安全性和数据的机密性。 RBAC的具体实现是通过Kubernetes API服务器的授权模块实现的，它会验证用户的身份，并根据相应的角色和绑定信息来授予或拒绝对资源的请求。 RBAC在Kubernetes中是一项重要的安全功能，能够帮助管理员有效管理和控制对集群资源的访问权限，并确保只有经过授权的实体能够执行相应的操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151// pkg/apis/rbac/types.go// 包括角色、绑定和策略等相关结构体和接口定义// PolicyRule存储了描述策略规则的信息type PolicyRule struct &#123; // 存储支持的Verbs,如"GET","CREATE"..., "*"表示全部支持 Verbs []string // APIGroups是包含资源的APIGroup的名称。 // ""表示核心API组，"*"表示所有API组。 APIGroups []string // 规则作用的资源对象，"*"表示指定apiGroups下的所有资源，"*/foo"表示指定apiGroups下所有子资源"foo" Resources []string // ResourceNames是规则适用于的可选名称白名单。空集表示允许所有内容。 ResourceNames []string // NonResourceURLs is a set of partial urls that a user should have access to. *s are allowed, but only as the full, final step in the path // If an action is not a resource API request, then the URL is split on '/' and is checked against the NonResourceURLs to look for a match. // Since non-resource URLs are not namespaced, this field is only applicable for ClusterRoles referenced from a ClusterRoleBinding. // Rules can either apply to API resources (such as "pods" or "secrets") or non-resource URL paths (such as "/api"), but not both. NonResourceURLs []string&#125;// Subject包含对角色绑定适用于的对象或用户标识的引用。它可以包含直接的API对象引用，也可以包含非对象（如用户和组名）的值type Subject struct &#123; // 此API组定义的值有"User"、"Group"和"ServiceAccount"。如果授权器不识别该类型值，则应报错 Kind string // 引用对象所属的APIGroup // ""表示ServiceAccount subjects // "rbac.authorization.k8s.io" for User and Group subjects APIGroup string // 引用对象名称 Name string // 引用对象所属的命名空间 // 如果对象类型是非命名空间类型（如"User"或"Group"），并且此值不为空，则授权器应报错 Namespace string&#125;// 指向正在使用的角色type RoleRef struct &#123; // APIGroup is the group for the resource being referenced APIGroup string // Kind is the type of resource being referenced Kind string // Name is the name of resource being referenced Name string&#125;// 角色type Role struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ObjectMeta // Rules holds all the PolicyRules for this Role Rules []PolicyRule&#125;// RoleBinding引用一个角色，但不包含角色本身。它可以绑定同一命名空间中的*Role*或ClusterRole// 它通过Subjects添加了关于用户的信息，并通过所在的命名空间添加了命名空间信息。在给定的命名空间中的RoleBinding只在该命名空间中生效。type RoleBinding struct &#123; metav1.TypeMeta metav1.ObjectMeta // Subjects holds references to the objects the role applies to. Subjects []Subject // RoleRef can reference a Role in the current namespace or a ClusterRole in the global namespace. // If the RoleRef cannot be resolved, the Authorizer must return an error. // This field is immutable. RoleRef RoleRef&#125;// RoleBinding列表type RoleBindingList struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ListMeta // Items is a list of roleBindings Items []RoleBinding&#125;type RoleList struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ListMeta // Items is a list of roles Items []Role&#125;// ClusterRole is a cluster level, logical grouping of PolicyRules that can be referenced as a unit by a RoleBinding or ClusterRoleBinding.type ClusterRole struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ObjectMeta // Rules holds all the PolicyRules for this ClusterRole Rules []PolicyRule // AggregationRule is an optional field that describes how to build the Rules for this ClusterRole. // If AggregationRule is set, then the Rules are controller managed and direct changes to Rules will be // stomped by the controller. AggregationRule *AggregationRule&#125;// AggregationRule describes how to locate ClusterRoles to aggregate into the ClusterRoletype AggregationRule struct &#123; // ClusterRoleSelectors holds a list of selectors which will be used to find ClusterRoles and create the rules. // If any of the selectors match, then the ClusterRole's permissions will be added ClusterRoleSelectors []metav1.LabelSelector&#125;// ClusterRoleBinding references a ClusterRole, but not contain it. It can reference a ClusterRole in the global namespace,// and adds who information via Subject.type ClusterRoleBinding struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ObjectMeta // Subjects holds references to the objects the role applies to. Subjects []Subject // RoleRef can only reference a ClusterRole in the global namespace. // If the RoleRef cannot be resolved, the Authorizer must return an error. // This field is immutable. RoleRef RoleRef&#125;// ClusterRoleBindingList is a collection of ClusterRoleBindingstype ClusterRoleBindingList struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ListMeta // Items is a list of ClusterRoleBindings Items []ClusterRoleBinding&#125;// ClusterRoleList is a collection of ClusterRolestype ClusterRoleList struct &#123; metav1.TypeMeta // Standard object's metadata. metav1.ListMeta // Items is a list of ClusterRoles Items []ClusterRole&#125; 123456789101112131415161718192021// staging/src/k8s.io/apiserver/pkg/server/config.gofunc DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler &#123; ... handler := filterlatency.TrackCompleted(apiHandler) handler = genericapifilters.WithAuthorization(handler, c.Authorization.Authorizer, c.Serializer) ...&#125;func (c completedConfig) New(name string, delegationTarget DelegationTarget) (*GenericAPIServer, error) &#123; ... handlerChainBuilder := func(handler http.Handler) http.Handler &#123; return c.BuildHandlerChainFunc(handler, c.Config) &#125; var debugSocket *routes.DebugSocket if c.DebugSocketPath != "" &#123; debugSocket = routes.NewDebugSocket(c.DebugSocketPath) &#125; apiServerHandler := NewAPIServerHandler(name, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler()) ... 在一次请求过程中会执行withAuthorization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206// staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.gofunc withAuthorization(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer, metrics recordAuthorizationMetricsFunc) http.Handler &#123; if a == nil &#123; klog.Warning("Authorization is disabled") return handler &#125; return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; ctx := req.Context() authorizationStart := time.Now() // 提取鉴权所需的信息 attributes, err := GetAuthorizerAttributes(ctx) if err != nil &#123; responsewriters.InternalError(w, req, err) return &#125; authorized, reason, err := a.Authorize(ctx, attributes) authorizationFinish := time.Now() defer func() &#123; metrics(ctx, authorized, err, authorizationStart, authorizationFinish) &#125;() // an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here. if authorized == authorizer.DecisionAllow &#123; audit.AddAuditAnnotations(ctx, decisionAnnotationKey, decisionAllow, reasonAnnotationKey, reason) handler.ServeHTTP(w, req) return &#125; if err != nil &#123; audit.AddAuditAnnotation(ctx, reasonAnnotationKey, reasonError) responsewriters.InternalError(w, req, err) return &#125; klog.V(4).InfoS("Forbidden", "URI", req.RequestURI, "reason", reason) audit.AddAuditAnnotations(ctx, decisionAnnotationKey, decisionForbid, reasonAnnotationKey, reason) responsewriters.Forbidden(ctx, attributes, w, req, reason, s) &#125;)&#125;// staging/src/k8s.io/apiserver/pkg/authorization/union/union.gofunc (authzHandler unionAuthzHandler) Authorize(ctx context.Context, a authorizer.Attributes) (authorizer.Decision, string, error) &#123; var ( errlist []error reasonlist []string ) // authzHandler 包括authorizerfactory.privilegedGroupAuthorizer // node.NodeAuthorizer, rbac.RBACAuthorizer // 只要任何一个handler返回Allow或者Deny，函数直接返回 for _, currAuthzHandler := range authzHandler &#123; decision, reason, err := currAuthzHandler.Authorize(ctx, a) if err != nil &#123; errlist = append(errlist, err) &#125; if len(reason) != 0 &#123; reasonlist = append(reasonlist, reason) &#125; switch decision &#123; case authorizer.DecisionAllow, authorizer.DecisionDeny: return decision, reason, err case authorizer.DecisionNoOpinion: // continue to the next authorizer &#125; &#125; return authorizer.DecisionNoOpinion, strings.Join(reasonlist, "\n"), utilerrors.NewAggregate(errlist)&#125;// plugin/pkg/auth/authorizer/rbac/rbac.go// 如果是RBAC会执行下面的代码进行鉴权func (r *RBACAuthorizer) Authorize(ctx context.Context, requestAttributes authorizer.Attributes) (authorizer.Decision, string, error) &#123; ruleCheckingVisitor := &amp;authorizingVisitor&#123;requestAttributes: requestAttributes&#125; r.authorizationRuleResolver.VisitRulesFor(requestAttributes.GetUser(), requestAttributes.GetNamespace(), ruleCheckingVisitor.visit) if ruleCheckingVisitor.allowed &#123; return authorizer.DecisionAllow, ruleCheckingVisitor.reason, nil &#125; // Build a detailed log of the denial. // Make the whole block conditional so we don't do a lot of string-building we won't use. if klogV := klog.V(5); klogV.Enabled() &#123; var operation string if requestAttributes.IsResourceRequest() &#123; b := &amp;bytes.Buffer&#123;&#125; b.WriteString(`"`) b.WriteString(requestAttributes.GetVerb()) b.WriteString(`" resource "`) b.WriteString(requestAttributes.GetResource()) if len(requestAttributes.GetAPIGroup()) &gt; 0 &#123; b.WriteString(`.`) b.WriteString(requestAttributes.GetAPIGroup()) &#125; if len(requestAttributes.GetSubresource()) &gt; 0 &#123; b.WriteString(`/`) b.WriteString(requestAttributes.GetSubresource()) &#125; b.WriteString(`"`) if len(requestAttributes.GetName()) &gt; 0 &#123; b.WriteString(` named "`) b.WriteString(requestAttributes.GetName()) b.WriteString(`"`) &#125; operation = b.String() &#125; else &#123; operation = fmt.Sprintf("%q nonResourceURL %q", requestAttributes.GetVerb(), requestAttributes.GetPath()) &#125; var scope string if ns := requestAttributes.GetNamespace(); len(ns) &gt; 0 &#123; scope = fmt.Sprintf("in namespace %q", ns) &#125; else &#123; scope = "cluster-wide" &#125; klogV.Infof("RBAC: no rules authorize user %q with groups %q to %s %s", requestAttributes.GetUser().GetName(), requestAttributes.GetUser().GetGroups(), operation, scope) &#125; reason := "" if len(ruleCheckingVisitor.errors) &gt; 0 &#123; reason = fmt.Sprintf("RBAC: %v", utilerrors.NewAggregate(ruleCheckingVisitor.errors)) &#125; return authorizer.DecisionNoOpinion, reason, nil&#125;// pkg/registry/rbac/validation/rule.gofunc (r *DefaultRuleResolver) VisitRulesFor(user user.Info, namespace string, visitor func(source fmt.Stringer, rule *rbacv1.PolicyRule, err error) bool) &#123; if clusterRoleBindings, err := r.clusterRoleBindingLister.ListClusterRoleBindings(); err != nil &#123; if !visitor(nil, nil, err) &#123; return &#125; &#125; else &#123; sourceDescriber := &amp;clusterRoleBindingDescriber&#123;&#125; for _, clusterRoleBinding := range clusterRoleBindings &#123; subjectIndex, applies := appliesTo(user, clusterRoleBinding.Subjects, "") if !applies &#123; continue &#125; // 获取角色对应的PolicyRule rules, err := r.GetRoleReferenceRules(clusterRoleBinding.RoleRef, "") if err != nil &#123; if !visitor(nil, nil, err) &#123; return &#125; continue &#125;GetSubresource sourceDescriber.binding = clusterRoleBinding sourceDescriber.subject = &amp;clusterRoleBinding.Subjects[subjectIndex] for i := range rules &#123; if !visitor(sourceDescriber, &amp;rules[i], nil) &#123; return &#125; &#125; &#125; &#125; if len(namespace) &gt; 0 &#123; if roleBindings, err := r.roleBindingLister.ListRoleBindings(namespace); err != nil &#123; if !visitor(nil, nil, err) &#123; return &#125; &#125; else &#123; sourceDescriber := &amp;roleBindingDescriber&#123;&#125; for _, roleBinding := range roleBindings &#123; subjectIndex, applies := appliesTo(user, roleBinding.Subjects, namespace) if !applies &#123; continue &#125; rules, err := r.GetRoleReferenceRules(roleBinding.RoleRef, namespace) if err != nil &#123; if !visitor(nil, nil, err) &#123; return &#125; continue &#125; sourceDescriber.binding = roleBinding sourceDescriber.subject = &amp;roleBinding.Subjects[subjectIndex] for i := range rules &#123; if !visitor(sourceDescriber, &amp;rules[i], nil) &#123; return &#125; &#125; &#125; &#125; &#125;&#125;// 如果通过则修改v.allowed=true,v.reason.并返回falsefunc (v *authorizingVisitor) visit(source fmt.Stringer, rule *rbacv1.PolicyRule, err error) bool &#123; // RuleAllows进行规则匹配 if rule != nil &amp;&amp; RuleAllows(v.requestAttributes, rule) &#123; v.allowed = true v.reason = fmt.Sprintf("RBAC: allowed by %s", source.String()) return false &#125; if err != nil &#123; v.errors = append(v.errors, err) &#125; return true&#125; REF:1.pkg/apis/rbac/types.go2.staging/src/k8s.io/apiserver/pkg/server/config.go3.staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go4.staging/src/k8s.io/apiserver/pkg/authorization/union/union.go5.plugin/pkg/auth/authorizer/rbac/rbac.go6.pkg/registry/rbac/validation/rule.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之kube-proxy[下]]]></title>
    <url>%2F2023%2F05%2F23%2Fk8s%E4%B9%8Bkube-proxy-%E4%B8%8B%2F</url>
    <content type="text"><![CDATA[k8s之kube-proxy[上] kube-proxy是Kubernetes集群中的一个核心组件，负责实现服务的负载均衡和网络代理功能。 kube-proxy的源码分析可以涉及多个文件和功能模块，以下是一些主要文件和功能模块的概述： cmd/kube-proxy/app：该包包含了kube-proxy的入口点代码，负责解析命令行参数、初始化配置和启动kube-proxy的主循环 pkg/proxy：该目录包含了kube-proxy的核心功能代码 pkg/proxy/ipvs：该目录包含了使用IPVS（IP Virtual Server）实现负载均衡的相关代码 pkg/proxy/iptables：该目录包含了使用iptables实现负载均衡的相关代码 pkg/proxy/util：该目录包含了一些辅助函数和工具类，用于支持kube-proxy的实现 kube-proxy入口123456// cmd/kube-proxy/proxy.gofunc main() &#123; command := app.NewProxyCommand() code := cli.Run(command) os.Exit(code)&#125; 12345678910111213141516171819// cmd/kube-proxy/app/server.gofunc (o *Options) Run() error &#123; defer close(o.errCh) if len(o.WriteConfigTo) &gt; 0 &#123; return o.writeConfigFile() &#125; if o.CleanupAndExit &#123; return cleanupAndExit() &#125; proxyServer, err := NewProxyServer(o) if err != nil &#123; return err &#125; o.proxyServer = proxyServer return o.runLoop()&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261// cmd/kube-proxy/app/server_others.gofunc NewProxyServer(o *Options) (*ProxyServer, error) &#123; return newProxyServer(o.config, o.master)&#125;func newProxyServer( config *proxyconfigapi.KubeProxyConfiguration, master string) (*ProxyServer, error) &#123; if c, err := configz.New(proxyconfigapi.GroupName); err == nil &#123; c.Set(config) &#125; else &#123; return nil, fmt.Errorf("unable to register configz: %s", err) &#125; var ipvsInterface utilipvs.Interface var ipsetInterface utilipset.Interface if len(config.ShowHiddenMetricsForVersion) &gt; 0 &#123; metrics.SetShowHidden() &#125; hostname, err := nodeutil.GetHostname(config.HostnameOverride) if err != nil &#123; return nil, err &#125; client, err := createClient(config.ClientConnection, master) if err != nil &#123; return nil, err &#125; nodeIP := detectNodeIP(client, hostname, config.BindAddress) klog.InfoS("Detected node IP", "address", nodeIP.String()) // Create event recorder eventBroadcaster := events.NewBroadcaster(&amp;events.EventSinkImpl&#123;Interface: client.EventsV1()&#125;) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, "kube-proxy") nodeRef := &amp;v1.ObjectReference&#123; Kind: "Node", Name: hostname, UID: types.UID(hostname), Namespace: "", &#125; var healthzServer healthcheck.ProxierHealthUpdater if len(config.HealthzBindAddress) &gt; 0 &#123; healthzServer = healthcheck.NewProxierHealthServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef) &#125; var proxier proxy.Provider var nodeInfo *v1.Node if config.DetectLocalMode == proxyconfigapi.LocalModeNodeCIDR &#123; klog.InfoS("Watching for node, awaiting podCIDR allocation", "hostname", hostname) nodeInfo, err = waitForPodCIDR(client, hostname) if err != nil &#123; return nil, err &#125; klog.InfoS("NodeInfo", "podCIDR", nodeInfo.Spec.PodCIDR, "podCIDRs", nodeInfo.Spec.PodCIDRs) &#125; primaryFamily := v1.IPv4Protocol primaryProtocol := utiliptables.ProtocolIPv4 if netutils.IsIPv6(nodeIP) &#123; primaryFamily = v1.IPv6Protocol primaryProtocol = utiliptables.ProtocolIPv6 &#125; execer := exec.New() iptInterface := utiliptables.New(execer, primaryProtocol) var ipt [2]utiliptables.Interface dualStack := true // While we assume that node supports, we do further checks below // Create iptables handlers for both families, one is already created // Always ordered as IPv4, IPv6 if primaryProtocol == utiliptables.ProtocolIPv4 &#123; ipt[0] = iptInterface ipt[1] = utiliptables.New(execer, utiliptables.ProtocolIPv6) &#125; else &#123; ipt[0] = utiliptables.New(execer, utiliptables.ProtocolIPv4) ipt[1] = iptInterface &#125; nodePortAddresses := config.NodePortAddresses if !ipt[0].Present() &#123; return nil, fmt.Errorf("iptables is not supported for primary IP family %q", primaryProtocol) &#125; else if !ipt[1].Present() &#123; klog.InfoS("kube-proxy running in single-stack mode: secondary ipFamily is not supported", "ipFamily", ipt[1].Protocol()) dualStack = false // Validate NodePortAddresses is single-stack npaByFamily := proxyutil.MapCIDRsByIPFamily(config.NodePortAddresses) secondaryFamily := proxyutil.OtherIPFamily(primaryFamily) badAddrs := npaByFamily[secondaryFamily] if len(badAddrs) &gt; 0 &#123; klog.InfoS("Ignoring --nodeport-addresses of the wrong family", "ipFamily", secondaryFamily, "addresses", badAddrs) nodePortAddresses = npaByFamily[primaryFamily] &#125; &#125; // 根据不同的模式设置不同的proxier // 现在支持两种模式iptables,ipvs if config.Mode == proxyconfigapi.ProxyModeIPTables &#123; klog.InfoS("Using iptables Proxier") if dualStack &#123; klog.InfoS("kube-proxy running in dual-stack mode", "ipFamily", iptInterface.Protocol()) klog.InfoS("Creating dualStackProxier for iptables") // Always ordered to match []ipt var localDetectors [2]proxyutiliptables.LocalTrafficDetector localDetectors, err = getDualStackLocalDetectorTuple(config.DetectLocalMode, config, ipt, nodeInfo) if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; // TODO this has side effects that should only happen when Run() is invoked. proxier, err = iptables.NewDualStackProxier( ipt, utilsysctl.New(), execer, config.IPTables.SyncPeriod.Duration, config.IPTables.MinSyncPeriod.Duration, config.IPTables.MasqueradeAll, *config.IPTables.LocalhostNodePorts, int(*config.IPTables.MasqueradeBit), localDetectors, hostname, nodeIPTuple(config.BindAddress), recorder, healthzServer, nodePortAddresses, ) &#125; else &#123; // Create a single-stack proxier if and only if the node does not support dual-stack (i.e, no iptables support). var localDetector proxyutiliptables.LocalTrafficDetector localDetector, err = getLocalDetector(config.DetectLocalMode, config, iptInterface, nodeInfo) if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; // TODO this has side effects that should only happen when Run() is invoked. proxier, err = iptables.NewProxier( primaryFamily, iptInterface, utilsysctl.New(), execer, config.IPTables.SyncPeriod.Duration, config.IPTables.MinSyncPeriod.Duration, config.IPTables.MasqueradeAll, *config.IPTables.LocalhostNodePorts, int(*config.IPTables.MasqueradeBit), localDetector, hostname, nodeIP, recorder, healthzServer, nodePortAddresses, ) &#125; if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; proxymetrics.RegisterMetrics() &#125; else if config.Mode == proxyconfigapi.ProxyModeIPVS &#123; kernelHandler := ipvs.NewLinuxKernelHandler() ipsetInterface = utilipset.New(execer) ipvsInterface = utilipvs.New() if err := ipvs.CanUseIPVSProxier(ipvsInterface, ipsetInterface, config.IPVS.Scheduler); err != nil &#123; return nil, fmt.Errorf("can't use the IPVS proxier: %v", err) &#125; klog.InfoS("Using ipvs Proxier") if dualStack &#123; klog.InfoS("Creating dualStackProxier for ipvs") nodeIPs := nodeIPTuple(config.BindAddress) // Always ordered to match []ipt var localDetectors [2]proxyutiliptables.LocalTrafficDetector localDetectors, err = getDualStackLocalDetectorTuple(config.DetectLocalMode, config, ipt, nodeInfo) if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; proxier, err = ipvs.NewDualStackProxier( ipt, ipvsInterface, ipsetInterface, utilsysctl.New(), execer, config.IPVS.SyncPeriod.Duration, config.IPVS.MinSyncPeriod.Duration, config.IPVS.ExcludeCIDRs, config.IPVS.StrictARP, config.IPVS.TCPTimeout.Duration, config.IPVS.TCPFinTimeout.Duration, config.IPVS.UDPTimeout.Duration, config.IPTables.MasqueradeAll, int(*config.IPTables.MasqueradeBit), localDetectors, hostname, nodeIPs, recorder, healthzServer, config.IPVS.Scheduler, nodePortAddresses, kernelHandler, ) &#125; else &#123; var localDetector proxyutiliptables.LocalTrafficDetector localDetector, err = getLocalDetector(config.DetectLocalMode, config, iptInterface, nodeInfo) if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; proxier, err = ipvs.NewProxier( primaryFamily, iptInterface, ipvsInterface, ipsetInterface, utilsysctl.New(), execer, config.IPVS.SyncPeriod.Duration, config.IPVS.MinSyncPeriod.Duration, config.IPVS.ExcludeCIDRs, config.IPVS.StrictARP, config.IPVS.TCPTimeout.Duration, config.IPVS.TCPFinTimeout.Duration, config.IPVS.UDPTimeout.Duration, config.IPTables.MasqueradeAll, int(*config.IPTables.MasqueradeBit), localDetector, hostname, nodeIP, recorder, healthzServer, config.IPVS.Scheduler, nodePortAddresses, kernelHandler, ) &#125; if err != nil &#123; return nil, fmt.Errorf("unable to create proxier: %v", err) &#125; proxymetrics.RegisterMetrics() &#125; return &amp;ProxyServer&#123; Config: config, Client: client, Proxier: proxier, Broadcaster: eventBroadcaster, Recorder: recorder, Conntracker: &amp;realConntracker&#123;&#125;, NodeRef: nodeRef, HealthzServer: healthzServer, &#125;, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839// cmd/kube-proxy/app/server.gofunc (o *Options) Run() error &#123; defer close(o.errCh) if len(o.WriteConfigTo) &gt; 0 &#123; return o.writeConfigFile() &#125; if o.CleanupAndExit &#123; return cleanupAndExit() &#125; proxyServer, err := NewProxyServer(o) if err != nil &#123; return err &#125; o.proxyServer = proxyServer return o.runLoop()&#125;// 最终调用proxyServer的Run方法func (o *Options) runLoop() error &#123; if o.watcher != nil &#123; o.watcher.Run() &#125; // run the proxy in goroutine go func() &#123; err := o.proxyServer.Run() o.errCh &lt;- err &#125;() for &#123; err := &lt;-o.errCh if err != nil &#123; return err &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153// cmd/kube-proxy/app/server.gotype ProxyServer struct &#123; Config *kubeproxyconfig.KubeProxyConfiguration Client clientset.Interface Broadcaster events.EventBroadcaster Recorder events.EventRecorder Conntracker Conntracker // if nil, ignored NodeRef *v1.ObjectReference HealthzServer healthcheck.ProxierHealthUpdater Proxier proxy.Provider&#125;func (s *ProxyServer) Run() error &#123; // To help debugging, immediately log version klog.InfoS("Version info", "version", version.Get()) klog.InfoS("Golang settings", "GOGC", os.Getenv("GOGC"), "GOMAXPROCS", os.Getenv("GOMAXPROCS"), "GOTRACEBACK", os.Getenv("GOTRACEBACK")) // TODO(vmarmol): Use container config for this. var oomAdjuster *oom.OOMAdjuster if s.Config.OOMScoreAdj != nil &#123; oomAdjuster = oom.NewOOMAdjuster() if err := oomAdjuster.ApplyOOMScoreAdj(0, int(*s.Config.OOMScoreAdj)); err != nil &#123; klog.V(2).InfoS("Failed to apply OOMScore", "err", err) &#125; &#125; if s.Broadcaster != nil &#123; stopCh := make(chan struct&#123;&#125;) s.Broadcaster.StartRecordingToSink(stopCh) &#125; // TODO(thockin): make it possible for healthz and metrics to be on the same port. var errCh chan error if s.Config.BindAddressHardFail &#123; errCh = make(chan error) &#125; // Start up a healthz server if requested serveHealthz(s.HealthzServer, errCh) // Start up a metrics server if requested serveMetrics(s.Config.MetricsBindAddress, s.Config.Mode, s.Config.EnableProfiling, errCh) // Tune conntrack, if requested // Conntracker is always nil for windows if s.Conntracker != nil &#123; max, err := getConntrackMax(s.Config.Conntrack) if err != nil &#123; return err &#125; if max &gt; 0 &#123; err := s.Conntracker.SetMax(max) if err != nil &#123; if err != errReadOnlySysFS &#123; return err &#125; // errReadOnlySysFS is caused by a known docker issue (https://github.com/docker/docker/issues/24000), // the only remediation we know is to restart the docker daemon. // Here we'll send an node event with specific reason and message, the // administrator should decide whether and how to handle this issue, // whether to drain the node and restart docker. Occurs in other container runtimes // as well. // TODO(random-liu): Remove this when the docker bug is fixed. const message = "CRI error: /sys is read-only: " + "cannot modify conntrack limits, problems may arise later (If running Docker, see docker issue #24000)" s.Recorder.Eventf(s.NodeRef, nil, api.EventTypeWarning, err.Error(), "StartKubeProxy", message) &#125; &#125; if s.Config.Conntrack.TCPEstablishedTimeout != nil &amp;&amp; s.Config.Conntrack.TCPEstablishedTimeout.Duration &gt; 0 &#123; timeout := int(s.Config.Conntrack.TCPEstablishedTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPEstablishedTimeout(timeout); err != nil &#123; return err &#125; &#125; if s.Config.Conntrack.TCPCloseWaitTimeout != nil &amp;&amp; s.Config.Conntrack.TCPCloseWaitTimeout.Duration &gt; 0 &#123; timeout := int(s.Config.Conntrack.TCPCloseWaitTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPCloseWaitTimeout(timeout); err != nil &#123; return err &#125; &#125; &#125; noProxyName, err := labels.NewRequirement(apis.LabelServiceProxyName, selection.DoesNotExist, nil) if err != nil &#123; return err &#125; noHeadlessEndpoints, err := labels.NewRequirement(v1.IsHeadlessService, selection.DoesNotExist, nil) if err != nil &#123; return err &#125; labelSelector := labels.NewSelector() labelSelector = labelSelector.Add(*noProxyName, *noHeadlessEndpoints) // Make informers that filter out objects that want a non-default service proxy. informerFactory := informers.NewSharedInformerFactoryWithOptions(s.Client, s.Config.ConfigSyncPeriod.Duration, informers.WithTweakListOptions(func(options *metav1.ListOptions) &#123; options.LabelSelector = labelSelector.String() &#125;)) // Create configs (i.e. Watches for Services and EndpointSlices) // Note: RegisterHandler() calls need to happen before creation of Sources because sources // only notify on changes, and the initial update (on process start) may be lost if no handlers // are registered yet. // 这段代码可以理解为通过Informer机制监听Service serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.Config.ConfigSyncPeriod.Duration) serviceConfig.RegisterEventHandler(s.Proxier) go serviceConfig.Run(wait.NeverStop) // 这段代码可以理解为通过Informer机制监听EndpointSlice endpointSliceConfig := config.NewEndpointSliceConfig(informerFactory.Discovery().V1().EndpointSlices(), s.Config.ConfigSyncPeriod.Duration) endpointSliceConfig.RegisterEventHandler(s.Proxier) go endpointSliceConfig.Run(wait.NeverStop) // This has to start after the calls to NewServiceConfig because that // function must configure its shared informer event handlers first. informerFactory.Start(wait.NeverStop) // Make an informer that selects for our nodename. currentNodeInformerFactory := informers.NewSharedInformerFactoryWithOptions(s.Client, s.Config.ConfigSyncPeriod.Duration, informers.WithTweakListOptions(func(options *metav1.ListOptions) &#123; options.FieldSelector = fields.OneTermEqualSelector("metadata.name", s.NodeRef.Name).String() &#125;)) // 这段代码可以理解为通过Informer机制监听Node nodeConfig := config.NewNodeConfig(currentNodeInformerFactory.Core().V1().Nodes(), s.Config.ConfigSyncPeriod.Duration) // https://issues.k8s.io/111321 if s.Config.DetectLocalMode == kubeproxyconfig.LocalModeNodeCIDR &#123; nodeConfig.RegisterEventHandler(&amp;proxy.NodePodCIDRHandler&#123;&#125;) &#125; nodeConfig.RegisterEventHandler(s.Proxier) go nodeConfig.Run(wait.NeverStop) // This has to start after the calls to NewNodeConfig because that must // configure the shared informer event handler first. currentNodeInformerFactory.Start(wait.NeverStop) // Birth Cry after the birth is successful s.birthCry() // iptables/ipvs Proxier都实现了SyncLoop() go s.Proxier.SyncLoop() return &lt;-errCh&#125; ServiceConfig, endpointSliceConfig, nodeConfig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114// pkg/proxy/config/config.gotype ServiceConfig struct &#123; listerSynced cache.InformerSynced eventHandlers []ServiceHandler&#125;// NewServiceConfig creates a new ServiceConfig.func NewServiceConfig(serviceInformer coreinformers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig &#123; result := &amp;ServiceConfig&#123; listerSynced: serviceInformer.Informer().HasSynced, &#125; serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: result.handleAddService, UpdateFunc: result.handleUpdateService, DeleteFunc: result.handleDeleteService, &#125;, resyncPeriod, ) return result&#125;// Run waits for cache synced and invokes handlers after syncing.func (c *ServiceConfig) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; klog.InfoS("Starting service config controller") if !cache.WaitForNamedCacheSync("service config", stopCh, c.listerSynced) &#123; return &#125; for i := range c.eventHandlers &#123; klog.V(3).InfoS("Calling handler.OnServiceSynced()") // 当所有初始事件处理程序都被调用并且状态完全更新到本地缓存后调用 c.eventHandlers[i].OnServiceSynced() &#125;&#125;type NodeConfig struct &#123; listerSynced cache.InformerSynced eventHandlers []NodeHandler&#125;// NewNodeConfig creates a new NodeConfig.func NewNodeConfig(nodeInformer coreinformers.NodeInformer, resyncPeriod time.Duration) *NodeConfig &#123; result := &amp;NodeConfig&#123; listerSynced: nodeInformer.Informer().HasSynced, &#125; nodeInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: result.handleAddNode, UpdateFunc: result.handleUpdateNode, DeleteFunc: result.handleDeleteNode, &#125;, resyncPeriod, ) return result&#125;// Run starts the goroutine responsible for calling registered handlers.func (c *NodeConfig) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; klog.InfoS("Starting node config controller") if !cache.WaitForNamedCacheSync("node config", stopCh, c.listerSynced) &#123; return &#125; for i := range c.eventHandlers &#123; klog.V(3).InfoS("Calling handler.OnNodeSynced()") c.eventHandlers[i].OnNodeSynced() &#125;&#125;type EndpointSliceConfig struct &#123; listerSynced cache.InformerSynced eventHandlers []EndpointSliceHandler&#125;// NewEndpointSliceConfig creates a new EndpointSliceConfig.func NewEndpointSliceConfig(endpointSliceInformer discoveryinformers.EndpointSliceInformer, resyncPeriod time.Duration) *EndpointSliceConfig &#123; result := &amp;EndpointSliceConfig&#123; listerSynced: endpointSliceInformer.Informer().HasSynced, &#125; endpointSliceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: result.handleAddEndpointSlice, UpdateFunc: result.handleUpdateEndpointSlice, DeleteFunc: result.handleDeleteEndpointSlice, &#125;, resyncPeriod, ) return result&#125;// Run waits for cache synced and invokes handlers after syncing.func (c *EndpointSliceConfig) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; klog.InfoS("Starting endpoint slice config controller") if !cache.WaitForNamedCacheSync("endpoint slice config", stopCh, c.listerSynced) &#123; return &#125; for _, h := range c.eventHandlers &#123; klog.V(3).InfoS("Calling handler.OnEndpointSlicesSynced()") h.OnEndpointSlicesSynced() &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738// pkg/proxy/ipvs/proxier.gofunc (proxier *Proxier) SyncLoop() &#123; // Update healthz timestamp at beginning in case Sync() never succeeds. if proxier.healthzServer != nil &#123; proxier.healthzServer.Updated() &#125; // synthesize "last change queued" time as the informers are syncing. metrics.SyncProxyRulesLastQueuedTimestamp.SetToCurrentTime() // 周期性的执行对应的参数 proxier.syncRunner.Loop(wait.NeverStop)&#125;func NewProxier(ipFamily v1.IPFamily...)&#123; ... // syncRunner初始化，fn为proxier.syncProxyRules // iptables,ipvs都实现了syncProxyRules // 周期性执行的方法`syncProxyRules`就是`kube-proxy`的核心逻辑，根据`Service`,`EndpointSlice`,`Node`的变化分别更新iptables或ipvs proxier.syncRunner = async.NewBoundedFrequencyRunner("sync-runner", proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) ...&#125;// pkg/proxy/iptables/proxier.gofunc (proxier *Proxier) SyncLoop() &#123; // Update healthz timestamp at beginning in case Sync() never succeeds. if proxier.healthzServer != nil &#123; proxier.healthzServer.Updated() &#125; // synthesize "last change queued" time as the informers are syncing. metrics.SyncProxyRulesLastQueuedTimestamp.SetToCurrentTime() proxier.syncRunner.Loop(wait.NeverStop)&#125;func NewProxier(ipFamily v1.IPFamily...) &#123; ... // syncRunner初始化，fn为proxier.syncProxyRules proxier.syncRunner = async.NewBoundedFrequencyRunner("sync-runner", proxier.syncProxyRules, minSyncPeriod, time.Hour, burstSyncs) ...&#125; iptables syncProxyRules这函数代码太长后面再慢慢分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813// pkg/proxy/iptables/proxier.gofunc (proxier *Proxier) syncProxyRules() &#123; proxier.mu.Lock() defer proxier.mu.Unlock() // don't sync rules till we've received services and endpoints if !proxier.isInitialized() &#123; klog.V(2).InfoS("Not syncing iptables until Services and Endpoints have been received from master") return &#125; // The value of proxier.needFullSync may change before the defer funcs run, so // we need to keep track of whether it was set at the *start* of the sync. tryPartialSync := !proxier.needFullSync // Keep track of how long syncs take. start := time.Now() defer func() &#123; metrics.SyncProxyRulesLatency.Observe(metrics.SinceInSeconds(start)) if tryPartialSync &#123; metrics.SyncPartialProxyRulesLatency.Observe(metrics.SinceInSeconds(start)) &#125; else &#123; metrics.SyncFullProxyRulesLatency.Observe(metrics.SinceInSeconds(start)) &#125; klog.V(2).InfoS("SyncProxyRules complete", "elapsed", time.Since(start)) &#125;() var serviceChanged, endpointsChanged sets.Set[string] if tryPartialSync &#123; serviceChanged = proxier.serviceChanges.PendingChanges() endpointsChanged = proxier.endpointsChanges.PendingChanges() &#125; serviceUpdateResult := proxier.svcPortMap.Update(proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) klog.V(2).InfoS("Syncing iptables rules") success := false defer func() &#123; if !success &#123; klog.InfoS("Sync failed", "retryingTime", proxier.syncPeriod) proxier.syncRunner.RetryAfter(proxier.syncPeriod) if tryPartialSync &#123; metrics.IptablesPartialRestoreFailuresTotal.Inc() &#125; // proxier.serviceChanges and proxier.endpointChanges have already // been flushed, so we've lost the state needed to be able to do // a partial sync. proxier.needFullSync = true &#125; &#125;() if !tryPartialSync &#123; // Ensure that our jump rules (eg from PREROUTING to KUBE-SERVICES) exist. // We can't do this as part of the iptables-restore because we don't want // to specify/replace *all* of the rules in PREROUTING, etc. // // We need to create these rules when kube-proxy first starts, and we need // to recreate them if the utiliptables Monitor detects that iptables has // been flushed. In both of those cases, the code will force a full sync. // In all other cases, it ought to be safe to assume that the rules // already exist, so we'll skip this step when doing a partial sync, to // save us from having to invoke /sbin/iptables 20 times on each sync // (which will be very slow on hosts with lots of iptables rules). for _, jump := range append(iptablesJumpChains, iptablesKubeletJumpChains...) &#123; if _, err := proxier.iptables.EnsureChain(jump.table, jump.dstChain); err != nil &#123; klog.ErrorS(err, "Failed to ensure chain exists", "table", jump.table, "chain", jump.dstChain) return &#125; args := jump.extraArgs if jump.comment != "" &#123; args = append(args, "-m", "comment", "--comment", jump.comment) &#125; args = append(args, "-j", string(jump.dstChain)) if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, jump.table, jump.srcChain, args...); err != nil &#123; klog.ErrorS(err, "Failed to ensure chain jumps", "table", jump.table, "srcChain", jump.srcChain, "dstChain", jump.dstChain) return &#125; &#125; &#125; // // Below this point we will not return until we try to write the iptables rules. // // Reset all buffers used later. // This is to avoid memory reallocations and thus improve performance. proxier.filterChains.Reset() proxier.filterRules.Reset() proxier.natChains.Reset() proxier.natRules.Reset() // Write chain lines for all the "top-level" chains we'll be filling in for _, chainName := range []utiliptables.Chain&#123;kubeServicesChain, kubeExternalServicesChain, kubeForwardChain, kubeNodePortsChain, kubeProxyFirewallChain&#125; &#123; proxier.filterChains.Write(utiliptables.MakeChainLine(chainName)) &#125; for _, chainName := range []utiliptables.Chain&#123;kubeServicesChain, kubeNodePortsChain, kubePostroutingChain, kubeMarkMasqChain&#125; &#123; proxier.natChains.Write(utiliptables.MakeChainLine(chainName)) &#125; // Install the kubernetes-specific postrouting rules. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. // NOTE: kubelet creates identical copies of these rules. If you want to change // these rules in the future, you MUST do so in a way that will interoperate // correctly with skewed versions of the rules created by kubelet. (Remove this // comment once IPTablesOwnershipCleanup is GA.) proxier.natRules.Write( "-A", string(kubePostroutingChain), "-m", "mark", "!", "--mark", fmt.Sprintf("%s/%s", proxier.masqueradeMark, proxier.masqueradeMark), "-j", "RETURN", ) // Clear the mark to avoid re-masquerading if the packet re-traverses the network stack. proxier.natRules.Write( "-A", string(kubePostroutingChain), "-j", "MARK", "--xor-mark", proxier.masqueradeMark, ) masqRule := []string&#123; "-A", string(kubePostroutingChain), "-m", "comment", "--comment", `"kubernetes service traffic requiring SNAT"`, "-j", "MASQUERADE", &#125; if proxier.iptables.HasRandomFully() &#123; masqRule = append(masqRule, "--random-fully") &#125; proxier.natRules.Write(masqRule) // Install the kubernetes-specific masquerade mark rule. We use a whole chain for // this so that it is easier to flush and change, for example if the mark // value should ever change. proxier.natRules.Write( "-A", string(kubeMarkMasqChain), "-j", "MARK", "--or-mark", proxier.masqueradeMark, ) isIPv6 := proxier.iptables.IsIPv6() if !isIPv6 &amp;&amp; proxier.localhostNodePorts &#123; // Kube-proxy's use of `route_localnet` to enable NodePorts on localhost // creates a security hole (https://issue.k8s.io/90259) which this // iptables rule mitigates. // NOTE: kubelet creates an identical copy of this rule. If you want to // change this rule in the future, you MUST do so in a way that will // interoperate correctly with skewed versions of the rule created by // kubelet. (Actually, kubelet uses "--dst"/"--src" rather than "-d"/"-s" // but that's just a command-line thing and results in the same rule being // created in the kernel.) proxier.filterChains.Write(utiliptables.MakeChainLine(kubeletFirewallChain)) proxier.filterRules.Write( "-A", string(kubeletFirewallChain), "-m", "comment", "--comment", `"block incoming localnet connections"`, "-d", "127.0.0.0/8", "!", "-s", "127.0.0.0/8", "-m", "conntrack", "!", "--ctstate", "RELATED,ESTABLISHED,DNAT", "-j", "DROP", ) &#125; // Accumulate NAT chains to keep. activeNATChains := map[utiliptables.Chain]bool&#123;&#125; // use a map as a set // To avoid growing this slice, we arbitrarily set its size to 64, // there is never more than that many arguments for a single line. // Note that even if we go over 64, it will still be correct - it // is just for efficiency, not correctness. args := make([]string, 64) // Compute total number of endpoint chains across all services // to get a sense of how big the cluster is. totalEndpoints := 0 for svcName := range proxier.svcPortMap &#123; totalEndpoints += len(proxier.endpointsMap[svcName]) &#125; proxier.largeClusterMode = (totalEndpoints &gt; largeClusterEndpointsThreshold) // These two variables are used to publish the sync_proxy_rules_no_endpoints_total // metric. serviceNoLocalEndpointsTotalInternal := 0 serviceNoLocalEndpointsTotalExternal := 0 // Build rules for each service-port. for svcName, svc := range proxier.svcPortMap &#123; svcInfo, ok := svc.(*servicePortInfo) if !ok &#123; klog.ErrorS(nil, "Failed to cast serviceInfo", "serviceName", svcName) continue &#125; protocol := strings.ToLower(string(svcInfo.Protocol())) svcPortNameString := svcInfo.nameString // Figure out the endpoints for Cluster and Local traffic policy. // allLocallyReachableEndpoints is the set of all endpoints that can be routed to // from this node, given the service's traffic policies. hasEndpoints is true // if the service has any usable endpoints on any node, not just this one. allEndpoints := proxier.endpointsMap[svcName] clusterEndpoints, localEndpoints, allLocallyReachableEndpoints, hasEndpoints := proxy.CategorizeEndpoints(allEndpoints, svcInfo, proxier.nodeLabels) // Note the endpoint chains that will be used for _, ep := range allLocallyReachableEndpoints &#123; if epInfo, ok := ep.(*endpointsInfo); ok &#123; activeNATChains[epInfo.ChainName] = true &#125; &#125; // clusterPolicyChain contains the endpoints used with "Cluster" traffic policy clusterPolicyChain := svcInfo.clusterPolicyChainName usesClusterPolicyChain := len(clusterEndpoints) &gt; 0 &amp;&amp; svcInfo.UsesClusterEndpoints() if usesClusterPolicyChain &#123; activeNATChains[clusterPolicyChain] = true &#125; // localPolicyChain contains the endpoints used with "Local" traffic policy localPolicyChain := svcInfo.localPolicyChainName usesLocalPolicyChain := len(localEndpoints) &gt; 0 &amp;&amp; svcInfo.UsesLocalEndpoints() if usesLocalPolicyChain &#123; activeNATChains[localPolicyChain] = true &#125; // internalPolicyChain is the chain containing the endpoints for // "internal" (ClusterIP) traffic. internalTrafficChain is the chain that // internal traffic is routed to (which is always the same as // internalPolicyChain). hasInternalEndpoints is true if we should // generate rules pointing to internalTrafficChain, or false if there are // no available internal endpoints. internalPolicyChain := clusterPolicyChain hasInternalEndpoints := hasEndpoints if svcInfo.InternalPolicyLocal() &#123; internalPolicyChain = localPolicyChain if len(localEndpoints) == 0 &#123; hasInternalEndpoints = false &#125; &#125; internalTrafficChain := internalPolicyChain // Similarly, externalPolicyChain is the chain containing the endpoints // for "external" (NodePort, LoadBalancer, and ExternalIP) traffic. // externalTrafficChain is the chain that external traffic is routed to // (which is always the service's "EXT" chain). hasExternalEndpoints is // true if there are endpoints that will be reached by external traffic. // (But we may still have to generate externalTrafficChain even if there // are no external endpoints, to ensure that the short-circuit rules for // local traffic are set up.) externalPolicyChain := clusterPolicyChain hasExternalEndpoints := hasEndpoints if svcInfo.ExternalPolicyLocal() &#123; externalPolicyChain = localPolicyChain if len(localEndpoints) == 0 &#123; hasExternalEndpoints = false &#125; &#125; externalTrafficChain := svcInfo.externalChainName // eventually jumps to externalPolicyChain // usesExternalTrafficChain is based on hasEndpoints, not hasExternalEndpoints, // because we need the local-traffic-short-circuiting rules even when there // are no externally-usable endpoints. usesExternalTrafficChain := hasEndpoints &amp;&amp; svcInfo.ExternallyAccessible() if usesExternalTrafficChain &#123; activeNATChains[externalTrafficChain] = true &#125; // Traffic to LoadBalancer IPs can go directly to externalTrafficChain // unless LoadBalancerSourceRanges is in use in which case we will // create a firewall chain. loadBalancerTrafficChain := externalTrafficChain fwChain := svcInfo.firewallChainName usesFWChain := hasEndpoints &amp;&amp; len(svcInfo.LoadBalancerIPStrings()) &gt; 0 &amp;&amp; len(svcInfo.LoadBalancerSourceRanges()) &gt; 0 if usesFWChain &#123; activeNATChains[fwChain] = true loadBalancerTrafficChain = fwChain &#125; var internalTrafficFilterTarget, internalTrafficFilterComment string var externalTrafficFilterTarget, externalTrafficFilterComment string if !hasEndpoints &#123; // The service has no endpoints at all; hasInternalEndpoints and // hasExternalEndpoints will also be false, and we will not // generate any chains in the "nat" table for the service; only // rules in the "filter" table rejecting incoming packets for // the service's IPs. internalTrafficFilterTarget = "REJECT" internalTrafficFilterComment = fmt.Sprintf(`"%s has no endpoints"`, svcPortNameString) externalTrafficFilterTarget = "REJECT" externalTrafficFilterComment = internalTrafficFilterComment &#125; else &#123; if !hasInternalEndpoints &#123; // The internalTrafficPolicy is "Local" but there are no local // endpoints. Traffic to the clusterIP will be dropped, but // external traffic may still be accepted. internalTrafficFilterTarget = "DROP" internalTrafficFilterComment = fmt.Sprintf(`"%s has no local endpoints"`, svcPortNameString) serviceNoLocalEndpointsTotalInternal++ &#125; if !hasExternalEndpoints &#123; // The externalTrafficPolicy is "Local" but there are no // local endpoints. Traffic to "external" IPs from outside // the cluster will be dropped, but traffic from inside // the cluster may still be accepted. externalTrafficFilterTarget = "DROP" externalTrafficFilterComment = fmt.Sprintf(`"%s has no local endpoints"`, svcPortNameString) serviceNoLocalEndpointsTotalExternal++ &#125; &#125; // Capture the clusterIP. if hasInternalEndpoints &#123; proxier.natRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s cluster IP"`, svcPortNameString), "-m", protocol, "-p", protocol, "-d", svcInfo.ClusterIP().String(), "--dport", strconv.Itoa(svcInfo.Port()), "-j", string(internalTrafficChain)) &#125; else &#123; // No endpoints. proxier.filterRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", internalTrafficFilterComment, "-m", protocol, "-p", protocol, "-d", svcInfo.ClusterIP().String(), "--dport", strconv.Itoa(svcInfo.Port()), "-j", internalTrafficFilterTarget, ) &#125; // Capture externalIPs. for _, externalIP := range svcInfo.ExternalIPStrings() &#123; if hasEndpoints &#123; // Send traffic bound for external IPs to the "external // destinations" chain. proxier.natRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s external IP"`, svcPortNameString), "-m", protocol, "-p", protocol, "-d", externalIP, "--dport", strconv.Itoa(svcInfo.Port()), "-j", string(externalTrafficChain)) &#125; if !hasExternalEndpoints &#123; // Either no endpoints at all (REJECT) or no endpoints for // external traffic (DROP anything that didn't get // short-circuited by the EXT chain.) proxier.filterRules.Write( "-A", string(kubeExternalServicesChain), "-m", "comment", "--comment", externalTrafficFilterComment, "-m", protocol, "-p", protocol, "-d", externalIP, "--dport", strconv.Itoa(svcInfo.Port()), "-j", externalTrafficFilterTarget, ) &#125; &#125; // Capture load-balancer ingress. for _, lbip := range svcInfo.LoadBalancerIPStrings() &#123; if hasEndpoints &#123; proxier.natRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s loadbalancer IP"`, svcPortNameString), "-m", protocol, "-p", protocol, "-d", lbip, "--dport", strconv.Itoa(svcInfo.Port()), "-j", string(loadBalancerTrafficChain)) &#125; if usesFWChain &#123; proxier.filterRules.Write( "-A", string(kubeProxyFirewallChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s traffic not accepted by %s"`, svcPortNameString, svcInfo.firewallChainName), "-m", protocol, "-p", protocol, "-d", lbip, "--dport", strconv.Itoa(svcInfo.Port()), "-j", "DROP") &#125; &#125; if !hasExternalEndpoints &#123; // Either no endpoints at all (REJECT) or no endpoints for // external traffic (DROP anything that didn't get short-circuited // by the EXT chain.) for _, lbip := range svcInfo.LoadBalancerIPStrings() &#123; proxier.filterRules.Write( "-A", string(kubeExternalServicesChain), "-m", "comment", "--comment", externalTrafficFilterComment, "-m", protocol, "-p", protocol, "-d", lbip, "--dport", strconv.Itoa(svcInfo.Port()), "-j", externalTrafficFilterTarget, ) &#125; &#125; // Capture nodeports. if svcInfo.NodePort() != 0 &#123; if hasEndpoints &#123; // Jump to the external destination chain. For better or for // worse, nodeports are not subect to loadBalancerSourceRanges, // and we can't change that. proxier.natRules.Write( "-A", string(kubeNodePortsChain), "-m", "comment", "--comment", svcPortNameString, "-m", protocol, "-p", protocol, "--dport", strconv.Itoa(svcInfo.NodePort()), "-j", string(externalTrafficChain)) &#125; if !hasExternalEndpoints &#123; // Either no endpoints at all (REJECT) or no endpoints for // external traffic (DROP anything that didn't get // short-circuited by the EXT chain.) proxier.filterRules.Write( "-A", string(kubeExternalServicesChain), "-m", "comment", "--comment", externalTrafficFilterComment, "-m", "addrtype", "--dst-type", "LOCAL", "-m", protocol, "-p", protocol, "--dport", strconv.Itoa(svcInfo.NodePort()), "-j", externalTrafficFilterTarget, ) &#125; &#125; // Capture healthCheckNodePorts. if svcInfo.HealthCheckNodePort() != 0 &#123; // no matter if node has local endpoints, healthCheckNodePorts // need to add a rule to accept the incoming connection proxier.filterRules.Write( "-A", string(kubeNodePortsChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s health check node port"`, svcPortNameString), "-m", "tcp", "-p", "tcp", "--dport", strconv.Itoa(svcInfo.HealthCheckNodePort()), "-j", "ACCEPT", ) &#125; // If the SVC/SVL/EXT/FW/SEP chains have not changed since the last sync // then we can omit them from the restore input. (We have already marked // them in activeNATChains, so they won't get deleted.) if tryPartialSync &amp;&amp; !serviceChanged.Has(svcName.NamespacedName.String()) &amp;&amp; !endpointsChanged.Has(svcName.NamespacedName.String()) &#123; continue &#125; // Set up internal traffic handling. if hasInternalEndpoints &#123; args = append(args[:0], "-m", "comment", "--comment", fmt.Sprintf(`"%s cluster IP"`, svcPortNameString), "-m", protocol, "-p", protocol, "-d", svcInfo.ClusterIP().String(), "--dport", strconv.Itoa(svcInfo.Port()), ) if proxier.masqueradeAll &#123; proxier.natRules.Write( "-A", string(internalTrafficChain), args, "-j", string(kubeMarkMasqChain)) &#125; else if proxier.localDetector.IsImplemented() &#123; // This masquerades off-cluster traffic to a service VIP. The // idea is that you can establish a static route for your // Service range, routing to any node, and that node will // bridge into the Service for you. Since that might bounce // off-node, we masquerade here. proxier.natRules.Write( "-A", string(internalTrafficChain), args, proxier.localDetector.IfNotLocal(), "-j", string(kubeMarkMasqChain)) &#125; &#125; // Set up external traffic handling (if any "external" destinations are // enabled). All captured traffic for all external destinations should // jump to externalTrafficChain, which will handle some special cases and // then jump to externalPolicyChain. if usesExternalTrafficChain &#123; proxier.natChains.Write(utiliptables.MakeChainLine(externalTrafficChain)) if !svcInfo.ExternalPolicyLocal() &#123; // If we are using non-local endpoints we need to masquerade, // in case we cross nodes. proxier.natRules.Write( "-A", string(externalTrafficChain), "-m", "comment", "--comment", fmt.Sprintf(`"masquerade traffic for %s external destinations"`, svcPortNameString), "-j", string(kubeMarkMasqChain)) &#125; else &#123; // If we are only using same-node endpoints, we can retain the // source IP in most cases. if proxier.localDetector.IsImplemented() &#123; // Treat all locally-originated pod -&gt; external destination // traffic as a special-case. It is subject to neither // form of traffic policy, which simulates going up-and-out // to an external load-balancer and coming back in. proxier.natRules.Write( "-A", string(externalTrafficChain), "-m", "comment", "--comment", fmt.Sprintf(`"pod traffic for %s external destinations"`, svcPortNameString), proxier.localDetector.IfLocal(), "-j", string(clusterPolicyChain)) &#125; // Locally originated traffic (not a pod, but the host node) // still needs masquerade because the LBIP itself is a local // address, so that will be the chosen source IP. proxier.natRules.Write( "-A", string(externalTrafficChain), "-m", "comment", "--comment", fmt.Sprintf(`"masquerade LOCAL traffic for %s external destinations"`, svcPortNameString), "-m", "addrtype", "--src-type", "LOCAL", "-j", string(kubeMarkMasqChain)) // Redirect all src-type=LOCAL -&gt; external destination to the // policy=cluster chain. This allows traffic originating // from the host to be redirected to the service correctly. proxier.natRules.Write( "-A", string(externalTrafficChain), "-m", "comment", "--comment", fmt.Sprintf(`"route LOCAL traffic for %s external destinations"`, svcPortNameString), "-m", "addrtype", "--src-type", "LOCAL", "-j", string(clusterPolicyChain)) &#125; // Anything else falls thru to the appropriate policy chain. if hasExternalEndpoints &#123; proxier.natRules.Write( "-A", string(externalTrafficChain), "-j", string(externalPolicyChain)) &#125; &#125; // Set up firewall chain, if needed if usesFWChain &#123; proxier.natChains.Write(utiliptables.MakeChainLine(fwChain)) // The service firewall rules are created based on the // loadBalancerSourceRanges field. This only works for VIP-like // loadbalancers that preserve source IPs. For loadbalancers which // direct traffic to service NodePort, the firewall rules will not // apply. args = append(args[:0], "-A", string(fwChain), "-m", "comment", "--comment", fmt.Sprintf(`"%s loadbalancer IP"`, svcPortNameString), ) // firewall filter based on each source range allowFromNode := false for _, src := range svcInfo.LoadBalancerSourceRanges() &#123; proxier.natRules.Write(args, "-s", src, "-j", string(externalTrafficChain)) _, cidr, err := netutils.ParseCIDRSloppy(src) if err != nil &#123; klog.ErrorS(err, "Error parsing CIDR in LoadBalancerSourceRanges, dropping it", "cidr", cidr) &#125; else if cidr.Contains(proxier.nodeIP) &#123; allowFromNode = true &#125; &#125; // For VIP-like LBs, the VIP is often added as a local // address (via an IP route rule). In that case, a request // from a node to the VIP will not hit the loadbalancer but // will loop back with the source IP set to the VIP. We // need the following rules to allow requests from this node. if allowFromNode &#123; for _, lbip := range svcInfo.LoadBalancerIPStrings() &#123; proxier.natRules.Write( args, "-s", lbip, "-j", string(externalTrafficChain)) &#125; &#125; // If the packet was able to reach the end of firewall chain, // then it did not get DNATed, so it will match the // corresponding KUBE-PROXY-FIREWALL rule. proxier.natRules.Write( "-A", string(fwChain), "-m", "comment", "--comment", fmt.Sprintf(`"other traffic to %s will be dropped by KUBE-PROXY-FIREWALL"`, svcPortNameString), ) &#125; // If Cluster policy is in use, create the chain and create rules jumping // from clusterPolicyChain to the clusterEndpoints if usesClusterPolicyChain &#123; proxier.natChains.Write(utiliptables.MakeChainLine(clusterPolicyChain)) proxier.writeServiceToEndpointRules(svcPortNameString, svcInfo, clusterPolicyChain, clusterEndpoints, args) &#125; // If Local policy is in use, create the chain and create rules jumping // from localPolicyChain to the localEndpoints if usesLocalPolicyChain &#123; proxier.natChains.Write(utiliptables.MakeChainLine(localPolicyChain)) proxier.writeServiceToEndpointRules(svcPortNameString, svcInfo, localPolicyChain, localEndpoints, args) &#125; // Generate the per-endpoint chains. for _, ep := range allLocallyReachableEndpoints &#123; epInfo, ok := ep.(*endpointsInfo) if !ok &#123; klog.ErrorS(nil, "Failed to cast endpointsInfo", "endpointsInfo", ep) continue &#125; endpointChain := epInfo.ChainName // Create the endpoint chain proxier.natChains.Write(utiliptables.MakeChainLine(endpointChain)) activeNATChains[endpointChain] = true args = append(args[:0], "-A", string(endpointChain)) args = proxier.appendServiceCommentLocked(args, svcPortNameString) // Handle traffic that loops back to the originator with SNAT. proxier.natRules.Write( args, "-s", epInfo.IP(), "-j", string(kubeMarkMasqChain)) // Update client-affinity lists. if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; args = append(args, "-m", "recent", "--name", string(endpointChain), "--set") &#125; // DNAT to final destination. args = append(args, "-m", protocol, "-p", protocol, "-j", "DNAT", "--to-destination", epInfo.Endpoint) proxier.natRules.Write(args) &#125; &#125; // Delete chains no longer in use. Since "iptables-save" can take several seconds // to run on hosts with lots of iptables rules, we don't bother to do this on // every sync in large clusters. (Stale chains will not be referenced by any // active rules, so they're harmless other than taking up memory.) if !proxier.largeClusterMode || time.Since(proxier.lastIPTablesCleanup) &gt; proxier.syncPeriod &#123; var existingNATChains map[utiliptables.Chain]struct&#123;&#125; proxier.iptablesData.Reset() if err := proxier.iptables.SaveInto(utiliptables.TableNAT, proxier.iptablesData); err == nil &#123; existingNATChains = utiliptables.GetChainsFromTable(proxier.iptablesData.Bytes()) for chain := range existingNATChains &#123; if !activeNATChains[chain] &#123; chainString := string(chain) if !isServiceChainName(chainString) &#123; // Ignore chains that aren't ours. continue &#125; // We must (as per iptables) write a chain-line // for it, which has the nice effect of flushing // the chain. Then we can remove the chain. proxier.natChains.Write(utiliptables.MakeChainLine(chain)) proxier.natRules.Write("-X", chainString) &#125; &#125; proxier.lastIPTablesCleanup = time.Now() &#125; else &#123; klog.ErrorS(err, "Failed to execute iptables-save: stale chains will not be deleted") &#125; &#125; // Finally, tail-call to the nodePorts chain. This needs to be after all // other service portal rules. nodeAddresses, err := proxier.nodePortAddresses.GetNodeAddresses(proxier.networkInterfacer) if err != nil &#123; klog.ErrorS(err, "Failed to get node ip address matching nodeport cidrs, services with nodeport may not work as intended", "CIDRs", proxier.nodePortAddresses) &#125; // nodeAddresses may contain dual-stack zero-CIDRs if proxier.nodePortAddresses is empty. // Ensure nodeAddresses only contains the addresses for this proxier's IP family. for addr := range nodeAddresses &#123; if utilproxy.IsZeroCIDR(addr) &amp;&amp; isIPv6 == netutils.IsIPv6CIDRString(addr) &#123; // if any of the addresses is zero cidr of this IP family, non-zero IPs can be excluded. nodeAddresses = sets.New[string](addr) break &#125; &#125; for address := range nodeAddresses &#123; if utilproxy.IsZeroCIDR(address) &#123; destinations := []string&#123;"-m", "addrtype", "--dst-type", "LOCAL"&#125; if isIPv6 &#123; // For IPv6, Regardless of the value of localhostNodePorts is true // or false, we should disable access to the nodePort via localhost. Since it never works and always // cause kernel warnings. destinations = append(destinations, "!", "-d", "::1/128") &#125; if !proxier.localhostNodePorts &amp;&amp; !isIPv6 &#123; // If set localhostNodePorts to "false"(route_localnet=0), We should generate iptables rules that // disable NodePort services to be accessed via localhost. Since it doesn't work and causes // the kernel to log warnings if anyone tries. destinations = append(destinations, "!", "-d", "127.0.0.0/8") &#125; proxier.natRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", `"kubernetes service nodeports; NOTE: this must be the last rule in this chain"`, destinations, "-j", string(kubeNodePortsChain)) break &#125; // Ignore IP addresses with incorrect version if isIPv6 &amp;&amp; !netutils.IsIPv6String(address) || !isIPv6 &amp;&amp; netutils.IsIPv6String(address) &#123; klog.ErrorS(nil, "IP has incorrect IP version", "IP", address) continue &#125; // For ipv6, Regardless of the value of localhostNodePorts is true or false, we should disallow access // to the nodePort via lookBack address. if isIPv6 &amp;&amp; utilproxy.IsLoopBack(address) &#123; klog.ErrorS(nil, "disallow nodePort services to be accessed via ipv6 localhost address", "IP", address) continue &#125; // For ipv4, When localhostNodePorts is set to false, Ignore ipv4 lookBack address if !isIPv6 &amp;&amp; utilproxy.IsLoopBack(address) &amp;&amp; !proxier.localhostNodePorts &#123; klog.ErrorS(nil, "disallow nodePort services to be accessed via ipv4 localhost address", "IP", address) continue &#125; // create nodeport rules for each IP one by one proxier.natRules.Write( "-A", string(kubeServicesChain), "-m", "comment", "--comment", `"kubernetes service nodeports; NOTE: this must be the last rule in this chain"`, "-d", address, "-j", string(kubeNodePortsChain)) &#125; // Drop the packets in INVALID state, which would potentially cause // unexpected connection reset. // https://github.com/kubernetes/kubernetes/issues/74839 proxier.filterRules.Write( "-A", string(kubeForwardChain), "-m", "conntrack", "--ctstate", "INVALID", "-j", "DROP", ) // If the masqueradeMark has been added then we want to forward that same // traffic, this allows NodePort traffic to be forwarded even if the default // FORWARD policy is not accept. proxier.filterRules.Write( "-A", string(kubeForwardChain), "-m", "comment", "--comment", `"kubernetes forwarding rules"`, "-m", "mark", "--mark", fmt.Sprintf("%s/%s", proxier.masqueradeMark, proxier.masqueradeMark), "-j", "ACCEPT", ) // The following rule ensures the traffic after the initial packet accepted // by the "kubernetes forwarding rules" rule above will be accepted. proxier.filterRules.Write( "-A", string(kubeForwardChain), "-m", "comment", "--comment", `"kubernetes forwarding conntrack rule"`, "-m", "conntrack", "--ctstate", "RELATED,ESTABLISHED", "-j", "ACCEPT", ) metrics.IptablesRulesTotal.WithLabelValues(string(utiliptables.TableFilter)).Set(float64(proxier.filterRules.Lines())) metrics.IptablesRulesTotal.WithLabelValues(string(utiliptables.TableNAT)).Set(float64(proxier.natRules.Lines())) // Sync rules. proxier.iptablesData.Reset() proxier.iptablesData.WriteString("*filter\n") proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) proxier.iptablesData.WriteString("COMMIT\n") proxier.iptablesData.WriteString("*nat\n") proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) proxier.iptablesData.WriteString("COMMIT\n") klog.V(2).InfoS("Reloading service iptables data", "numServices", len(proxier.svcPortMap), "numEndpoints", totalEndpoints, "numFilterChains", proxier.filterChains.Lines(), "numFilterRules", proxier.filterRules.Lines(), "numNATChains", proxier.natChains.Lines(), "numNATRules", proxier.natRules.Lines(), ) klog.V(9).InfoS("Restoring iptables", "rules", proxier.iptablesData.Bytes()) // NOTE: NoFlushTables is used so we don't flush non-kubernetes chains in the table err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil &#123; if pErr, ok := err.(utiliptables.ParseError); ok &#123; lines := utiliptables.ExtractLines(proxier.iptablesData.Bytes(), pErr.Line(), 3) klog.ErrorS(pErr, "Failed to execute iptables-restore", "rules", lines) &#125; else &#123; klog.ErrorS(err, "Failed to execute iptables-restore") &#125; metrics.IptablesRestoreFailuresTotal.Inc() return &#125; success = true proxier.needFullSync = false for name, lastChangeTriggerTimes := range endpointUpdateResult.LastChangeTriggerTimes &#123; for _, lastChangeTriggerTime := range lastChangeTriggerTimes &#123; latency := metrics.SinceInSeconds(lastChangeTriggerTime) metrics.NetworkProgrammingLatency.Observe(latency) klog.V(4).InfoS("Network programming", "endpoint", klog.KRef(name.Namespace, name.Name), "elapsed", latency) &#125; &#125; metrics.SyncProxyRulesNoLocalEndpointsTotal.WithLabelValues("internal").Set(float64(serviceNoLocalEndpointsTotalInternal)) metrics.SyncProxyRulesNoLocalEndpointsTotal.WithLabelValues("external").Set(float64(serviceNoLocalEndpointsTotalExternal)) if proxier.healthzServer != nil &#123; proxier.healthzServer.Updated() &#125; metrics.SyncProxyRulesLastTimestamp.SetToCurrentTime() // Update service healthchecks. The endpoints list might include services that are // not "OnlyLocal", but the services list will not, and the serviceHealthServer // will just drop those endpoints. if err := proxier.serviceHealthServer.SyncServices(proxier.svcPortMap.HealthCheckNodePorts()); err != nil &#123; klog.ErrorS(err, "Error syncing healthcheck services") &#125; if err := proxier.serviceHealthServer.SyncEndpoints(proxier.endpointsMap.LocalReadyEndpoints()); err != nil &#123; klog.ErrorS(err, "Error syncing healthcheck endpoints") &#125; // Finish housekeeping, clear stale conntrack entries for UDP Services conntrack.CleanStaleEntries(proxier.iptables.IsIPv6(), proxier.exec, proxier.svcPortMap, serviceUpdateResult, endpointUpdateResult)&#125; REF:1.cmd/kube-proxy/proxy.go2.cmd/kube-proxy/app/server.go3.cmd/kube-proxy/app/server_others.go4.pkg/proxy/config/config.go5.pkg/proxy/ipvs/proxier.go6.pkg/proxy/iptables/proxier.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之kube-proxy[上]]]></title>
    <url>%2F2023%2F05%2F20%2Fk8s%E4%B9%8Bkube-proxy%2F</url>
    <content type="text"><![CDATA[kube-proxy 是集群中每个节点（node）上所运行的网络代理， 是实现 Kubernetes 服务（Service） 概念的一部分。 kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。 一次请求过程中需要kube-proxy参与吗？ 做个实验:在kube-proxy正常工作的情况下使用上面的yaml文件创建Service,发现服务能正常访问。关闭kube-proxy发现服务也能正常访问。关闭kube-proxy重新创建Service然后你会发现服务不能访问。这里可以先得出一个结论: kube-proxy只会在Service创建时创建一些规则(iptables/ipvs),然后在Service在被访问的过程,kube-proxy并不参与。 使用下面的yaml文件创建Service12345678910111213141516171819202122232425262728293031323334353637# deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: my-curlspec: replicas: 1 selector: matchLabels: app: my-curl strategy: type: RollingUpdate template: metadata: labels: app: my-curl spec: containers: - name: my-curl image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: my-curlspec: selector: app: my-curl ports: - name: http port: 8080 targetPort: 8080 type: NodePort iptables的每条链下面的规则处理顺序都是从上往下逐条遍历，除非遇到DROP,REJECT,RETURN。如果链下面是自定义链，则跳转到对应的自定义链执行链下的所有规则,然后跳转回来执行原来那条链后面的规则。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# # ServiceIP: 10.98.75.173# PodIP: 10.244.0.221➜ ✗ sudo iptables -L -n -t natChain PREROUTING (policy ACCEPT)target prot opt source destination KUBE-SERVICES all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */Chain INPUT (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination KUBE-SERVICES all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */Chain POSTROUTING (policy ACCEPT)target prot opt source destination KUBE-POSTROUTING all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */MASQUERADE all -- 192.168.250.0/24 !192.168.250.0/24 /* managed by anbox-bridge */FLANNEL-POSTRTG all -- 0.0.0.0/0 0.0.0.0/0 /* flanneld masq */Chain FLANNEL-POSTRTG (1 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 mark match 0x4000/0x4000 /* flanneld masq */RETURN all -- 10.244.0.0/24 10.244.0.0/16 /* flanneld masq */RETURN all -- 10.244.0.0/16 10.244.0.0/24 /* flanneld masq */RETURN all -- !10.244.0.0/16 10.244.0.0/24 /* flanneld masq */MASQUERADE all -- 10.244.0.0/16 !224.0.0.0/4 /* flanneld masq */ random-fullyMASQUERADE all -- !10.244.0.0/16 10.244.0.0/16 /* flanneld masq */ random-fullyChain KUBE-EXT-W7DCC3F5ZN5NE3KW (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 0.0.0.0/0 0.0.0.0/0 /* masquerade traffic for default/my-curl:http external destinations */KUBE-SVC-W7DCC3F5ZN5NE3KW all -- 0.0.0.0/0 0.0.0.0/0 Chain KUBE-KUBELET-CANARY (0 references)target prot opt source destination Chain KUBE-MARK-DROP (0 references)target prot opt source destination MARK all -- 0.0.0.0/0 0.0.0.0/0 MARK or 0x8000Chain KUBE-MARK-MASQ (14 references)target prot opt source destination MARK all -- 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000Chain KUBE-NODEPORTS (1 references)target prot opt source destination KUBE-EXT-W7DCC3F5ZN5NE3KW tcp -- 0.0.0.0/0 0.0.0.0/0 /* default/my-curl:http */ tcp dpt:31122Chain KUBE-POSTROUTING (1 references)target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 mark match ! 0x4000/0x4000MARK all -- 0.0.0.0/0 0.0.0.0/0 MARK xor 0x4000MASQUERADE all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ random-fullyChain KUBE-PROXY-CANARY (0 references)target prot opt source destination Chain KUBE-SEP-2KOR5GS2OWGIGVHG (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.220 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp */ tcp to:10.244.0.220:53Chain KUBE-SEP-E5ZPFNRTFC3O2YGB (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 192.168.2.123 0.0.0.0/0 /* default/kubernetes:https */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* default/kubernetes:https */ tcp to:192.168.2.123:6443Chain KUBE-SEP-ELL5SOECSE25BPHX (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.220 0.0.0.0/0 /* kube-system/kube-dns:dns */DNAT udp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns */ udp to:10.244.0.220:53Chain KUBE-SEP-G3W5CCH2EXTJTWNT (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.219 0.0.0.0/0 /* kube-system/kube-dns:metrics */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:metrics */ tcp to:10.244.0.219:9153Chain KUBE-SEP-G5TPU3TRPGLSSBUB (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.220 0.0.0.0/0 /* kube-system/kube-dns:metrics */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:metrics */ tcp to:10.244.0.220:9153Chain KUBE-SEP-IJMQAJ4HGPA7P6ZX (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.219 0.0.0.0/0 /* kube-system/kube-dns:dns */DNAT udp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns */ udp to:10.244.0.219:53Chain KUBE-SEP-SZ5GI3C7S6AQLVNR (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.221 0.0.0.0/0 /* default/my-curl:http */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* default/my-curl:http */ tcp to:10.244.0.221:8080Chain KUBE-SEP-ZXBKTOI5KHHUPKIG (1 references)target prot opt source destination KUBE-MARK-MASQ all -- 10.244.0.219 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp */DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp */ tcp to:10.244.0.219:53Chain KUBE-SERVICES (2 references)target prot opt source destination KUBE-SVC-JD5MR3NA4I4DYORP tcp -- 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153KUBE-SVC-W7DCC3F5ZN5NE3KW tcp -- 0.0.0.0/0 10.98.75.173 /* default/my-curl:http cluster IP */ tcp dpt:8080KUBE-SVC-NPX46M4PTMTKRN6Y tcp -- 0.0.0.0/0 10.96.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:443KUBE-SVC-TCOU7JCQXEZGVUNU udp -- 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns cluster IP */ udp dpt:53KUBE-SVC-ERIFXISQEP7F7OF4 tcp -- 0.0.0.0/0 10.96.0.10 /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53KUBE-NODEPORTS all -- 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCALChain KUBE-SVC-ERIFXISQEP7F7OF4 (1 references)target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.96.0.10 /* kube-system/kube-dns:dns-tcp cluster IP */ tcp dpt:53KUBE-SEP-ZXBKTOI5KHHUPKIG all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp -&gt; 10.244.0.219:53 */ statistic mode random probability 0.50000000000KUBE-SEP-2KOR5GS2OWGIGVHG all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns-tcp -&gt; 10.244.0.220:53 */Chain KUBE-SVC-JD5MR3NA4I4DYORP (1 references)target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.96.0.10 /* kube-system/kube-dns:metrics cluster IP */ tcp dpt:9153KUBE-SEP-G3W5CCH2EXTJTWNT all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:metrics -&gt; 10.244.0.219:9153 */ statistic mode random probability 0.50000000000KUBE-SEP-G5TPU3TRPGLSSBUB all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:metrics -&gt; 10.244.0.220:9153 */Chain KUBE-SVC-NPX46M4PTMTKRN6Y (1 references)target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.96.0.1 /* default/kubernetes:https cluster IP */ tcp dpt:443KUBE-SEP-E5ZPFNRTFC3O2YGB all -- 0.0.0.0/0 0.0.0.0/0 /* default/kubernetes:https -&gt; 192.168.2.123:6443 */Chain KUBE-SVC-TCOU7JCQXEZGVUNU (1 references)target prot opt source destination KUBE-MARK-MASQ udp -- !10.244.0.0/16 10.96.0.10 /* kube-system/kube-dns:dns cluster IP */ udp dpt:53KUBE-SEP-IJMQAJ4HGPA7P6ZX all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns -&gt; 10.244.0.219:53 */ statistic mode random probability 0.50000000000KUBE-SEP-ELL5SOECSE25BPHX all -- 0.0.0.0/0 0.0.0.0/0 /* kube-system/kube-dns:dns -&gt; 10.244.0.220:53 */Chain KUBE-SVC-W7DCC3F5ZN5NE3KW (2 references)target prot opt source destination KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.98.75.173 /* default/my-curl:http cluster IP */ tcp dpt:8080KUBE-SEP-SZ5GI3C7S6AQLVNR all -- 0.0.0.0/0 0.0.0.0/0 /* default/my-curl:http -&gt; 10.244.0.221:8080 */ 在Chain PREROUTING中的规则执行顺序如下：KUBE-SERVICES我们只关注我们创建的Service这条链KUBE-SVC-W7DCC3F5ZN5NE3KW KUBE-SERVICES —&gt; KUBE-SERVICES —&gt; KUBE-SVC-W7DCC3F5ZN5NE3KW —&gt; KUBE-MARK-MASQ —&gt; KUBE-SEP-SZ5GI3C7S6AQLVNR—&gt; KUBE-MARK-MASQ —&gt; DNAT —&gt; KUBE-NODEPORTS —&gt; KUBE-EXT-W7DCC3F5ZN5NE3KW —&gt; KUBE-MARK-MASQ —&gt;KUBE-SVC-W7DCC3F5ZN5NE3KW —&gt; KUBE-MARK-MASQ —&gt; KUBE-SEP-SZ5GI3C7S6AQLVNR—&gt; KUBE-MARK-MASQ —&gt; DNAT(10.244.0.221:8080)最终会把请求转发到pod内。 在链KUBE-SERVICES下每一个Service都会有一条自定义链，在一次请求过程中需要一个一个遍历执行，如果Service过多则有很大的性能消耗。 这些iptables规则是怎么生成的呢？ kube-proxy会监听以下资源： Endpoints：kube-proxy会监听Service和Endpoints之间的变化。当Service的Endpoint发生变化时，kube-proxy会更新相应的iptables规则或IPVS规则，以确保流量正确地路由到新的Endpoints。 Service：kube-proxy会监听Service的创建、更新和删除。当Service发生变化时，kube-proxy会相应地更新相应的iptables规则或IPVS规则。 Node：kube-proxy会监听Node的变化，例如节点的加入或离开集群。当节点发生变化时，kube-proxy会相应地更新iptables规则或IPVS规则，以确保流量正确地路由到可用的节点。 通过监听这些资源的变化，kube-proxy能够动态地更新网络规则，以确保流量能够正确地路由到集群中的Pod 下篇文章我们将讲下kube-proxy的具体实现。]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet之prober]]></title>
    <url>%2F2023%2F05%2F19%2Fkubelet%E4%B9%8Bprober%2F</url>
    <content type="text"><![CDATA[在 Kubernetes 中，kubelet 组件负责管理和运行每个节点上的容器。kubelet 组件包含一个名为 prober 的子组件，它是用于容器的健康检查的一部分。 prober 是 kubelet 组件的一部分，它负责执行容器的健康检查。健康检查是一种用于确定容器是否正常运行的机制。prober 会定期向容器发送健康检查请求，并根据容器的响应来确定其状态。 prober 支持多种健康检查方式，包括： HTTP 健康检查：向容器的指定端口发送 HTTP 请求，并根据响应的状态码判断容器的健康状态。 TCP 健康检查：向容器的指定端口发送 TCP 连接请求，并根据连接是否成功判断容器的健康状态。 Exec 健康检查：在容器内部执行指定的命令，并根据命令的执行结果判断容器的健康状态。 gRPC 健康检查：与容器内的gRPC服务建立连接，并发送指定的gRPC 请求。根据服务端返回的响应，prober 可以确定服务是否正常运行通过定期执行这些健康检查，prober 可以监测容器的运行状态，并及时采取相应的措施，如重启容器或报告容器的健康状态给 Kubernetes 控制平面。 prober123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130// pkg/kubelet/prober/prober.go// Prober helps to check the liveness/readiness/startup of a container.type prober struct &#123; exec execprobe.Prober http httpprobe.Prober tcp tcpprobe.Prober grpc grpcprobe.Prober runner kubecontainer.CommandRunner recorder record.EventRecorder&#125;func newProber( runner kubecontainer.CommandRunner, recorder record.EventRecorder) *prober &#123; const followNonLocalRedirects = false return &amp;prober&#123; exec: execprobe.New(), http: httpprobe.New(followNonLocalRedirects), tcp: tcpprobe.New(), grpc: grpcprobe.New(), runner: runner, recorder: recorder, &#125;&#125;func (pb *prober) probe(ctx context.Context, probeType probeType, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (results.Result, error) &#123; var probeSpec *v1.Probe switch probeType &#123; case readiness: probeSpec = container.ReadinessProbe case liveness: probeSpec = container.LivenessProbe case startup: probeSpec = container.StartupProbe default: return results.Failure, fmt.Errorf("unknown probe type: %q", probeType) &#125; if probeSpec == nil &#123; klog.InfoS("Probe is nil", "probeType", probeType, "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name) return results.Success, nil &#125; result, output, err := pb.runProbeWithRetries(ctx, probeType, probeSpec, pod, status, container, containerID, maxProbeRetries) if err != nil || (result != probe.Success &amp;&amp; result != probe.Warning) &#123; // Probe failed in one way or another. if err != nil &#123; klog.V(1).ErrorS(err, "Probe errored", "probeType", probeType, "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name) pb.recordContainerEvent(pod, &amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, "%s probe errored: %v", probeType, err) &#125; else &#123; // result != probe.Success klog.V(1).InfoS("Probe failed", "probeType", probeType, "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name, "probeResult", result, "output", output) pb.recordContainerEvent(pod, &amp;container, v1.EventTypeWarning, events.ContainerUnhealthy, "%s probe failed: %s", probeType, output) &#125; return results.Failure, err &#125; if result == probe.Warning &#123; pb.recordContainerEvent(pod, &amp;container, v1.EventTypeWarning, events.ContainerProbeWarning, "%s probe warning: %s", probeType, output) klog.V(3).InfoS("Probe succeeded with a warning", "probeType", probeType, "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name, "output", output) &#125; else &#123; klog.V(3).InfoS("Probe succeeded", "probeType", probeType, "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", container.Name) &#125; return results.Success, nil&#125;// 有重试的probe, 成功后立刻返回；如果一直不成功返回最后一次的结果func (pb *prober) runProbeWithRetries(ctx context.Context, probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID, retries int) (probe.Result, string, error) &#123; var err error var result probe.Result var output string for i := 0; i &lt; retries; i++ &#123; result, output, err = pb.runProbe(ctx, probeType, p, pod, status, container, containerID) if err == nil &#123; return result, output, nil &#125; &#125; return result, output, err&#125;func (pb *prober) runProbe(ctx context.Context, probeType probeType, p *v1.Probe, pod *v1.Pod, status v1.PodStatus, container v1.Container, containerID kubecontainer.ContainerID) (probe.Result, string, error) &#123; timeout := time.Duration(p.TimeoutSeconds) * time.Second // 分别执行对应的健康检查 if p.Exec != nil &#123; klog.V(4).InfoS("Exec-Probe runProbe", "pod", klog.KObj(pod), "containerName", container.Name, "execCommand", p.Exec.Command) command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) return pb.exec.Probe(pb.newExecInContainer(ctx, container, containerID, command, timeout)) &#125; if p.HTTPGet != nil &#123; req, err := httpprobe.NewRequestForHTTPGetAction(p.HTTPGet, &amp;container, status.PodIP, "probe") if err != nil &#123; return probe.Unknown, "", err &#125; if klogV4 := klog.V(4); klogV4.Enabled() &#123; port := req.URL.Port() host := req.URL.Hostname() path := req.URL.Path scheme := req.URL.Scheme headers := p.HTTPGet.HTTPHeaders klogV4.InfoS("HTTP-Probe", "scheme", scheme, "host", host, "port", port, "path", path, "timeout", timeout, "headers", headers) &#125; return pb.http.Probe(req, timeout) &#125; if p.TCPSocket != nil &#123; port, err := probe.ResolveContainerPort(p.TCPSocket.Port, &amp;container) if err != nil &#123; return probe.Unknown, "", err &#125; host := p.TCPSocket.Host if host == "" &#123; host = status.PodIP &#125; klog.V(4).InfoS("TCP-Probe", "host", host, "port", port, "timeout", timeout) return pb.tcp.Probe(host, port, timeout) &#125; if p.GRPC != nil &#123; host := status.PodIP service := "" if p.GRPC.Service != nil &#123; service = *p.GRPC.Service &#125; klog.V(4).InfoS("GRPC-Probe", "host", host, "service", service, "port", p.GRPC.Port, "timeout", timeout) return pb.grpc.Probe(host, service, int(p.GRPC.Port), timeout) &#125; klog.InfoS("Failed to find probe builder for container", "containerName", container.Name) return probe.Unknown, "", fmt.Errorf("missing probe handler for %s:%s", format.Pod(pod), container.Name)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229// pkg/kubelet/prober/prober_manager.go// Manager它用于管理Pod的探测（probing）过程。对于每个指定了探测（probe）的容器（container）,//Manager会创建一个探测工作者（probe worker）。探测工作者会定期对其所分配的容器进行探//测，并缓存探测结果。当需要时（通过UpdatePodStatus方法请求），Manager会使用缓存的探测结果//来设置相应的Ready状态（PodStatus）type Manager interface &#123; // 为每个创建了probe的容器创建probe workers.每个新建的pod都应该调用这个方法 AddPod(pod *v1.Pod) // StopLivenessAndStartup handles stopping liveness and startup probes during termination. StopLivenessAndStartup(pod *v1.Pod) // 删除removed pod状态,包括终止probe workers和删除缓存 RemovePod(pod *v1.Pod) // CleanupPods handles cleaning up pods which should no longer be running. // It takes a map of "desired pods" which should not be cleaned up. CleanupPods(desiredPods map[types.UID]sets.Empty) // UpdatePodStatus modifies the given PodStatus with the appropriate Ready state for each // container based on container running status, cached probe results and worker states. UpdatePodStatus(types.UID, *v1.PodStatus)&#125;// manager 实现了Manager接口type manager struct &#123; // Map of active workers for probes workers map[probeKey]*worker // Lock for accessing &amp; mutating workers workerLock sync.RWMutex // The statusManager cache provides pod IP and container IDs for probing. statusManager status.Manager // readinessManager manages the results of readiness probes readinessManager results.Manager // livenessManager manages the results of liveness probes livenessManager results.Manager // startupManager manages the results of startup probes startupManager results.Manager // prober executes the probe actions. prober *prober start time.Time&#125;func (m *manager) AddPod(pod *v1.Pod) &#123; m.workerLock.Lock() defer m.workerLock.Unlock() key := probeKey&#123;podUID: pod.UID&#125; for _, c := range pod.Spec.Containers &#123; key.containerName = c.Name if c.StartupProbe != nil &#123; key.probeType = startup if _, ok := m.workers[key]; ok &#123; klog.V(8).ErrorS(nil, "Startup probe already exists for container", "pod", klog.KObj(pod), "containerName", c.Name) return &#125; w := newWorker(m, startup, pod, c) m.workers[key] = w go w.run() &#125; if c.ReadinessProbe != nil &#123; key.probeType = readiness if _, ok := m.workers[key]; ok &#123; klog.V(8).ErrorS(nil, "Readiness probe already exists for container", "pod", klog.KObj(pod), "containerName", c.Name) return &#125; w := newWorker(m, readiness, pod, c) m.workers[key] = w go w.run() &#125; if c.LivenessProbe != nil &#123; key.probeType = liveness if _, ok := m.workers[key]; ok &#123; klog.V(8).ErrorS(nil, "Liveness probe already exists for container", "pod", klog.KObj(pod), "containerName", c.Name) return &#125; w := newWorker(m, liveness, pod, c) m.workers[key] = w go w.run() &#125; &#125;&#125;func (m *manager) StopLivenessAndStartup(pod *v1.Pod) &#123; m.workerLock.RLock() defer m.workerLock.RUnlock() key := probeKey&#123;podUID: pod.UID&#125; for _, c := range pod.Spec.Containers &#123; key.containerName = c.Name for _, probeType := range [...]probeType&#123;liveness, startup&#125; &#123; key.probeType = probeType if worker, ok := m.workers[key]; ok &#123; worker.stop() &#125; &#125; &#125;&#125;func (m *manager) RemovePod(pod *v1.Pod) &#123; m.workerLock.RLock() defer m.workerLock.RUnlock() key := probeKey&#123;podUID: pod.UID&#125; for _, c := range pod.Spec.Containers &#123; key.containerName = c.Name for _, probeType := range [...]probeType&#123;readiness, liveness, startup&#125; &#123; key.probeType = probeType if worker, ok := m.workers[key]; ok &#123; worker.stop() &#125; &#125; &#125;&#125;func (m *manager) CleanupPods(desiredPods map[types.UID]sets.Empty) &#123; m.workerLock.RLock() defer m.workerLock.RUnlock() for key, worker := range m.workers &#123; if _, ok := desiredPods[key.podUID]; !ok &#123; worker.stop() &#125; &#125;&#125;func (m *manager) UpdatePodStatus(podUID types.UID, podStatus *v1.PodStatus) &#123; for i, c := range podStatus.ContainerStatuses &#123; var started bool if c.State.Running == nil &#123; started = false &#125; else if result, ok := m.startupManager.Get(kubecontainer.ParseContainerID(c.ContainerID)); ok &#123; started = result == results.Success &#125; else &#123; // The check whether there is a probe which hasn't run yet. _, exists := m.getWorker(podUID, c.Name, startup) started = !exists &#125; podStatus.ContainerStatuses[i].Started = &amp;started if started &#123; var ready bool if c.State.Running == nil &#123; ready = false &#125; else if result, ok := m.readinessManager.Get(kubecontainer.ParseContainerID(c.ContainerID)); ok &amp;&amp; result == results.Success &#123; ready = true &#125; else &#123; // The check whether there is a probe which hasn't run yet. w, exists := m.getWorker(podUID, c.Name, readiness) ready = !exists // no readinessProbe -&gt; always ready if exists &#123; // Trigger an immediate run of the readinessProbe to update ready state select &#123; case w.manualTriggerCh &lt;- struct&#123;&#125;&#123;&#125;: default: // Non-blocking. klog.InfoS("Failed to trigger a manual run", "probe", w.probeType.String()) &#125; &#125; &#125; podStatus.ContainerStatuses[i].Ready = ready &#125; &#125; // init containers are ready if they have exited with success or if a readiness probe has // succeeded. for i, c := range podStatus.InitContainerStatuses &#123; var ready bool if c.State.Terminated != nil &amp;&amp; c.State.Terminated.ExitCode == 0 &#123; ready = true &#125; podStatus.InitContainerStatuses[i].Ready = ready &#125;&#125;// pkg/kubelet/prober/worker.go// 同期性的执行探针健康检测func (w *worker) run() &#123; ctx := context.Background() probeTickerPeriod := time.Duration(w.spec.PeriodSeconds) * time.Second // If kubelet restarted the probes could be started in rapid succession. // Let the worker wait for a random portion of tickerPeriod before probing. // Do it only if the kubelet has started recently. if probeTickerPeriod &gt; time.Since(w.probeManager.start) &#123; time.Sleep(time.Duration(rand.Float64() * float64(probeTickerPeriod))) &#125; probeTicker := time.NewTicker(probeTickerPeriod) defer func() &#123; // Clean up. probeTicker.Stop() if !w.containerID.IsEmpty() &#123; w.resultsManager.Remove(w.containerID) &#125; w.probeManager.removeWorker(w.pod.UID, w.container.Name, w.probeType) ProberResults.Delete(w.proberResultsSuccessfulMetricLabels) ProberResults.Delete(w.proberResultsFailedMetricLabels) ProberResults.Delete(w.proberResultsUnknownMetricLabels) ProberDuration.Delete(w.proberDurationSuccessfulMetricLabels) ProberDuration.Delete(w.proberDurationUnknownMetricLabels) &#125;()probeLoop: for w.doProbe(ctx) &#123; // Wait for next probe tick. select &#123; case &lt;-w.stopCh: break probeLoop case &lt;-probeTicker.C: case &lt;-w.manualTriggerCh: // continue &#125; &#125;&#125; execProbe123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// pkg/probe/exec/exec.go// 返回结构体execProber(实现了Prober接口)func New() Prober &#123; return execProber&#123;&#125;&#125;// Prober is an interface defining the Probe object for container readiness/liveness checks.type Prober interface &#123; Probe(e exec.Cmd) (probe.Result, string, error)&#125;type execProber struct&#123;&#125;func (pr execProber) Probe(e exec.Cmd) (probe.Result, string, error) &#123; var dataBuffer bytes.Buffer writer := ioutils.LimitWriter(&amp;dataBuffer, maxReadLength) e.SetStderr(writer) e.SetStdout(writer) // Start and Wait are for running a process non-blocking // 以非阻塞的方式执行命令，不等待结果返回 err := e.Start() // 执行成功后必须调用Wait方法释放系统资源 if err == nil &#123; err = e.Wait() &#125; data := dataBuffer.Bytes() klog.V(4).Infof("Exec probe response: %q", string(data)) if err != nil &#123; exit, ok := err.(exec.ExitError) if ok &#123; // ExitStatus() == 0 命令执行成功 if exit.ExitStatus() == 0 &#123; return probe.Success, string(data), nil &#125; return probe.Failure, string(data), nil &#125; timeoutErr, ok := err.(*TimeoutError) if ok &#123; if utilfeature.DefaultFeatureGate.Enabled(features.ExecProbeTimeout) &#123; // When exec probe timeout, data is empty, so we should return timeoutErr.Error() as the stdout. return probe.Failure, timeoutErr.Error(), nil &#125; klog.Warningf("Exec probe timed out after %s but ExecProbeTimeout feature gate was disabled", timeoutErr.Timeout()) &#125; // 返回的error不是ExitError，则返回Unknown return probe.Unknown, "", err &#125; // 命令执行未出错返回Success return probe.Success, string(data), nil&#125; tcpProbe12345678910111213141516171819202122232425262728293031323334// pkg/probe/tcp/tcp.gofunc New() Prober &#123; return tcpProber&#123;&#125;&#125;// Prober is an interface that defines the Probe function for doing TCP readiness/liveness checks.type Prober interface &#123; Probe(host string, port int, timeout time.Duration) (probe.Result, string, error)&#125;type tcpProber struct&#123;&#125;// Probe checks that a TCP connection to the address can be opened.func (pr tcpProber) Probe(host string, port int, timeout time.Duration) (probe.Result, string, error) &#123; return DoTCPProbe(net.JoinHostPort(host, strconv.Itoa(port)), timeout)&#125;// DoTCPProbe checks that a TCP socket to the address can be opened.// If the socket can be opened, it returns Success// If the socket fails to open, it returns Failure.func DoTCPProbe(addr string, timeout time.Duration) (probe.Result, string, error) &#123; d := probe.ProbeDialer() d.Timeout = timeout conn, err := d.Dial("tcp", addr) if err != nil &#123; // Convert errors to failures to handle timeouts. return probe.Failure, err.Error(), nil &#125; err = conn.Close() if err != nil &#123; klog.Errorf("Unexpected error closing TCP probe socket: %v (%#v)", err, err) &#125; return probe.Success, "", nil&#125; httpProbe12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// pkg/probe/http/http.go// New creates Prober that will skip TLS verification while probing.// followNonLocalRedirects configures whether the prober should follow redirects to a different hostname.// If disabled, redirects to other hosts will trigger a warning result.func New(followNonLocalRedirects bool) Prober &#123; tlsConfig := &amp;tls.Config&#123;InsecureSkipVerify: true&#125; return NewWithTLSConfig(tlsConfig, followNonLocalRedirects)&#125;// NewWithTLSConfig takes tls config as parameter.// followNonLocalRedirects configures whether the prober should follow redirects to a different hostname.// If disabled, redirects to other hosts will trigger a warning result.func NewWithTLSConfig(config *tls.Config, followNonLocalRedirects bool) Prober &#123; // We do not want the probe use node's local proxy set. transport := utilnet.SetTransportDefaults( &amp;http.Transport&#123; TLSClientConfig: config, DisableKeepAlives: true, Proxy: http.ProxyURL(nil), DisableCompression: true, // removes Accept-Encoding header // DialContext creates unencrypted TCP connections // and is also used by the transport for HTTPS connection DialContext: probe.ProbeDialer().DialContext, &#125;) return httpProber&#123;transport, followNonLocalRedirects&#125;&#125;// Prober is an interface that defines the Probe function for doing HTTP readiness/liveness checks.type Prober interface &#123; Probe(req *http.Request, timeout time.Duration) (probe.Result, string, error)&#125;type httpProber struct &#123; transport *http.Transport followNonLocalRedirects bool&#125;// Probe returns a ProbeRunner capable of running an HTTP check.func (pr httpProber) Probe(req *http.Request, timeout time.Duration) (probe.Result, string, error) &#123; client := &amp;http.Client&#123; Timeout: timeout, Transport: pr.transport, CheckRedirect: RedirectChecker(pr.followNonLocalRedirects), &#125; return DoHTTPProbe(req, client)&#125;// GetHTTPInterface is an interface for making HTTP requests, that returns a response and error.type GetHTTPInterface interface &#123; Do(req *http.Request) (*http.Response, error)&#125;// DoHTTPProbe checks if a GET request to the url succeeds.// If the HTTP response code is successful (i.e. 400 &gt; code &gt;= 200), it returns Success.// If the HTTP response code is unsuccessful or HTTP communication fails, it returns Failure.// This is exported because some other packages may want to do direct HTTP probes.func DoHTTPProbe(req *http.Request, client GetHTTPInterface) (probe.Result, string, error) &#123; url := req.URL headers := req.Header res, err := client.Do(req) if err != nil &#123; // Convert errors into failures to catch timeouts. return probe.Failure, err.Error(), nil &#125; defer res.Body.Close() b, err := utilio.ReadAtMost(res.Body, maxRespBodyLength) if err != nil &#123; if err == utilio.ErrLimitReached &#123; klog.V(4).Infof("Non fatal body truncation for %s, Response: %v", url.String(), *res) &#125; else &#123; return probe.Failure, "", err &#125; &#125; body := string(b) if res.StatusCode &gt;= http.StatusOK &amp;&amp; res.StatusCode &lt; http.StatusBadRequest &#123; if res.StatusCode &gt;= http.StatusMultipleChoices &#123; // Redirect klog.V(4).Infof("Probe terminated redirects for %s, Response: %v", url.String(), *res) return probe.Warning, fmt.Sprintf("Probe terminated redirects, Response body: %v", body), nil &#125; klog.V(4).Infof("Probe succeeded for %s, Response: %v", url.String(), *res) return probe.Success, body, nil &#125; klog.V(4).Infof("Probe failed for %s with request headers %v, response body: %v", url.String(), headers, body) // Note: Until https://issue.k8s.io/99425 is addressed, this user-facing failure message must not contain the response body. failureMsg := fmt.Sprintf("HTTP probe failed with statuscode: %d", res.StatusCode) return probe.Failure, failureMsg, nil&#125; grpcProbe1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283// pkg/probe/grpc/grpc.go// Prober is an interface that defines the Probe function for doing GRPC readiness/liveness/startup checks.type Prober interface &#123; Probe(host, service string, port int, timeout time.Duration) (probe.Result, string, error)&#125;type grpcProber struct &#123;&#125;// New Prober for execute grpc probefunc New() Prober &#123; return grpcProber&#123;&#125;&#125;// Probe executes a grpc call to check the liveness/readiness/startup of container.// Returns the Result status, command output, and errors if any.// Any failure is considered as a probe failure to mimic grpc_health_probe tool behavior.// err is always nilfunc (p grpcProber) Probe(host, service string, port int, timeout time.Duration) (probe.Result, string, error) &#123; v := version.Get() opts := []grpc.DialOption&#123; grpc.WithUserAgent(fmt.Sprintf("kube-probe/%s.%s", v.Major, v.Minor)), grpc.WithBlock(), grpc.WithTransportCredentials(insecure.NewCredentials()), //credentials are currently not supported grpc.WithContextDialer(func(ctx context.Context, addr string) (net.Conn, error) &#123; return probe.ProbeDialer().DialContext(ctx, "tcp", addr) &#125;), &#125; ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() addr := net.JoinHostPort(host, fmt.Sprintf("%d", port)) conn, err := grpc.DialContext(ctx, addr, opts...) if err != nil &#123; if err == context.DeadlineExceeded &#123; klog.V(4).ErrorS(err, "failed to connect grpc service due to timeout", "addr", addr, "service", service, "timeout", timeout) return probe.Failure, fmt.Sprintf("timeout: failed to connect service %q within %v: %+v", addr, timeout, err), nil &#125; else &#123; klog.V(4).ErrorS(err, "failed to connect grpc service", "service", addr) return probe.Failure, fmt.Sprintf("error: failed to connect service at %q: %+v", addr, err), nil &#125; &#125; defer func() &#123; _ = conn.Close() &#125;() client := grpchealth.NewHealthClient(conn) resp, err := client.Check(metadata.NewOutgoingContext(ctx, make(metadata.MD)), &amp;grpchealth.HealthCheckRequest&#123; Service: service, &#125;) if err != nil &#123; stat, ok := status.FromError(err) if ok &#123; switch stat.Code() &#123; case codes.Unimplemented: klog.V(4).ErrorS(err, "server does not implement the grpc health protocol (grpc.health.v1.Health)", "addr", addr, "service", service) return probe.Failure, fmt.Sprintf("error: this server does not implement the grpc health protocol (grpc.health.v1.Health): %s", stat.Message()), nil case codes.DeadlineExceeded: klog.V(4).ErrorS(err, "rpc request not finished within timeout", "addr", addr, "service", service, "timeout", timeout) return probe.Failure, fmt.Sprintf("timeout: health rpc did not complete within %v", timeout), nil default: klog.V(4).ErrorS(err, "rpc probe failed") &#125; &#125; else &#123; klog.V(4).ErrorS(err, "health rpc probe failed") &#125; return probe.Failure, fmt.Sprintf("error: health rpc probe failed: %+v", err), nil &#125; if resp.GetStatus() != grpchealth.HealthCheckResponse_SERVING &#123; return probe.Failure, fmt.Sprintf("service unhealthy (responded with %q)", resp.GetStatus().String()), nil &#125; return probe.Success, fmt.Sprintf("service healthy"), nil&#125; 启动入口1234567891011121314151617181920212223// pkg/kubelet/kubelet.go// 初始化，NewMainKubelet省略了部分参数func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...) &#123; ... if kubeDeps.ProbeManager != nil &#123; klet.probeManager = kubeDeps.ProbeManager &#125; else &#123; klet.probeManager = prober.NewManager( klet.statusManager, klet.livenessManager, klet.readinessManager, klet.startupManager, klet.runner, kubeDeps.Recorder) &#125; ...&#125;func (kl *Kubelet) SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) &#123; ... // 写入缓存，并启动probe workers kl.probeManager.AddPod(pod) ...&#125; REF:1.pkg/kubelet/prober/prober.go2.pkg/kubelet/prober/prober_manager.go3.pkg/kubelet/prober/worker.go4.pkg/probe/exec/exec.go5.pkg/probe/tcp/tcp.go6.pkg/probe/http/http.go7.pkg/probe/grpc/grpc.go8.pkg/kubelet/kubelet.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet之imagegc-manager]]></title>
    <url>%2F2023%2F05%2F18%2Fkubelet%E4%B9%8Bimagegc-manager%2F</url>
    <content type="text"><![CDATA[imagegc-manager负责管理容器镜像生命周期。当磁盘的使用率达到所设置的值时会将没有使用的镜像给删除掉。 kubelet通过如下的的参数来设置ImageGC策略： ImageGCHighThresholdPercent：触发gc的阈值，超过该值将会执行gc，当值为100时，不启动gc ImageGCLowThresholdPercent：低于这个值不会进行gc ImageMinimumGCAge：最短GC年龄（即距离首次被探测到的间隔），小于该阈值时不会被gc 123456// pkg/kubelet/kubelet.goimageGCPolicy := images.ImageGCPolicy&#123; MinAge: kubeCfg.ImageMinimumGCAge.Duration, HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent), LowThresholdPercent: int(kubeCfg.ImageGCLowThresholdPercent),&#125; 12345678910111213141516171819202122232425262728293031// pkg/kubelet/images/types.go// 定义了镜像拉取时的一些错误类型var ( // ErrImagePullBackOff - Container image pull failed, kubelet is backing off image pull ErrImagePullBackOff = errors.New("ImagePullBackOff") // ErrImageInspect - Unable to inspect image ErrImageInspect = errors.New("ImageInspectError") // ErrImagePull - General image pull error ErrImagePull = errors.New("ErrImagePull") // ErrImageNeverPull - Required Image is absent on host and PullPolicy is NeverPullImage // 镜像拉取策略为Never且镜像不存在对应的节点上 ErrImageNeverPull = errors.New("ErrImageNeverPull") // ErrInvalidImageName - Unable to parse the image name. ErrInvalidImageName = errors.New("InvalidImageName"))// ImageManager provides an interface to manage the lifecycle of images.// Implementations of this interface are expected to deal with pulling (downloading),// managing, and deleting container images.// Implementations are expected to abstract the underlying runtimes.// Implementations are expected to be thread safe.type ImageManager interface &#123; // 判断镜像是否被使用 EnsureImageExists(ctx context.Context, pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error)&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394// pkg/kubelet/images/puller.gotype pullResult struct &#123; imageRef string err error pullDuration time.Duration&#125;type imagePuller interface &#123; pullImage(context.Context, kubecontainer.ImageSpec, []v1.Secret, chan&lt;- pullResult, *runtimeapi.PodSandboxConfig)&#125;// 确保parallelImagePuller,serialImagePuller都实现了imagePuller接口var _, _ imagePuller = &amp;parallelImagePuller&#123;&#125;, &amp;serialImagePuller&#123;&#125;// 通过channel实现并行拉取镜像type parallelImagePuller struct &#123; imageService kubecontainer.ImageService tokens chan struct&#123;&#125;&#125;func newParallelImagePuller(imageService kubecontainer.ImageService, maxParallelImagePulls *int32) imagePuller &#123; if maxParallelImagePulls == nil || *maxParallelImagePulls &lt; 1 &#123; return &amp;parallelImagePuller&#123;imageService, nil&#125; &#125; return &amp;parallelImagePuller&#123;imageService, make(chan struct&#123;&#125;, *maxParallelImagePulls)&#125;&#125;func (pip *parallelImagePuller) pullImage(ctx context.Context, spec kubecontainer.ImageSpec, pullSecrets []v1.Secret, pullChan chan&lt;- pullResult, podSandboxConfig *runtimeapi.PodSandboxConfig) &#123; go func() &#123; // 如果pip.tokens不为nil if pip.tokens != nil &#123; // 发送数据到pip.tokens,如果channel已满则会阻塞 // 否则往下走拉取镜像 pip.tokens &lt;- struct&#123;&#125;&#123;&#125; // 从channel读取数据，释放一个空位 defer func() &#123; &lt;-pip.tokens &#125;() &#125; startTime := time.Now() imageRef, err := pip.imageService.PullImage(ctx, spec, pullSecrets, podSandboxConfig) pullChan &lt;- pullResult&#123; imageRef: imageRef, err: err, pullDuration: time.Since(startTime), &#125; &#125;()&#125;// Maximum number of image pull requests than can be queued.// 最大请求排队数，如果超过了这个数则会发生阻塞const maxImagePullRequests = 10type serialImagePuller struct &#123; imageService kubecontainer.ImageService pullRequests chan *imagePullRequest&#125;func newSerialImagePuller(imageService kubecontainer.ImageService) imagePuller &#123; imagePuller := &amp;serialImagePuller&#123;imageService, make(chan *imagePullRequest, maxImagePullRequests)&#125; // 启动一个协程不断的从pullRequest通道中获取数据并串行拉取镜像 go wait.Until(imagePuller.processImagePullRequests, time.Second, wait.NeverStop) return imagePuller&#125;type imagePullRequest struct &#123; ctx context.Context spec kubecontainer.ImageSpec pullSecrets []v1.Secret pullChan chan&lt;- pullResult podSandboxConfig *runtimeapi.PodSandboxConfig&#125;// 这方法会在EnsureImageExists中调用func (sip *serialImagePuller) pullImage(ctx context.Context, spec kubecontainer.ImageSpec, pullSecrets []v1.Secret, pullChan chan&lt;- pullResult, podSandboxConfig *runtimeapi.PodSandboxConfig) &#123; sip.pullRequests &lt;- &amp;imagePullRequest&#123; ctx: ctx, spec: spec, pullSecrets: pullSecrets, pullChan: pullChan, podSandboxConfig: podSandboxConfig, &#125;&#125;func (sip *serialImagePuller) processImagePullRequests() &#123; for pullRequest := range sip.pullRequests &#123; startTime := time.Now() imageRef, err := sip.imageService.PullImage(pullRequest.ctx, pullRequest.spec, pullRequest.pullSecrets, pullRequest.podSandboxConfig) pullRequest.pullChan &lt;- pullResult&#123; imageRef: imageRef, err: err, pullDuration: time.Since(startTime), &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144// pkg/kubelet/images/image_manager.gotype ImagePodPullingTimeRecorder interface &#123; RecordImageStartedPulling(podUID types.UID) RecordImageFinishedPulling(podUID types.UID)&#125;// imageManager provides the functionalities for image pulling.type imageManager struct &#123; recorder record.EventRecorder imageService kubecontainer.ImageService backOff *flowcontrol.Backoff // It will check the presence of the image, and report the 'image pulling', image pulled' events correspondingly. puller imagePuller podPullingTimeRecorder ImagePodPullingTimeRecorder&#125;// 确保imageManager实现了ImageManager接口var _ ImageManager = &amp;imageManager&#123;&#125;// NewImageManager instantiates a new ImageManager object.func NewImageManager(recorder record.EventRecorder, imageService kubecontainer.ImageService, imageBackOff *flowcontrol.Backoff, serialized bool, maxParallelImagePulls *int32, qps float32, burst int, podPullingTimeRecorder ImagePodPullingTimeRecorder) ImageManager &#123; imageService = throttleImagePulling(imageService, qps, burst) var puller imagePuller // 判断使用哪种镜像拉取策略(串行或并行) if serialized &#123; puller = newSerialImagePuller(imageService) &#125; else &#123; puller = newParallelImagePuller(imageService, maxParallelImagePulls) &#125; return &amp;imageManager&#123; recorder: recorder, imageService: imageService, backOff: imageBackOff, puller: puller, podPullingTimeRecorder: podPullingTimeRecorder, &#125;&#125;// shouldPullImage returns whether we should pull an image according to// the presence and pull policy of the image.func shouldPullImage(container *v1.Container, imagePresent bool) bool &#123; if container.ImagePullPolicy == v1.PullNever &#123; return false &#125; if container.ImagePullPolicy == v1.PullAlways || (container.ImagePullPolicy == v1.PullIfNotPresent &amp;&amp; (!imagePresent)) &#123; return true &#125; return false&#125;// records an event using ref, event msg. log to glog using prefix, msg, logFnfunc (m *imageManager) logIt(ref *v1.ObjectReference, eventtype, event, prefix, msg string, logFn func(args ...interface&#123;&#125;)) &#123; if ref != nil &#123; m.recorder.Event(ref, eventtype, event, msg) &#125; else &#123; logFn(fmt.Sprint(prefix, " ", msg)) &#125;&#125;// EnsureImageExists pulls the image for the specified pod and container, and returns// (imageRef, error message, error).// 为pod和container拉取指定的镜像func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, container *v1.Container, pullSecrets []v1.Secret, podSandboxConfig *runtimeapi.PodSandboxConfig) (string, string, error) &#123; logPrefix := fmt.Sprintf("%s/%s/%s", pod.Namespace, pod.Name, container.Image) // 返回*v1.ObjectReference,指向container(如果这个container属于这个pod) ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil &#123; klog.ErrorS(err, "Couldn't make a ref to pod", "pod", klog.KObj(pod), "containerName", container.Name) &#125; // 如果镜像不包含tag和摘要,为这个镜像设置默认的tag, latest image, err := applyDefaultImageTag(container.Image) if err != nil &#123; msg := fmt.Sprintf("Failed to apply default image tag %q: %v", container.Image, err) m.logIt(ref, v1.EventTypeWarning, events.FailedToInspectImage, logPrefix, msg, klog.Warning) return "", msg, ErrInvalidImageName &#125; // 获取容器注解 var podAnnotations []kubecontainer.Annotation for k, v := range pod.GetAnnotations() &#123; podAnnotations = append(podAnnotations, kubecontainer.Annotation&#123; Name: k, Value: v, &#125;) &#125; spec := kubecontainer.ImageSpec&#123; Image: image, Annotations: podAnnotations, &#125; // 返回镜像ID或digest,如果不存在则返回("", nil) imageRef, err := m.imageService.GetImageRef(ctx, spec) if err != nil &#123; msg := fmt.Sprintf("Failed to inspect image %q: %v", container.Image, err) m.logIt(ref, v1.EventTypeWarning, events.FailedToInspectImage, logPrefix, msg, klog.Warning) return "", msg, ErrImageInspect &#125; // present为true,表示镜像已存在 present := imageRef != "" if !shouldPullImage(container, present) &#123; if present &#123; msg := fmt.Sprintf("Container image %q already present on machine", container.Image) m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, msg, klog.Info) return imageRef, "", nil &#125; msg := fmt.Sprintf("Container image %q is not present with pull policy of Never", container.Image) m.logIt(ref, v1.EventTypeWarning, events.ErrImageNeverPullPolicy, logPrefix, msg, klog.Warning) return "", msg, ErrImageNeverPull &#125; backOffKey := fmt.Sprintf("%s_%s", pod.UID, container.Image) if m.backOff.IsInBackOffSinceUpdate(backOffKey, m.backOff.Clock.Now()) &#123; msg := fmt.Sprintf("Back-off pulling image %q", container.Image) m.logIt(ref, v1.EventTypeNormal, events.BackOffPullImage, logPrefix, msg, klog.Info) return "", msg, ErrImagePullBackOff &#125; m.podPullingTimeRecorder.RecordImageStartedPulling(pod.UID) m.logIt(ref, v1.EventTypeNormal, events.PullingImage, logPrefix, fmt.Sprintf("Pulling image %q", container.Image), klog.Info) startTime := time.Now() pullChan := make(chan pullResult) // 拉取镜像 m.puller.pullImage(ctx, spec, pullSecrets, pullChan, podSandboxConfig) imagePullResult := &lt;-pullChan if imagePullResult.err != nil &#123; m.logIt(ref, v1.EventTypeWarning, events.FailedToPullImage, logPrefix, fmt.Sprintf("Failed to pull image %q: %v", container.Image, imagePullResult.err), klog.Warning) m.backOff.Next(backOffKey, m.backOff.Clock.Now()) msg, err := evalCRIPullErr(container, imagePullResult.err) return "", msg, err &#125; m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID) m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, fmt.Sprintf("Successfully pulled image %q in %v (%v including waiting)", container.Image, imagePullResult.pullDuration.Truncate(time.Millisecond), time.Since(startTime).Truncate(time.Millisecond)), klog.Info) m.backOff.GC() return imagePullResult.imageRef, "", nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356// pkg/kubelet/images/image_gc_manager.go// 定义了ImageGCManager要实现的方法type ImageGCManager interface &#123; // Applies the garbage collection policy. Errors include being unable to free // enough space as per the garbage collection policy. GarbageCollect(ctx context.Context) error // Start async garbage collection of images. Start() GetImageList() ([]container.Image, error) // Delete all unused images. DeleteUnusedImages(ctx context.Context) error&#125;type ImageGCPolicy struct &#123; // Any usage above this threshold will always trigger garbage collection. // This is the highest usage we will allow. // 超过高水位总会解发gc HighThresholdPercent int // Any usage below this threshold will never trigger garbage collection. // This is the lowest threshold we will try to garbage collect to. LowThresholdPercent int // Minimum age at which an image can be garbage collected. MinAge time.Duration&#125;type realImageGCManager struct &#123; // Container runtime runtime container.Runtime // Records of images and their use. imageRecords map[string]*imageRecord imageRecordsLock sync.Mutex // The image garbage collection policy in use. policy ImageGCPolicy // statsProvider provides stats used during image garbage collection. statsProvider StatsProvider // Recorder for Kubernetes events. recorder record.EventRecorder // Reference to this node. nodeRef *v1.ObjectReference // Track initialization initialized bool // imageCache is the cache of latest image list. imageCache imageCache // sandbox image exempted from GC sandboxImage string // tracer for recording spans tracer trace.Tracer&#125;// imageCache实现了set和get方法type imageCache struct &#123; // sync.Mutex is the mutex protects the image cache. sync.Mutex // images is the image cache. images []container.Image&#125;func (i *imageCache) set(images []container.Image) &#123; i.Lock() defer i.Unlock() // The image list needs to be sorted when it gets read and used in // setNodeStatusImages. We sort the list on write instead of on read, // because the image cache is more often read than written sort.Sort(sliceutils.ByImageSize(images)) i.images = images&#125;func (i *imageCache) get() []container.Image &#123; i.Lock() defer i.Unlock() return i.images&#125;// 记录追踪的镜像的一些信息type imageRecord struct &#123; // Time when this image was first detected. firstDetected time.Time // Time when we last saw this image being used. lastUsed time.Time // Size of the image in bytes. size int64 // Pinned status of the image pinned bool&#125;// 创建ImageGCManagerfunc NewImageGCManager(runtime container.Runtime, statsProvider StatsProvider, recorder record.EventRecorder, nodeRef *v1.ObjectReference, policy ImageGCPolicy, sandboxImage string, tracerProvider trace.TracerProvider) (ImageGCManager, error) &#123; // 对策略的一些判断 if policy.HighThresholdPercent &lt; 0 || policy.HighThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf("invalid HighThresholdPercent %d, must be in range [0-100]", policy.HighThresholdPercent) &#125; if policy.LowThresholdPercent &lt; 0 || policy.LowThresholdPercent &gt; 100 &#123; return nil, fmt.Errorf("invalid LowThresholdPercent %d, must be in range [0-100]", policy.LowThresholdPercent) &#125; if policy.LowThresholdPercent &gt; policy.HighThresholdPercent &#123; return nil, fmt.Errorf("LowThresholdPercent %d can not be higher than HighThresholdPercent %d", policy.LowThresholdPercent, policy.HighThresholdPercent) &#125; tracer := tracerProvider.Tracer(instrumentationScope) im := &amp;realImageGCManager&#123; runtime: runtime, policy: policy, imageRecords: make(map[string]*imageRecord), statsProvider: statsProvider, recorder: recorder, nodeRef: nodeRef, initialized: false, sandboxImage: sandboxImage, tracer: tracer, &#125; return im, nil&#125;func (im *realImageGCManager) Start() &#123; ctx := context.Background() // 5分钟执行一次detectImages go wait.Until(func() &#123; // Initial detection make detected time "unknown" in the past. var ts time.Time if im.initialized &#123; ts = time.Now() &#125; _, err := im.detectImages(ctx, ts) if err != nil &#123; klog.InfoS("Failed to monitor images", "err", err) &#125; else &#123; im.initialized = true &#125; &#125;, 5*time.Minute, wait.NeverStop) // Start a goroutine periodically updates image cache. // 每30s更新下imageCache go wait.Until(func() &#123; // 获取镜像列表 images, err := im.runtime.ListImages(ctx) if err != nil &#123; klog.InfoS("Failed to update image list", "err", err) &#125; else &#123; im.imageCache.set(images) &#125; &#125;, 30*time.Second, wait.NeverStop)&#125;func (im *realImageGCManager) detectImages(ctx context.Context, detectTime time.Time) (sets.String, error) &#123; // 定义一个集合记录正在使用中的镜像 imagesInUse := sets.NewString() // Always consider the container runtime pod sandbox image in use // sandboxImage永远被认为在使用中 imageRef, err := im.runtime.GetImageRef(ctx, container.ImageSpec&#123;Image: im.sandboxImage&#125;) if err == nil &amp;&amp; imageRef != "" &#123; imagesInUse.Insert(imageRef) &#125; // 获取当前节点上的所有镜像 images, err := im.runtime.ListImages(ctx) if err != nil &#123; return imagesInUse, err &#125; // 返回pod列表 pods, err := im.runtime.GetPods(ctx, true) if err != nil &#123; return imagesInUse, err &#125; // Make a set of images in use by containers. // 遍历pod中的container将镜像ID添加到集合中 for _, pod := range pods &#123; for _, container := range pod.Containers &#123; klog.V(5).InfoS("Container uses image", "pod", klog.KRef(pod.Namespace, pod.Name), "containerName", container.Name, "containerImage", container.Image, "imageID", container.ImageID) imagesInUse.Insert(container.ImageID) &#125; &#125; // Add new images and record those being used. now := time.Now() currentImages := sets.NewString() im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() // 遍历当前节点上的所有镜像 for _, image := range images &#123; klog.V(5).InfoS("Adding image ID to currentImages", "imageID", image.ID) currentImages.Insert(image.ID) // New image, set it as detected now. if _, ok := im.imageRecords[image.ID]; !ok &#123; // 镜像不存在imageRecords中 klog.V(5).InfoS("Image ID is new", "imageID", image.ID) im.imageRecords[image.ID] = &amp;imageRecord&#123; firstDetected: detectTime, &#125; &#125; // Set last used time to now if the image is being used. if isImageUsed(image.ID, imagesInUse) &#123; klog.V(5).InfoS("Setting Image ID lastUsed", "imageID", image.ID, "lastUsed", now) im.imageRecords[image.ID].lastUsed = now &#125; klog.V(5).InfoS("Image ID has size", "imageID", image.ID, "size", image.Size) im.imageRecords[image.ID].size = image.Size klog.V(5).InfoS("Image ID is pinned", "imageID", image.ID, "pinned", image.Pinned) im.imageRecords[image.ID].pinned = image.Pinned &#125; // Remove old images from our records. for image := range im.imageRecords &#123; if !currentImages.Has(image) &#123; klog.V(5).InfoS("Image ID is no longer present; removing from imageRecords", "imageID", image) // 如果一个不存在这个节点的镜像列表 delete(im.imageRecords, image) &#125; &#125; return imagesInUse, nil&#125;// 会在StartGarbageCollection中被调用，每5分钟执行一次func (im *realImageGCManager) GarbageCollect(ctx context.Context) error &#123; ctx, otelSpan := im.tracer.Start(ctx, "Images/GarbageCollect") defer otelSpan.End() // Get disk usage on disk holding images. fsStats, err := im.statsProvider.ImageFsStats(ctx) if err != nil &#123; return err &#125; var capacity, available int64 if fsStats.CapacityBytes != nil &#123; capacity = int64(*fsStats.CapacityBytes) &#125; if fsStats.AvailableBytes != nil &#123; available = int64(*fsStats.AvailableBytes) &#125; if available &gt; capacity &#123; klog.InfoS("Availability is larger than capacity", "available", available, "capacity", capacity) available = capacity &#125; // Check valid capacity. if capacity == 0 &#123; err := goerrors.New("invalid capacity 0 on image filesystem") im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.InvalidDiskCapacity, err.Error()) return err &#125; // If over the max threshold, free enough to place us at the lower threshold. usagePercent := 100 - int(available*100/capacity) if usagePercent &gt;= im.policy.HighThresholdPercent &#123; amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available klog.InfoS("Disk usage on image filesystem is over the high threshold, trying to free bytes down to the low threshold", "usage", usagePercent, "highThreshold", im.policy.HighThresholdPercent, "amountToFree", amountToFree, "lowThreshold", im.policy.LowThresholdPercent) // 删除不使用的镜像 freed, err := im.freeSpace(ctx, amountToFree, time.Now()) if err != nil &#123; return err &#125; if freed &lt; amountToFree &#123; err := fmt.Errorf("Failed to garbage collect required amount of images. Attempted to free %d bytes, but only found %d bytes eligible to free.", amountToFree, freed) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.FreeDiskSpaceFailed, err.Error()) return err &#125; &#125; return nil&#125;func (im *realImageGCManager) freeSpace(ctx context.Context, bytesToFree int64, freeTime time.Time) (int64, error) &#123; imagesInUse, err := im.detectImages(ctx, freeTime) if err != nil &#123; return 0, err &#125; im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() // Get all images in eviction order. images := make([]evictionInfo, 0, len(im.imageRecords)) for image, record := range im.imageRecords &#123; if isImageUsed(image, imagesInUse) &#123; klog.V(5).InfoS("Image ID is being used", "imageID", image) continue &#125; // Check if image is pinned, prevent garbage collection if record.pinned &#123; klog.V(5).InfoS("Image is pinned, skipping garbage collection", "imageID", image) continue &#125; images = append(images, evictionInfo&#123; id: image, imageRecord: *record, &#125;) &#125; sort.Sort(byLastUsedAndDetected(images)) // Delete unused images until we've freed up enough space. var deletionErrors []error spaceFreed := int64(0) for _, image := range images &#123; klog.V(5).InfoS("Evaluating image ID for possible garbage collection", "imageID", image.id) // Images that are currently in used were given a newer lastUsed. if image.lastUsed.Equal(freeTime) || image.lastUsed.After(freeTime) &#123; klog.V(5).InfoS("Image ID was used too recently, not eligible for garbage collection", "imageID", image.id, "lastUsed", image.lastUsed, "freeTime", freeTime) continue &#125; // Avoid garbage collect the image if the image is not old enough. // In such a case, the image may have just been pulled down, and will be used by a container right away. // 如果镜像加入缓存的时间 &lt; im.policy.MinAge,则不会对镜像进行回收 if freeTime.Sub(image.firstDetected) &lt; im.policy.MinAge &#123; klog.V(5).InfoS("Image ID's age is less than the policy's minAge, not eligible for garbage collection", "imageID", image.id, "age", freeTime.Sub(image.firstDetected), "minAge", im.policy.MinAge) continue &#125; // Remove image. Continue despite errors. klog.InfoS("Removing image to free bytes", "imageID", image.id, "size", image.size) err := im.runtime.RemoveImage(ctx, container.ImageSpec&#123;Image: image.id&#125;) if err != nil &#123; deletionErrors = append(deletionErrors, err) continue &#125; delete(im.imageRecords, image.id) spaceFreed += image.size if spaceFreed &gt;= bytesToFree &#123; break &#125; &#125; if len(deletionErrors) &gt; 0 &#123; return spaceFreed, fmt.Errorf("wanted to free %d bytes, but freed %d bytes space with errors in image deletion: %v", bytesToFree, spaceFreed, errors.NewAggregate(deletionErrors)) &#125; return spaceFreed, nil&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041// cmd/kubelet/app/server.gofunc createAndInitKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, hostname string, hostnameOverridden bool, nodeName types.NodeName, nodeIPs []net.IP) (k kubelet.Bootstrap, err error) &#123; ... k.StartGarbageCollection() ...&#125;func (kl *Kubelet) StartGarbageCollection() &#123; ... if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &#123; klog.V(2).InfoS("ImageGCHighThresholdPercent is set 100, Disable image GC") return &#125; go wait.Until(func() &#123; ctx := context.Background() if err := kl.imageManager.GarbageCollect(ctx); err != nil &#123; if prevImageGCFailed &#123; klog.ErrorS(err, "Image garbage collection failed multiple times in a row") // Only create an event for repeated failures kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.ImageGCFailed, err.Error()) &#125; else &#123; klog.ErrorS(err, "Image garbage collection failed once. Stats initialization may not have completed yet") &#125; prevImageGCFailed = true &#125; else &#123; var vLevel klog.Level = 4 if prevImageGCFailed &#123; vLevel = 1 prevImageGCFailed = false &#125; klog.V(vLevel).InfoS("Image garbage collection succeeded") &#125; &#125;, ImageGCPeriod, wait.NeverStop) ...&#125; REF:1.pkg/kubelet/images/types.go2.pkg/kubelet/images/image_gc_manager.go3.pkg/kubelet/images/image_manager.go4.pkg/kubelet/images/puller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet之pod_manager]]></title>
    <url>%2F2023%2F05%2F17%2Fkubelet%E4%B9%8Bpod-manager%2F</url>
    <content type="text"><![CDATA[Manager定义了访问Pod的一系列方法，维护了static pod和mirror pod之间的关系。kubelet从三个来源获取pod变化:file,http,apiserver。pod中除了apiserver这一来源的pod都叫static pod(apiserver并不知道这些pod的存在)。为了让apiserver感知到这些pod的存在，kubelet为每个static pod都创建了mirror pod，这些mirror pod只能查看不能修改。mirror pod作为static pod的副本(尽管有不同的元数据如UID…)拥有一样的全名称(相同的名称和命名空间)。kubelet通过pod全名向apiserver上报pod状态。当static pod被删除,对应的mirror pod也会被删除。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295// pkg/kubelet/pod/pod_manager.gotype Manager interface &#123; // GetPods returns the regular pods bound to the kubelet and their spec. GetPods() []*v1.Pod // GetPodByFullName returns the (non-mirror) pod that matches full name, as well as // whether the pod was found. GetPodByFullName(podFullName string) (*v1.Pod, bool) // GetPodByName provides the (non-mirror) pod that matches namespace and // name, as well as whether the pod was found. GetPodByName(namespace, name string) (*v1.Pod, bool) // GetPodByUID provides the (non-mirror) pod that matches pod UID, as well as // whether the pod is found. GetPodByUID(types.UID) (*v1.Pod, bool) // GetPodByMirrorPod returns the static pod for the given mirror pod and // whether it was known to the pod manager. GetPodByMirrorPod(*v1.Pod) (*v1.Pod, bool) // GetMirrorPodByPod returns the mirror pod for the given static pod and // whether it was known to the pod manager. GetMirrorPodByPod(*v1.Pod) (*v1.Pod, bool) // GetPodsAndMirrorPods returns the both regular and mirror pods. GetPodsAndMirrorPods() ([]*v1.Pod, []*v1.Pod) // SetPods replaces the internal pods with the new pods. // It is currently only used for testing. SetPods(pods []*v1.Pod) // AddPod adds the given pod to the manager. AddPod(pod *v1.Pod) // UpdatePod updates the given pod in the manager. UpdatePod(pod *v1.Pod) // DeletePod deletes the given pod from the manager. For mirror pods, // this means deleting the mappings related to mirror pods. For non- // mirror pods, this means deleting from indexes for all non-mirror pods. DeletePod(pod *v1.Pod) // GetOrphanedMirrorPodNames returns names of orphaned mirror pods GetOrphanedMirrorPodNames() []string // TranslatePodUID returns the actual UID of a pod. If the UID belongs to // a mirror pod, returns the UID of its static pod. Otherwise, returns the // original UID. // // All public-facing functions should perform this translation for UIDs // because user may provide a mirror pod UID, which is not recognized by // internal Kubelet functions. TranslatePodUID(uid types.UID) kubetypes.ResolvedPodUID // GetUIDTranslations returns the mappings of static pod UIDs to mirror pod // UIDs and mirror pod UIDs to static pod UIDs. GetUIDTranslations() (podToMirror map[kubetypes.ResolvedPodUID]kubetypes.MirrorPodUID, mirrorToPod map[kubetypes.MirrorPodUID]kubetypes.ResolvedPodUID) // IsMirrorPodOf returns true if mirrorPod is a correct representation of // pod; false otherwise. IsMirrorPodOf(mirrorPod, pod *v1.Pod) bool MirrorClient&#125;// basicManager实现Manager接口方法type basicManager struct &#123; // Protects all internal maps. lock sync.RWMutex // Regular pods indexed by UID. podByUID map[kubetypes.ResolvedPodUID]*v1.Pod // Mirror pods indexed by UID. mirrorPodByUID map[kubetypes.MirrorPodUID]*v1.Pod // Pods indexed by full name for easy access. podByFullName map[string]*v1.Pod mirrorPodByFullName map[string]*v1.Pod // Mirror pod UID to pod UID map. translationByUID map[kubetypes.MirrorPodUID]kubetypes.ResolvedPodUID // A mirror pod client to create/delete mirror pods. // MirrorClient实现了CreateMirrorPod和DeleteMirrorPod方法 MirrorClient&#125;// func NewBasicPodManager(client MirrorClient) Manager &#123; pm := &amp;basicManager&#123;&#125; pm.MirrorClient = client pm.SetPods(nil) return pm&#125;// Set the internal pods based on the new pods.func (pm *basicManager) SetPods(newPods []*v1.Pod) &#123; pm.lock.Lock() defer pm.lock.Unlock() // 初始化对象 pm.podByUID = make(map[kubetypes.ResolvedPodUID]*v1.Pod) pm.podByFullName = make(map[string]*v1.Pod) pm.mirrorPodByUID = make(map[kubetypes.MirrorPodUID]*v1.Pod) pm.mirrorPodByFullName = make(map[string]*v1.Pod) pm.translationByUID = make(map[kubetypes.MirrorPodUID]kubetypes.ResolvedPodUID) pm.updatePodsInternal(newPods...)&#125;//添加一个podfunc (pm *basicManager) AddPod(pod *v1.Pod) &#123; pm.UpdatePod(pod)&#125;// 更新podfunc (pm *basicManager) UpdatePod(pod *v1.Pod) &#123; pm.lock.Lock() defer pm.lock.Unlock() pm.updatePodsInternal(pod)&#125;// updateMetrics updates the metrics surfaced by the pod manager.// oldPod or newPod may be nil to signify creation or deletion.func updateMetrics(oldPod, newPod *v1.Pod) &#123; var numEC int if oldPod != nil &#123; numEC -= len(oldPod.Spec.EphemeralContainers) &#125; if newPod != nil &#123; numEC += len(newPod.Spec.EphemeralContainers) &#125; if numEC != 0 &#123; metrics.ManagedEphemeralContainers.Add(float64(numEC)) &#125;&#125;// updatePodsInternal replaces the given pods in the current state of the// manager, updating the various indices. The caller is assumed to hold the// lock.// 更新缓存中的pod,调用此方法需要加锁func (pm *basicManager) updatePodsInternal(pods ...*v1.Pod) &#123; for _, pod := range pods &#123; podFullName := kubecontainer.GetPodFullName(pod) // This logic relies on a static pod and its mirror to have the same name. // It is safe to type convert here due to the IsMirrorPod guard. if kubetypes.IsMirrorPod(pod) &#123; mirrorPodUID := kubetypes.MirrorPodUID(pod.UID) pm.mirrorPodByUID[mirrorPodUID] = pod pm.mirrorPodByFullName[podFullName] = pod if p, ok := pm.podByFullName[podFullName]; ok &#123; pm.translationByUID[mirrorPodUID] = kubetypes.ResolvedPodUID(p.UID) &#125; &#125; else &#123; resolvedPodUID := kubetypes.ResolvedPodUID(pod.UID) updateMetrics(pm.podByUID[resolvedPodUID], pod) pm.podByUID[resolvedPodUID] = pod pm.podByFullName[podFullName] = pod if mirror, ok := pm.mirrorPodByFullName[podFullName]; ok &#123; pm.translationByUID[kubetypes.MirrorPodUID(mirror.UID)] = resolvedPodUID &#125; &#125; &#125;&#125;func (pm *basicManager) DeletePod(pod *v1.Pod) &#123; updateMetrics(pod, nil) pm.lock.Lock() defer pm.lock.Unlock() podFullName := kubecontainer.GetPodFullName(pod) // It is safe to type convert here due to the IsMirrorPod guard. if kubetypes.IsMirrorPod(pod) &#123; mirrorPodUID := kubetypes.MirrorPodUID(pod.UID) delete(pm.mirrorPodByUID, mirrorPodUID) delete(pm.mirrorPodByFullName, podFullName) delete(pm.translationByUID, mirrorPodUID) &#125; else &#123; delete(pm.podByUID, kubetypes.ResolvedPodUID(pod.UID)) delete(pm.podByFullName, podFullName) &#125;&#125;// 获取缓存中的podsfunc (pm *basicManager) GetPods() []*v1.Pod &#123; pm.lock.RLock() defer pm.lock.RUnlock() return podsMapToPods(pm.podByUID)&#125;func (pm *basicManager) GetPodsAndMirrorPods() ([]*v1.Pod, []*v1.Pod) &#123; pm.lock.RLock() defer pm.lock.RUnlock() pods := podsMapToPods(pm.podByUID) mirrorPods := mirrorPodsMapToMirrorPods(pm.mirrorPodByUID) return pods, mirrorPods&#125;func (pm *basicManager) GetPodByUID(uid types.UID) (*v1.Pod, bool) &#123; pm.lock.RLock() defer pm.lock.RUnlock() pod, ok := pm.podByUID[kubetypes.ResolvedPodUID(uid)] // Safe conversion, map only holds non-mirrors. return pod, ok&#125;func (pm *basicManager) GetPodByName(namespace, name string) (*v1.Pod, bool) &#123; podFullName := kubecontainer.BuildPodFullName(name, namespace) return pm.GetPodByFullName(podFullName)&#125;func (pm *basicManager) GetPodByFullName(podFullName string) (*v1.Pod, bool) &#123; pm.lock.RLock() defer pm.lock.RUnlock() pod, ok := pm.podByFullName[podFullName] return pod, ok&#125;func (pm *basicManager) TranslatePodUID(uid types.UID) kubetypes.ResolvedPodUID &#123; // It is safe to type convert to a resolved UID because type conversion is idempotent. if uid == "" &#123; return kubetypes.ResolvedPodUID(uid) &#125; pm.lock.RLock() defer pm.lock.RUnlock() if translated, ok := pm.translationByUID[kubetypes.MirrorPodUID(uid)]; ok &#123; return translated &#125; return kubetypes.ResolvedPodUID(uid)&#125;func (pm *basicManager) GetUIDTranslations() (podToMirror map[kubetypes.ResolvedPodUID]kubetypes.MirrorPodUID, mirrorToPod map[kubetypes.MirrorPodUID]kubetypes.ResolvedPodUID) &#123; pm.lock.RLock() defer pm.lock.RUnlock() podToMirror = make(map[kubetypes.ResolvedPodUID]kubetypes.MirrorPodUID, len(pm.translationByUID)) mirrorToPod = make(map[kubetypes.MirrorPodUID]kubetypes.ResolvedPodUID, len(pm.translationByUID)) // Insert empty translation mapping for all static pods. for uid, pod := range pm.podByUID &#123; if !kubetypes.IsStaticPod(pod) &#123; continue &#125; podToMirror[uid] = "" &#125; // Fill in translations. Notice that if there is no mirror pod for a // static pod, its uid will be translated into empty string "". This // is WAI, from the caller side we can know that the static pod doesn't // have a corresponding mirror pod instead of using static pod uid directly. for k, v := range pm.translationByUID &#123; mirrorToPod[k] = v podToMirror[v] = k &#125; return podToMirror, mirrorToPod&#125;func (pm *basicManager) GetOrphanedMirrorPodNames() []string &#123; pm.lock.RLock() defer pm.lock.RUnlock() var podFullNames []string for podFullName := range pm.mirrorPodByFullName &#123; if _, ok := pm.podByFullName[podFullName]; !ok &#123; podFullNames = append(podFullNames, podFullName) &#125; &#125; return podFullNames&#125;func (pm *basicManager) IsMirrorPodOf(mirrorPod, pod *v1.Pod) bool &#123; // Check name and namespace first. if pod.Name != mirrorPod.Name || pod.Namespace != mirrorPod.Namespace &#123; return false &#125; hash, ok := getHashFromMirrorPod(mirrorPod) if !ok &#123; return false &#125; return hash == getPodHash(pod)&#125;func podsMapToPods(UIDMap map[kubetypes.ResolvedPodUID]*v1.Pod) []*v1.Pod &#123; pods := make([]*v1.Pod, 0, len(UIDMap)) for _, pod := range UIDMap &#123; pods = append(pods, pod) &#125; return pods&#125;func mirrorPodsMapToMirrorPods(UIDMap map[kubetypes.MirrorPodUID]*v1.Pod) []*v1.Pod &#123; pods := make([]*v1.Pod, 0, len(UIDMap)) for _, pod := range UIDMap &#123; pods = append(pods, pod) &#125; return pods&#125;func (pm *basicManager) GetMirrorPodByPod(pod *v1.Pod) (*v1.Pod, bool) &#123; pm.lock.RLock() defer pm.lock.RUnlock() mirrorPod, ok := pm.mirrorPodByFullName[kubecontainer.GetPodFullName(pod)] return mirrorPod, ok&#125;func (pm *basicManager) GetPodByMirrorPod(mirrorPod *v1.Pod) (*v1.Pod, bool) &#123; pm.lock.RLock() defer pm.lock.RUnlock() pod, ok := pm.podByFullName[kubecontainer.GetPodFullName(mirrorPod)] return pod, ok&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133// pkg/kubelet/pod/mirror_client.gotype MirrorClient interface &#123; // CreateMirrorPod creates a mirror pod in the API server for the given // pod or returns an error. The mirror pod will have the same annotations // as the given pod as well as an extra annotation containing the hash of // the static pod. CreateMirrorPod(pod *v1.Pod) error // DeleteMirrorPod deletes the mirror pod with the given full name from // the API server or returns an error. DeleteMirrorPod(podFullName string, uid *types.UID) (bool, error)&#125;// nodeGetter is a subset of NodeLister, simplified for testing.type nodeGetter interface &#123; // Get retrieves the Node for a given name. Get(name string) (*v1.Node, error)&#125;// basicMirrorClient is a functional MirrorClient. Mirror pods are stored in// the kubelet directly because they need to be in sync with the internal// pods.type basicMirrorClient struct &#123; apiserverClient clientset.Interface nodeGetter nodeGetter nodeName string&#125;// NewBasicMirrorClient returns a new MirrorClient.func NewBasicMirrorClient(apiserverClient clientset.Interface, nodeName string, nodeGetter nodeGetter) MirrorClient &#123; return &amp;basicMirrorClient&#123; apiserverClient: apiserverClient, nodeName: nodeName, nodeGetter: nodeGetter, &#125;&#125;// 创建MirrorPodfunc (mc *basicMirrorClient) CreateMirrorPod(pod *v1.Pod) error &#123; if mc.apiserverClient == nil &#123; return nil &#125; // Make a copy of the pod. copyPod := *pod copyPod.Annotations = make(map[string]string) for k, v := range pod.Annotations &#123; copyPod.Annotations[k] = v &#125; hash := getPodHash(pod) copyPod.Annotations[kubetypes.ConfigMirrorAnnotationKey] = hash // With the MirrorPodNodeRestriction feature, mirror pods are required to have an owner reference // to the owning node. // See https://git.k8s.io/enhancements/keps/sig-auth/1314-node-restriction-pods/README.md nodeUID, err := mc.getNodeUID() if err != nil &#123; return fmt.Errorf("failed to get node UID: %v", err) &#125; controller := true copyPod.OwnerReferences = []metav1.OwnerReference&#123;&#123; APIVersion: v1.SchemeGroupVersion.String(), Kind: "Node", Name: mc.nodeName, UID: nodeUID, Controller: &amp;controller, &#125;&#125; apiPod, err := mc.apiserverClient.CoreV1().Pods(copyPod.Namespace).Create(context.TODO(), &amp;copyPod, metav1.CreateOptions&#123;&#125;) if err != nil &amp;&amp; apierrors.IsAlreadyExists(err) &#123; // Check if the existing pod is the same as the pod we want to create. if h, ok := apiPod.Annotations[kubetypes.ConfigMirrorAnnotationKey]; ok &amp;&amp; h == hash &#123; return nil &#125; &#125; return err&#125;// DeleteMirrorPod deletes a mirror pod.// It takes the full name of the pod and optionally a UID. If the UID// is non-nil, the pod is deleted only if its UID matches the supplied UID.// It returns whether the pod was actually deleted, and any error returned// while parsing the name of the pod.// Non-existence of the pod or UID mismatch is not treated as an error; the// routine simply returns false in that case.func (mc *basicMirrorClient) DeleteMirrorPod(podFullName string, uid *types.UID) (bool, error) &#123; if mc.apiserverClient == nil &#123; return false, nil &#125; name, namespace, err := kubecontainer.ParsePodFullName(podFullName) if err != nil &#123; klog.ErrorS(err, "Failed to parse a pod full name", "podFullName", podFullName) return false, err &#125; var uidValue types.UID if uid != nil &#123; uidValue = *uid &#125; klog.V(2).InfoS("Deleting a mirror pod", "pod", klog.KRef(namespace, name), "podUID", uidValue) var GracePeriodSeconds int64 if err := mc.apiserverClient.CoreV1().Pods(namespace).Delete(context.TODO(), name, metav1.DeleteOptions&#123;GracePeriodSeconds: &amp;GracePeriodSeconds, Preconditions: &amp;metav1.Preconditions&#123;UID: uid&#125;&#125;); err != nil &#123; // Unfortunately, there's no generic error for failing a precondition if !(apierrors.IsNotFound(err) || apierrors.IsConflict(err)) &#123; // We should return the error here, but historically this routine does // not return an error unless it can't parse the pod name klog.ErrorS(err, "Failed deleting a mirror pod", "pod", klog.KRef(namespace, name)) &#125; return false, nil &#125; return true, nil&#125;func (mc *basicMirrorClient) getNodeUID() (types.UID, error) &#123; node, err := mc.nodeGetter.Get(mc.nodeName) if err != nil &#123; return "", err &#125; if node.UID == "" &#123; return "", fmt.Errorf("UID unset for node %s", mc.nodeName) &#125; return node.UID, nil&#125;func getHashFromMirrorPod(pod *v1.Pod) (string, bool) &#123; hash, ok := pod.Annotations[kubetypes.ConfigMirrorAnnotationKey] return hash, ok&#125;func getPodHash(pod *v1.Pod) string &#123; // The annotation exists for all static pods. return pod.Annotations[kubetypes.ConfigHashAnnotationKey]&#125; REF:1.pkg/kubelet/pod/mirror_client.go2.pkg/kubelet/pod/pod_manager.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet之pleg]]></title>
    <url>%2F2023%2F05%2F16%2Fkubelet%E4%B9%8Bpleg%2F</url>
    <content type="text"><![CDATA[在k8s中kubelet是运行在k8s节点中的daemon程序，负责管理节点上的pod并将状态上报给apiserver，并驱动对应的pod更新到预期的状态。为此kubelet需要对pod specs和容器状态进行响应。kubelet从多个源中监听pod specs变化,kubelet会定期调用容器运行时获取容器最新的状态。 随着 pod/容器数量的增加，轮询会产生不可忽略的开销，而且因为kubelet的并行性(它会为每一个pod创建一个goroutine调用容器)更加会增加资源的消耗。同期性的大量并发请求将导致容器运行时消耗大量CPU(即使spec/state没有改变)，这将导致极差的性能和一些可靠性问题。 最终，它限制了 Kubelet 的可扩展性。 这个提案的目标是通过降低pod的管理开销以改善kubelet的可扩展性和性能。 减少不必要的工作(如果spec/state没有变化则不需要处理) 降低对容器运行时的并发访问 通过事件驱动的方式代替轮询 通过Relisting获取容器状态变化。为了获取pod生命周期事件，。我们可以通过relisting所有的容器(比如docker ps)，获取PLEG所需要的容器状态变化。虽然这跟原来的轮询有点类似，但这只会有一个线程对容器运行时进行访问。 以上的内容来自于k8s-design-proposals,详细的内容可以自己去看看。 PLEG全称Pod Lifecycle Event Generator PodLifecycleEvent12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// pkg/kubelet/pleg/pleg.go// PodLifecycleEvent is an event that reflects the change of the pod state.// 描述pod状态变化type PodLifecycleEvent struct &#123; // The pod ID. ID types.UID // The type of the event. Type PodLifeCycleEventType // The accompanied data which varies based on the event type. // - ContainerStarted/ContainerStopped: the container name (string). // - All other event types: unused. Data interface&#123;&#125;&#125;type PodLifecycleEventGenerator interface &#123; Start() Stop() Update(relistDuration *RelistDuration) Watch() chan *PodLifecycleEvent Healthy() (bool, error) Relist() UpdateCache(*kubecontainer.Pod, types.UID) (error, bool)&#125;type PodLifeCycleEventType stringtype RelistDuration struct &#123; // The period for relisting. // 相当于轮询周期 RelistPeriod time.Duration // The relisting threshold needs to be greater than the relisting period + // the relisting time, which can vary significantly. Set a conservative // threshold to avoid flipping between healthy and unhealthy. // 这个时间必须大于 relistingPeriod + relisting所花费的时间 RelistThreshold time.Duration&#125;changes// 事件的类型const ( // ContainerStarted - event type when the new state of container is running. ContainerStarted PodLifeCycleEventType = "ContainerStarted" // ContainerDied - event type when the new state of container is exited. ContainerDied PodLifeCycleEventType = "ContainerDied" // ContainerRemoved - event type when the old state of container is exited. ContainerRemoved PodLifeCycleEventType = "ContainerRemoved" // PodSync is used to trigger syncing of a pod when the observed change of // the state of the pod cannot be captured by any single event above. // 当一个pod的状态变化没有被任何一个事件补获到时会触发PodSync PodSync PodLifeCycleEventType = "PodSync" // ContainerChanged - event type when the new state of container is unknown. ContainerChanged PodLifeCycleEventType = "ContainerChanged")// pkg/kubelet/kubelet.gofunc NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...) &#123; ... // pleg和eventedPleg使用的是同一个channel eventChannel := make(chan *pleg.PodLifecycleEvent, plegChannelCapacity) if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) &#123; // adjust Generic PLEG relisting period and threshold to higher value when Evented PLEG is turned on genericRelistDuration := &amp;pleg.RelistDuration&#123; RelistPeriod: eventedPlegRelistPeriod, RelistThreshold: eventedPlegRelistThreshold, &#125; klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock&#123;&#125;) // In case Evented PLEG has to fall back on Generic PLEG due to an error, // Evented PLEG should be able to reset the Generic PLEG relisting duration // to the default value. eventedRelistDuration := &amp;pleg.RelistDuration&#123; RelistPeriod: genericPlegRelistPeriod, RelistThreshold: genericPlegRelistThreshold, &#125; klet.eventedPleg = pleg.NewEventedPLEG(klet.containerRuntime, klet.runtimeService, eventChannel, klet.podCache, klet.pleg, eventedPlegMaxStreamRetries, eventedRelistDuration, clock.RealClock&#123;&#125;) &#125; else &#123; genericRelistDuration := &amp;pleg.RelistDuration&#123; RelistPeriod: genericPlegRelistPeriod, RelistThreshold: genericPlegRelistThreshold, &#125; klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, eventChannel, genericRelistDuration, klet.podCache, clock.RealClock&#123;&#125;) &#125; ...&#125; 1234567891011// pkg/kubelet/kubelet.gofunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; ... // Start the pod lifecycle event generator. kl.pleg.Start() // Feature gate打开后才会远行eventedPLEG if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) &#123; kl.eventedPleg.Start() &#125; ...&#125; GenericPLEGGenericPLEG通过周期性的listing以发现容器变化，它假设在一个relist周期中容器不会发生创建，终止和垃圾回收。如果在relisting过程中发生了容器变化，GenericPLEG将会丢失这些信息。如果此次relisting发生了错误时间窗口会变得更长。许多kubelet的内部组件都依赖于终止的容器作为保留记录的墓碑。垃圾回收器的实现可以处理这种情况。然而，为了保证kubelet能够处理丢失的容器事件，建议将重列周期设置短，并在kubelet中设置一个辅助的、较长的定期同步作为安全措施123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231// pkg/kubelet/pleg/generic.gotype GenericPLEG struct &#123; // The container runtime. runtime kubecontainer.Runtime // The channel from which the subscriber listens events. eventChannel chan *PodLifecycleEvent // The internal cache for pod/container information. podRecords podRecords // Time of the last relisting. relistTime atomic.Value // Cache for storing the runtime states required for syncing pods. cache kubecontainer.Cache // For testability. clock clock.Clock // Pods that failed to have their status retrieved during a relist. These pods will be // retried during the next relisting. podsToReinspect map[types.UID]*kubecontainer.Pod // Stop the Generic PLEG by closing the channel. stopCh chan struct&#123;&#125; // Locks the relisting of the Generic PLEG relistLock sync.Mutex // Indicates if the Generic PLEG is running or not isRunning bool // Locks the start/stop operation of Generic PLEG runningMu sync.Mutex // Indicates relisting related parameters relistDuration *RelistDuration // Mutex to serialize updateCache called by relist vs UpdateCache interface podCacheMutex sync.Mutex&#125;// NewGenericPLEG 创建一个GenericPLEGfunc NewGenericPLEG(runtime kubecontainer.Runtime, eventChannel chan *PodLifecycleEvent, relistDuration *RelistDuration, cache kubecontainer.Cache, clock clock.Clock) PodLifecycleEventGenerator &#123; return &amp;GenericPLEG&#123; relistDuration: relistDuration, runtime: runtime, eventChannel: eventChannel, podRecords: make(podRecords), cache: cache, clock: clock, &#125;&#125;// 运行Relist, 周期为g.relistDuration.RelistPeriodfunc (g *GenericPLEG) Start() &#123; g.runningMu.Lock() defer g.runningMu.Unlock() if !g.isRunning &#123; g.isRunning = true g.stopCh = make(chan struct&#123;&#125;) go wait.Until(g.Relist, g.relistDuration.RelistPeriod, g.stopCh) &#125;&#125;// 定期执行的函数// 查询容器运行时获取pods/containers列表并和本地缓存中的pods/containers比较，然后生成事件func (g *GenericPLEG) Relist() &#123; g.relistLock.Lock() defer g.relistLock.Unlock() ctx := context.Background() klog.V(5).InfoS("GenericPLEG: Relisting") if lastRelistTime := g.getRelistTime(); !lastRelistTime.IsZero() &#123; metrics.PLEGRelistInterval.Observe(metrics.SinceInSeconds(lastRelistTime)) &#125; timestamp := g.clock.Now() defer func() &#123; metrics.PLEGRelistDuration.Observe(metrics.SinceInSeconds(timestamp)) &#125;() // 获取所的Pod podList, err := g.runtime.GetPods(ctx, true) if err != nil &#123; klog.ErrorS(err, "GenericPLEG: Unable to retrieve pods") return &#125; g.updateRelistTime(timestamp) pods := kubecontainer.Pods(podList) // update running pod and container count updateRunningPodAndContainerMetrics(pods) g.podRecords.setCurrent(pods) // Compare the old and the current pods, and generate events. eventsByPodID := map[types.UID][]*PodLifecycleEvent&#123;&#125; for pid := range g.podRecords &#123; oldPod := g.podRecords.getOld(pid) pod := g.podRecords.getCurrent(pid) // 获取oldPod和pod中的container allContainers := getContainersFromPods(oldPod, pod) for _, container := range allContainers &#123; // 生成事件 events := computeEvents(oldPod, pod, &amp;container.ID) for _, e := range events &#123; updateEvents(eventsByPodID, e) &#125; &#125; &#125; var needsReinspection map[types.UID]*kubecontainer.Pod if g.cacheEnabled() &#123; needsReinspection = make(map[types.UID]*kubecontainer.Pod) &#125; // If there are events associated with a pod, we should update the // podCache. for pid, events := range eventsByPodID &#123; pod := g.podRecords.getCurrent(pid) if g.cacheEnabled() &#123; // updateCache() will inspect the pod and update the cache. If an // error occurs during the inspection, we want PLEG to retry again // in the next relist. To achieve this, we do not update the // associated podRecord of the pod, so that the change will be // detect again in the next relist. // TODO: If many pods changed during the same relist period, // inspecting the pod and getting the PodStatus to update the cache // serially may take a while. We should be aware of this and // parallelize if needed. if err, updated := g.updateCache(ctx, pod, pid); err != nil &#123; // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(4).ErrorS(err, "PLEG: Ignoring events for pod", "pod", klog.KRef(pod.Namespace, pod.Name)) // make sure we try to reinspect the pod during the next relisting needsReinspection[pid] = pod continue &#125; else &#123; // this pod was in the list to reinspect and we did so because it had events, so remove it // from the list (we don't want the reinspection code below to inspect it a second time in // this relist execution) delete(g.podsToReinspect, pid) if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) &#123; if !updated &#123; continue &#125; &#125; &#125; &#125; // Update the internal storage and send out the events. g.podRecords.update(pid) // Map from containerId to exit code; used as a temporary cache for lookup containerExitCode := make(map[string]int) for i := range events &#123; // Filter out events that are not reliable and no other components use yet. if events[i].Type == ContainerChanged &#123; continue &#125; select &#123; // 将事件发送到eventChannel中 // 在kubelet的主循环syncLoop中会对eventChannel进行watch case g.eventChannel &lt;- events[i]: default: metrics.PLEGDiscardEvents.Inc() klog.ErrorS(nil, "Event channel is full, discard this relist() cycle event") &#125; // Log exit code of containers when they finished in a particular event if events[i].Type == ContainerDied &#123; // Fill up containerExitCode map for ContainerDied event when first time appeared if len(containerExitCode) == 0 &amp;&amp; pod != nil &amp;&amp; g.cache != nil &#123; // Get updated podStatus status, err := g.cache.Get(pod.ID) if err == nil &#123; for _, containerStatus := range status.ContainerStatuses &#123; containerExitCode[containerStatus.ID.ID] = containerStatus.ExitCode &#125; &#125; &#125; if containerID, ok := events[i].Data.(string); ok &#123; if exitCode, ok := containerExitCode[containerID]; ok &amp;&amp; pod != nil &#123; klog.V(2).InfoS("Generic (PLEG): container finished", "podID", pod.ID, "containerID", containerID, "exitCode", exitCode) &#125; &#125; &#125; &#125; &#125; if g.cacheEnabled() &#123; // reinspect any pods that failed inspection during the previous relist if len(g.podsToReinspect) &gt; 0 &#123; klog.V(5).InfoS("GenericPLEG: Reinspecting pods that previously failed inspection") for pid, pod := range g.podsToReinspect &#123; if err, _ := g.updateCache(ctx, pod, pid); err != nil &#123; // Rely on updateCache calling GetPodStatus to log the actual error. klog.V(5).ErrorS(err, "PLEG: pod failed reinspection", "pod", klog.KRef(pod.Namespace, pod.Name)) needsReinspection[pid] = pod &#125; &#125; &#125; // Update the cache timestamp. This needs to happen *after* // all pods have been properly updated in the cache. g.cache.UpdateTime(timestamp) &#125; // make sure we retain the list of pods that need reinspecting the next time relist is called玄武 g.podsToReinspect = needsReinspection&#125;// pkg/kubelet/kubelet.gofunc (kl *Kubelet) syncLoop(ctx context.Context, updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; ... plegCh := kl.pleg.Watch() for &#123; if err := kl.runtimeState.runtimeErrors(); err != nil &#123; klog.ErrorS(err, "Skipping pod synchronization") // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) // 在syncLoopIteration中处理plegCh,也就是上面的eventChannel if !kl.syncLoopIteration(ctx, updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; EventedPLEG通过事件驱动的PLEG(pod lifecycle event generator)降低relisting频率以提高性能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184// pkg/kubelet/pleg/evented.gotype EventedPLEG struct &#123; // The container runtime. runtime kubecontainer.Runtime // The runtime service. runtimeService internalapi.RuntimeService // The channel from which the subscriber listens events. eventChannel chan *PodLifecycleEvent // Cache for storing the runtime states required for syncing pods. cache kubecontainer.Cache // For testability. clock clock.Clock // GenericPLEG is used to force relist when required. genericPleg PodLifecycleEventGenerator // The maximum number of retries when getting container events from the runtime. eventedPlegMaxStreamRetries int // Indicates relisting related parameters relistDuration *RelistDuration // Stop the Evented PLEG by closing the channel. stopCh chan struct&#123;&#125; // Stops the periodic update of the cache global timestamp. stopCacheUpdateCh chan struct&#123;&#125; // Locks the start/stop operation of the Evented PLEG. runningMu sync.Mutex&#125;// NewEventedPLEG instantiates a new EventedPLEG object and return it.func NewEventedPLEG(runtime kubecontainer.Runtime, runtimeService internalapi.RuntimeService, eventChannel chan *PodLifecycleEvent, cache kubecontainer.Cache, genericPleg PodLifecycleEventGenerator, eventedPlegMaxStreamRetries int, relistDuration *RelistDuration, clock clock.Clock) PodLifecycleEventGenerator &#123; return &amp;EventedPLEG&#123; runtime: runtime, runtimeService: runtimeService, eventChannel: eventChannel, cache: cache, genericPleg: genericPleg, eventedPlegMaxStreamRetries: eventedPlegMaxStreamRetries, relistDuration: relistDuration, clock: clock, &#125;&#125;func (e *EventedPLEG) Start() &#123; e.runningMu.Lock() defer e.runningMu.Unlock() if isEventedPLEGInUse() &#123; return &#125; setEventedPLEGUsage(true) e.stopCh = make(chan struct&#123;&#125;) e.stopCacheUpdateCh = make(chan struct&#123;&#125;) go wait.Until(e.watchEventsChannel, 0, e.stopCh) go wait.Until(e.updateGlobalCache, globalCacheUpdatePeriod, e.stopCacheUpdateCh)&#125;func (e *EventedPLEG) watchEventsChannel() &#123; containerEventsResponseCh := make(chan *runtimeapi.ContainerEventResponse, cap(e.eventChannel)) defer close(containerEventsResponseCh) // Get the container events from the runtime. go func() &#123; numAttempts := 0 for &#123; // 如果重试次数超过一定的次数则表示EventedPLEG if numAttempts &gt;= e.eventedPlegMaxStreamRetries &#123; // 判断是否使用EventedPLEG // 即使打开了EventedPLEG feature gate,EventedPLEG也可能不可用 // 原因可能是容器运行时没实现容器事件流(container events stream) if isEventedPLEGInUse() &#123; // Fall back to Generic PLEG relisting since Evented PLEG is not working. klog.V(4).InfoS("Fall back to Generic PLEG relisting since Evented PLEG is not working") e.Stop() e.genericPleg.Stop() // Stop the existing Generic PLEG which runs with longer relisting period when Evented PLEG is in use. e.Update(e.relistDuration) // Update the relisting period to the default value for the Generic PLEG. // 重新切换加genericPleg e.genericPleg.Start() break &#125; &#125; // 从容器运行中获取事件 err := e.runtimeService.GetContainerEvents(containerEventsResponseCh) if err != nil &#123; metrics.EventedPLEGConnErr.Inc() numAttempts++ // 如果获取事件失败，则进行Relist以获取容器的最新状态 e.Relist() // Force a relist to get the latest container and pods running metric. klog.V(4).InfoS("Evented PLEG: Failed to get container events, retrying: ", "err", err) &#125; &#125; &#125;() if isEventedPLEGInUse() &#123; // 处理容器运行时事件 e.processCRIEvents(containerEventsResponseCh) &#125;&#125;func (e *EventedPLEG) Watch() chan *PodLifecycleEvent &#123; return e.eventChannel&#125;// Relist relists all containers using GenericPLEG// 这里使用的是GenericPLEG中的Relist方法func (e *EventedPLEG) Relist() &#123; e.genericPleg.Relist()&#125;// In case the Evented PLEG experiences undetectable issues in the underlying// GRPC connection there is a remote chance the pod might get stuck in a// given state while it has progressed in its life cycle. This function will be// called periodically to update the global timestamp of the cache so that those// pods stuck at GetNewerThan in pod workers will get unstuck.func (e *EventedPLEG) updateGlobalCache() &#123; e.cache.UpdateTime(time.Now())&#125;func (e *EventedPLEG) processCRIEvents(containerEventsResponseCh chan *runtimeapi.ContainerEventResponse) &#123; for event := range containerEventsResponseCh &#123; // Ignore the event if PodSandboxStatus is nil. // This might happen under some race condition where the podSandbox has // been deleted, and therefore container runtime couldn't find the // podSandbox for the container when generating the event. // It is safe to ignore because // a) a event would have been received for the sandbox deletion, // b) in worst case, a relist will eventually sync the pod status. // TODO(#114371): Figure out a way to handle this case instead of ignoring. if event.PodSandboxStatus == nil || event.PodSandboxStatus.Metadata == nil &#123; klog.ErrorS(nil, "Evented PLEG: received ContainerEventResponse with nil PodSandboxStatus or PodSandboxStatus.Metadata", "containerEventResponse", event) continue &#125; podID := types.UID(event.PodSandboxStatus.Metadata.Uid) shouldSendPLEGEvent := false status, err := e.runtime.GeneratePodStatus(event) if err != nil &#123; // nolint:logcheck // Not using the result of klog.V inside the // if branch is okay, we just use it to determine whether the // additional "podStatus" key and its value should be added. if klog.V(6).Enabled() &#123; klog.ErrorS(err, "Evented PLEG: error generating pod status from the received event", "podUID", podID, "podStatus", status) &#125; else &#123; klog.ErrorS(err, "Evented PLEG: error generating pod status from the received event", "podUID", podID, "podStatus", status) &#125; &#125; else &#123; if klogV := klog.V(6); klogV.Enabled() &#123; klogV.InfoS("Evented PLEG: Generated pod status from the received event", "podUID", podID, "podStatus", status) &#125; else &#123; klog.V(4).InfoS("Evented PLEG: Generated pod status from the received event", "podUID", podID) &#125; // Preserve the pod IP across cache updates if the new IP is empty. // When a pod is torn down, kubelet may race with PLEG and retrieve // a pod status after network teardown, but the kubernetes API expects // the completed pod's IP to be available after the pod is dead. status.IPs = e.getPodIPs(podID, status) &#125; e.updateRunningPodMetric(status) e.updateRunningContainerMetric(status) e.updateLatencyMetric(event) if event.ContainerEventType == runtimeapi.ContainerEventType_CONTAINER_DELETED_EVENT &#123; for _, sandbox := range status.SandboxStatuses &#123; if sandbox.Id == event.ContainerId &#123; // When the CONTAINER_DELETED_EVENT is received by the kubelet, // the runtime has indicated that the container has been removed // by the runtime and hence, it must be removed from the cache // of kubelet too. e.cache.Delete(podID) &#125; &#125; shouldSendPLEGEvent = true &#125; else &#123; if e.cache.Set(podID, status, err, time.Unix(event.GetCreatedAt(), 0)) &#123; shouldSendPLEGEvent = true &#125; &#125; if shouldSendPLEGEvent &#123; // 将事件发送到e.eventChannel中 e.processCRIEvent(event) &#125; &#125;&#125; REF:1.https://github.com/kubernetes/design-proposals-archive/blob/main/node/pod-lifecycle-event-generator.md2.pkg/kubelet/pleg/pleg.go3.pkg/kubelet/kubelet.go4.pkg/kubelet/pleg/evented.go5.pkg/kubelet/pleg/generic.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之podgc]]></title>
    <url>%2F2023%2F05%2F12%2Fkube-controller%E4%B9%8Bpodgc%2F</url>
    <content type="text"><![CDATA[podgc controller负责对pod进行垃圾回收。 如果Terminated状态的数量大于terminatedPodThreshold,删除部分Terminated状态的pod 删除不再服务的node上的pod 删除孤儿pod(一个pod绑定到了一个不存在的Node) 删除Terminating状态且未被调度到某一Node上的pod 123456789101112// cmd/kube-controller-manager/app/core.go// 启动入口func startPodGCController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) &#123; go podgc.NewPodGC( ctx, controllerContext.ClientBuilder.ClientOrDie("pod-garbage-collector"), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.InformerFactory.Core().V1().Nodes(), int(controllerContext.ComponentConfig.PodGCController.TerminatedPodGCThreshold), ).Run(ctx) return nil, true, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336// pkg/controller/podgc/gc_controller.goconst ( // gcCheckPeriod defines frequency of running main controller loop gcCheckPeriod = 20 * time.Second // quarantineTime defines how long Orphaned GC waits for nodes to show up // in an informer before issuing a GET call to check if they are truly gone // 隔离时间，延迟入队时间，在quarantineTime秒后会再次判断node是否真正的消失 // 可以避免一些node短暂离开的情况 quarantineTime = 40 * time.Second // field manager used to add pod failure condition and change the pod phase fieldManager = "PodGC")type PodGCController struct &#123; kubeClient clientset.Interface podLister corelisters.PodLister podListerSynced cache.InformerSynced nodeLister corelisters.NodeLister nodeListerSynced cache.InformerSynced nodeQueue workqueue.DelayingInterface terminatedPodThreshold int gcCheckPeriod time.Duration quarantineTime time.Duration&#125;func init() &#123; // Register prometheus metrics RegisterMetrics()&#125;func NewPodGC(ctx context.Context, kubeClient clientset.Interface, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, terminatedPodThreshold int) *PodGCController &#123; return NewPodGCInternal(ctx, kubeClient, podInformer, nodeInformer, terminatedPodThreshold, gcCheckPeriod, quarantineTime)&#125;// This function is only intended for integration testsfunc NewPodGCInternal(ctx context.Context, kubeClient clientset.Interface, podInformer coreinformers.PodInformer, nodeInformer coreinformers.NodeInformer, terminatedPodThreshold int, gcCheckPeriod, quarantineTime time.Duration) *PodGCController &#123; gcc := &amp;PodGCController&#123; kubeClient: kubeClient, terminatedPodThreshold: terminatedPodThreshold, podLister: podInformer.Lister(), podListerSynced: podInformer.Informer().HasSynced, nodeLister: nodeInformer.Lister(), nodeListerSynced: nodeInformer.Informer().HasSynced, nodeQueue: workqueue.NewNamedDelayingQueue("orphaned_pods_nodes"), gcCheckPeriod: gcCheckPeriod, quarantineTime: quarantineTime, &#125; return gcc&#125;func (gcc *PodGCController) Run(ctx context.Context) &#123; defer utilruntime.HandleCrash() klog.Infof("Starting GC controller") defer gcc.nodeQueue.ShutDown() defer klog.Infof("Shutting down GC controller") if !cache.WaitForNamedCacheSync("GC", ctx.Done(), gcc.podListerSynced, gcc.nodeListerSynced) &#123; return &#125; go wait.UntilWithContext(ctx, gcc.gc, gcc.gcCheckPeriod) &lt;-ctx.Done()&#125;func (gcc *PodGCController) gc(ctx context.Context) &#123; // 返回indexer中的所有pod pods, err := gcc.podLister.List(labels.Everything()) if err != nil &#123; klog.Errorf("Error while listing all pods: %v", err) return &#125; nodes, err := gcc.nodeLister.List(labels.Everything()) if err != nil &#123; klog.Errorf("Error while listing all nodes: %v", err) return &#125; // terminatedPodThreshold 系统中可以存在Terminated状态pod的数量 if gcc.terminatedPodThreshold &gt; 0 &#123; // 删除部分Terminated状态的pod gcc.gcTerminated(ctx, pods) &#125; // 删除不再服务的node上的pod if utilfeature.DefaultFeatureGate.Enabled(features.NodeOutOfServiceVolumeDetach) &#123; gcc.gcTerminating(ctx, pods) &#125; // 删除孤儿pod(一个pod绑定到了一个不存在的Node) gcc.gcOrphaned(ctx, pods, nodes) // 删除Terminating状态且未被调度到某一Node上的pod // 什么情况下会产生这种pod?? 将创建没来得及被调度就被删除了 gcc.gcUnscheduledTerminating(ctx, pods)&#125;func isPodTerminated(pod *v1.Pod) bool &#123; if phase := pod.Status.Phase; phase != v1.PodPending &amp;&amp; phase != v1.PodRunning &amp;&amp; phase != v1.PodUnknown &#123; return true &#125; return false&#125;// isPodTerminating returns true if the pod is terminating.func isPodTerminating(pod *v1.Pod) bool &#123; return pod.ObjectMeta.DeletionTimestamp != nil&#125;func (gcc *PodGCController) gcTerminating(ctx context.Context, pods []*v1.Pod) &#123; klog.V(4).Info("GC'ing terminating pods that are on out-of-service nodes") terminatingPods := []*v1.Pod&#123;&#125; for _, pod := range pods &#123; if isPodTerminating(pod) &#123; // 获取pod所在的node node, err := gcc.nodeLister.Get(pod.Spec.NodeName) if err != nil &#123; klog.Errorf("failed to get node %s : %s", pod.Spec.NodeName, err) continue &#125; // Add this pod to terminatingPods list only if the following conditions are met: // 1. Node is not ready. // 2. Node has `node.kubernetes.io/out-of-service` taint. // 判断Node是否是Ready,Node是否包含node.kubernetes.io/out-of-service // 同时满足这两个条件，则加入terminatingPods列表 if !nodeutil.IsNodeReady(node) &amp;&amp; taints.TaintKeyExists(node.Spec.Taints, v1.TaintNodeOutOfService) &#123; klog.V(4).Infof("garbage collecting pod %s that is terminating. Phase [%v]", pod.Name, pod.Status.Phase) terminatingPods = append(terminatingPods, pod) &#125; &#125; &#125; deleteCount := len(terminatingPods) if deleteCount == 0 &#123; return &#125; klog.V(4).Infof("Garbage collecting %v pods that are terminating on node tainted with node.kubernetes.io/out-of-service", deleteCount) // sort only when necessary sort.Sort(byEvictionAndCreationTimestamp(terminatingPods)) var wait sync.WaitGroup // 删除terminatingPods for i := 0; i &lt; deleteCount; i++ &#123; wait.Add(1) go func(pod *v1.Pod) &#123; defer wait.Done() deletingPodsTotal.WithLabelValues().Inc() if err := gcc.markFailedAndDeletePod(ctx, pod); err != nil &#123; // ignore not founds utilruntime.HandleError(err) deletingPodsErrorTotal.WithLabelValues().Inc() &#125; &#125;(terminatingPods[i]) &#125; wait.Wait()&#125;func (gcc *PodGCController) gcTerminated(ctx context.Context, pods []*v1.Pod) &#123; terminatedPods := []*v1.Pod&#123;&#125; for _, pod := range pods &#123; if isPodTerminated(pod) &#123; terminatedPods = append(terminatedPods, pod) &#125; &#125; terminatedPodCount := len(terminatedPods) deleteCount := terminatedPodCount - gcc.terminatedPodThreshold if deleteCount &lt;= 0 &#123; return &#125; klog.InfoS("Garbage collecting pods", "numPods", deleteCount) // sort only when necessary sort.Sort(byEvictionAndCreationTimestamp(terminatedPods)) var wait sync.WaitGroup for i := 0; i &lt; deleteCount; i++ &#123; wait.Add(1) go func(pod *v1.Pod) &#123; defer wait.Done() if err := gcc.markFailedAndDeletePod(ctx, pod); err != nil &#123; // ignore not founds defer utilruntime.HandleError(err) &#125; &#125;(terminatedPods[i]) &#125; wait.Wait()&#125;// gcOrphaned deletes pods that are bound to nodes that don't exist.func (gcc *PodGCController) gcOrphaned(ctx context.Context, pods []*v1.Pod, nodes []*v1.Node) &#123; klog.V(4).Infof("GC'ing orphaned") existingNodeNames := sets.NewString() // 获取存在的NodeName for _, node := range nodes &#123; existingNodeNames.Insert(node.Name) &#125; // Add newly found unknown nodes to quarantine for _, pod := range pods &#123; // pod的nodeName不为空且nodeName不存在于existingNodeNames // 将NodeName入队，延迟时间为quarantineTime if pod.Spec.NodeName != "" &amp;&amp; !existingNodeNames.Has(pod.Spec.NodeName) &#123; gcc.nodeQueue.AddAfter(pod.Spec.NodeName, gcc.quarantineTime) &#125; &#125; // Check if nodes are still missing after quarantine period // 在隔离期时间过后检查node是否存在 deletedNodesNames, quit := gcc.discoverDeletedNodes(ctx, existingNodeNames) if quit &#123; return &#125; // 删除孤儿Pod for _, pod := range pods &#123; // pod.Spec.NodeName 不在已删除的node集合，说明不是孤儿Pod if !deletedNodesNames.Has(pod.Spec.NodeName) &#123; continue &#125; klog.V(2).InfoS("Found orphaned Pod assigned to the Node, deleting.", "pod", klog.KObj(pod), "node", pod.Spec.NodeName) condition := corev1apply.PodCondition(). WithType(v1.DisruptionTarget). WithStatus(v1.ConditionTrue). WithReason("DeletionByPodGC"). WithMessage("PodGC: node no longer exists"). WithLastTransitionTime(metav1.Now()) if err := gcc.markFailedAndDeletePodWithCondition(ctx, pod, condition); err != nil &#123; utilruntime.HandleError(err) &#125; else &#123; klog.InfoS("Forced deletion of orphaned Pod succeeded", "pod", klog.KObj(pod)) &#125; &#125;&#125;func (gcc *PodGCController) discoverDeletedNodes(ctx context.Context, existingNodeNames sets.String) (sets.String, bool) &#123; deletedNodesNames := sets.NewString() for gcc.nodeQueue.Len() &gt; 0 &#123; item, quit := gcc.nodeQueue.Get() if quit &#123; return nil, true &#125; nodeName := item.(string) if !existingNodeNames.Has(nodeName) &#123; exists, err := gcc.checkIfNodeExists(ctx, nodeName) switch &#123; case err != nil: klog.ErrorS(err, "Error while getting node", "node", klog.KRef("", nodeName)) // Node will be added back to the queue in the subsequent loop if still needed case !exists: deletedNodesNames.Insert(nodeName) &#125; &#125; gcc.nodeQueue.Done(item) &#125; return deletedNodesNames, false&#125;func (gcc *PodGCController) checkIfNodeExists(ctx context.Context, name string) (bool, error) &#123; _, fetchErr := gcc.kubeClient.CoreV1().Nodes().Get(ctx, name, metav1.GetOptions&#123;&#125;) if errors.IsNotFound(fetchErr) &#123; return false, nil &#125; return fetchErr == nil, fetchErr&#125;// gcUnscheduledTerminating deletes pods that are terminating and haven't been scheduled to a particular node.func (gcc *PodGCController) gcUnscheduledTerminating(ctx context.Context, pods []*v1.Pod) &#123; klog.V(4).Infof("GC'ing unscheduled pods which are terminating.") for _, pod := range pods &#123; // 未被删除或已经调度到node if pod.DeletionTimestamp == nil || len(pod.Spec.NodeName) &gt; 0 &#123; continue &#125; klog.V(2).InfoS("Found unscheduled terminating Pod not assigned to any Node, deleting.", "pod", klog.KObj(pod)) if err := gcc.markFailedAndDeletePod(ctx, pod); err != nil &#123; utilruntime.HandleError(err) &#125; else &#123; klog.InfoS("Forced deletion of unscheduled terminating Pod succeeded", "pod", klog.KObj(pod)) &#125; &#125;&#125;// byEvictionAndCreationTimestamp sorts a list by Evicted status and then creation timestamp,// using their names as a tie breaker.// Evicted pods will be deleted first to avoid impact on terminated pods created by controllers.type byEvictionAndCreationTimestamp []*v1.Podfunc (o byEvictionAndCreationTimestamp) Len() int &#123; return len(o) &#125;func (o byEvictionAndCreationTimestamp) Swap(i, j int) &#123; o[i], o[j] = o[j], o[i] &#125;func (o byEvictionAndCreationTimestamp) Less(i, j int) bool &#123; iEvicted, jEvicted := eviction.PodIsEvicted(o[i].Status), eviction.PodIsEvicted(o[j].Status) // Evicted pod is smaller if iEvicted != jEvicted &#123; return iEvicted &#125; if o[i].CreationTimestamp.Equal(&amp;o[j].CreationTimestamp) &#123; return o[i].Name &lt; o[j].Name &#125; return o[i].CreationTimestamp.Before(&amp;o[j].CreationTimestamp)&#125;func (gcc *PodGCController) markFailedAndDeletePod(ctx context.Context, pod *v1.Pod) error &#123; return gcc.markFailedAndDeletePodWithCondition(ctx, pod, nil)&#125;func (gcc *PodGCController) markFailedAndDeletePodWithCondition(ctx context.Context, pod *v1.Pod, condition *corev1apply.PodConditionApplyConfiguration) error &#123; klog.InfoS("PodGC is force deleting Pod", "pod", klog.KRef(pod.Namespace, pod.Name)) if utilfeature.DefaultFeatureGate.Enabled(features.PodDisruptionConditions) &#123; // Mark the pod as failed - this is especially important in case the pod // is orphaned, in which case the pod would remain in the Running phase // forever as there is no kubelet running to change the phase. if pod.Status.Phase != v1.PodSucceeded &amp;&amp; pod.Status.Phase != v1.PodFailed &#123; podApply := corev1apply.Pod(pod.Name, pod.Namespace).WithStatus(corev1apply.PodStatus()) // we don't need to extract the pod apply configuration and can send // only phase and the DisruptionTarget condition as PodGC would not // own other fields. If the DisruptionTarget condition is owned by // PodGC it means that it is in the Failed phase, so sending the // condition will not be re-attempted. podApply.Status.WithPhase(v1.PodFailed) if condition != nil &#123; podApply.Status.WithConditions(condition) &#125; if _, err := gcc.kubeClient.CoreV1().Pods(pod.Namespace).ApplyStatus(ctx, podApply, metav1.ApplyOptions&#123;FieldManager: fieldManager, Force: true&#125;); err != nil &#123; return err &#125; &#125; &#125; return gcc.kubeClient.CoreV1().Pods(pod.Namespace).Delete(ctx, pod.Name, *metav1.NewDeleteOptions(0))&#125; REF:1.cmd/kube-controller-manager/app/core.go2.pkg/controller/podgc/gc_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之namespace]]></title>
    <url>%2F2023%2F05%2F11%2Fkube-controller%E4%B9%8Bnamespace%2F</url>
    <content type="text"><![CDATA[Kubernetes Namespace Controller是Kubernetes中的一个控制器，负责管理Kubernetes中的命名空间。命名空间是Kubernetes资源的一个逻辑分组，用于将资源隔离在不同的命名空间中，从而实现更好的资源管理和隔离。 Namespace Controller主要负责以下任务：在删除命名空间时，确保所有在该命名空间中创建的资源都被正确地清理和回收。 1234567891011121314151617181920212223242526272829303132333435// 启动函数// cmd/kube-controller-manager/app/core.gofunc startNamespaceController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) &#123; // the namespace cleanup controller is very chatty. It makes lots of discovery calls and then it makes lots of delete calls // the ratelimiter negatively affects its speed. Deleting 100 total items in a namespace (that's only a few of each resource // including events), takes ~10 seconds by default. nsKubeconfig := controllerContext.ClientBuilder.ConfigOrDie("namespace-controller") nsKubeconfig.QPS *= 20 nsKubeconfig.Burst *= 100 namespaceKubeClient := clientset.NewForConfigOrDie(nsKubeconfig) return startModifiedNamespaceController(ctx, controllerContext, namespaceKubeClient, nsKubeconfig)&#125;func startModifiedNamespaceController(ctx context.Context, controllerContext ControllerContext, namespaceKubeClient clientset.Interface, nsKubeconfig *restclient.Config) (controller.Interface, bool, error) &#123; metadataClient, err := metadata.NewForConfig(nsKubeconfig) if err != nil &#123; return nil, true, err &#125; discoverResourcesFn := namespaceKubeClient.Discovery().ServerPreferredNamespacedResources namespaceController := namespacecontroller.NewNamespaceController( ctx, namespaceKubeClient, metadataClient, discoverResourcesFn, controllerContext.InformerFactory.Core().V1().Namespaces(), controllerContext.ComponentConfig.NamespaceController.NamespaceSyncPeriod.Duration, v1.FinalizerKubernetes, ) go namespaceController.Run(ctx, int(controllerContext.ComponentConfig.NamespaceController.ConcurrentNamespaceSyncs)) return nil, true, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157// pkg/controller/namespace/namespace_controller.goconst ( // namespaceDeletionGracePeriod is the time period to wait before processing a received namespace event. // This allows time for the following to occur: // * lifecycle admission plugins on HA apiservers to also observe a namespace // deletion and prevent new objects from being created in the terminating namespace // * non-leader etcd servers to observe last-minute object creations in a namespace // so this controller's cleanup can actually clean up all objects // 延迟入队时间 namespaceDeletionGracePeriod = 5 * time.Second)// NamespaceController is responsible for performing actions dependent upon a namespace phasetype NamespaceController struct &#123; // lister that can list namespaces from a shared cache lister corelisters.NamespaceLister // returns true when the namespace cache is ready listerSynced cache.InformerSynced // namespaces that have been queued up for processing by workers queue workqueue.RateLimitingInterface // helper to delete all resources in the namespace when the namespace is deleted. namespacedResourcesDeleter deletion.NamespacedResourcesDeleterInterface&#125;// NewNamespaceController creates a new NamespaceControllerfunc NewNamespaceController( ctx context.Context, kubeClient clientset.Interface, metadataClient metadata.Interface, discoverResourcesFn func() ([]*metav1.APIResourceList, error), namespaceInformer coreinformers.NamespaceInformer, resyncPeriod time.Duration, finalizerToken v1.FinalizerName) *NamespaceController &#123; // create the controller so we can inject the enqueue function namespaceController := &amp;NamespaceController&#123; queue: workqueue.NewNamedRateLimitingQueue(nsControllerRateLimiter(), "namespace"), namespacedResourcesDeleter: deletion.NewNamespacedResourcesDeleter(ctx, kubeClient.CoreV1().Namespaces(), metadataClient, kubeClient.CoreV1(), discoverResourcesFn, finalizerToken), &#125; // configure the namespace informer event handlers namespaceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; namespace := obj.(*v1.Namespace) namespaceController.enqueueNamespace(namespace) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; namespace := newObj.(*v1.Namespace) namespaceController.enqueueNamespace(namespace) &#125;, &#125;, resyncPeriod, ) namespaceController.lister = namespaceInformer.Lister() namespaceController.listerSynced = namespaceInformer.Informer().HasSynced return namespaceController&#125;func (nm *NamespaceController) enqueueNamespace(obj interface&#123;&#125;) &#123; key, err := controller.KeyFunc(obj) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("Couldn't get key for object %+v: %v", obj, err)) return &#125; namespace := obj.(*v1.Namespace) // don't queue if we aren't deleted // !! 只有删除操作才会入队，所以说NamespaceController只处理删除操作 if namespace.DeletionTimestamp == nil || namespace.DeletionTimestamp.IsZero() &#123; return &#125; // delay processing namespace events to allow HA api servers to observe namespace deletion, // and HA etcd servers to observe last minute object creations inside the namespace nm.queue.AddAfter(key, namespaceDeletionGracePeriod)&#125;func (nm *NamespaceController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() defer nm.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting namespace controller") defer logger.Info("Shutting down namespace controller") if !cache.WaitForNamedCacheSync("namespace", ctx.Done(), nm.listerSynced) &#123; return &#125; logger.V(5).Info("Starting workers of namespace controller") for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, nm.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (nm *NamespaceController) worker(ctx context.Context) &#123; // workFunc 返回值是一个bool, 表明有队列是否被关闭 workFunc := func(ctx context.Context) bool &#123; // 从队列中获取一个key, 如果队列已经关闭，直接退出 key, quit := nm.queue.Get() if quit &#123; return true &#125; defer nm.queue.Done(key) err := nm.syncNamespaceFromKey(ctx, key.(string)) if err == nil &#123; // no error, forget this entry and return nm.queue.Forget(key) return false &#125; // ok=true,表明资源未完全清除完 // 这里会有一个延迟入队,等待资源被清除 if estimate, ok := err.(*deletion.ResourcesRemainingError); ok &#123; t := estimate.Estimate/2 + 1 klog.FromContext(ctx).V(4).Info("Content remaining in namespace", "namespace", key, "waitSeconds", t) nm.queue.AddAfter(key, time.Duration(t)*time.Second) &#125; else &#123; // rather than wait for a full resync, re-add the namespace to the queue to be processed // 其它错误，直接重新入队 nm.queue.AddRateLimited(key) utilruntime.HandleError(fmt.Errorf("deletion of namespace %v failed: %v", key, err)) &#125; return false &#125; for &#123; quit := workFunc(ctx) // 队列已关闭 if quit &#123; return &#125; &#125;&#125;// syncNamespaceFromKey looks for a namespace with the specified key in its store and synchronizes itfunc (nm *NamespaceController) syncNamespaceFromKey(ctx context.Context, key string) (err error) &#123; startTime := time.Now() logger := klog.FromContext(ctx) defer func() &#123; logger.V(4).Info("Finished syncing namespace", "namespace", key, "duration", time.Since(startTime)) &#125;() namespace, err := nm.lister.Get(key) if errors.IsNotFound(err) &#123; logger.Info("Namespace has been deleted", "namespace", key) return nil &#125; if err != nil &#123; utilruntime.HandleError(fmt.Errorf("Unable to retrieve namespace %v from store: %v", key, err)) return err &#125; return nm.namespacedResourcesDeleter.Delete(ctx, namespace.Name)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145// pkg/controller/namespace/deletion/namespaced_resources_deleter.go// Delete deletes all resources in the given namespace.// Before deleting resources:// - It ensures that deletion timestamp is set on the// namespace (does nothing if deletion timestamp is missing).// - Verifies that the namespace is in the "terminating" phase// (updates the namespace phase if it is not yet marked terminating)//// After deleting the resources:// * It removes finalizer token from the given namespace.//// Returns an error if any of those steps fail.// Returns ResourcesRemainingError if it deleted some resources but needs// to wait for them to go away.// Caller is expected to keep calling this until it succeeds.func (d *namespacedResourcesDeleter) Delete(ctx context.Context, nsName string) error &#123; // Multiple controllers may edit a namespace during termination // first get the latest state of the namespace before proceeding // if the namespace was deleted already, don't do anything namespace, err := d.nsClient.Get(context.TODO(), nsName, metav1.GetOptions&#123;&#125;) if err != nil &#123; if errors.IsNotFound(err) &#123; return nil &#125; return err &#125; if namespace.DeletionTimestamp == nil &#123; return nil &#125; klog.FromContext(ctx).V(5).Info("Namespace controller - syncNamespace", "namespace", namespace.Name, "finalizerToken", d.finalizerToken) // ensure that the status is up to date on the namespace // if we get a not found error, we assume the namespace is truly gone namespace, err = d.retryOnConflictError(namespace, d.updateNamespaceStatusFunc) if err != nil &#123; if errors.IsNotFound(err) &#123; return nil &#125; return err &#125; // the latest view of the namespace asserts that namespace is no longer deleting.. if namespace.DeletionTimestamp.IsZero() &#123; return nil &#125; // return if it is already finalized. // 如果len(Spec.Finalizers)==0, return if finalized(namespace) &#123; return nil &#125; // there may still be content for us to remove estimate, err := d.deleteAllContent(ctx, namespace) if err != nil &#123; return err &#125; // 还有资源未删除 if estimate &gt; 0 &#123; return &amp;ResourcesRemainingError&#123;estimate&#125; &#125; // we have removed content, so mark it finalized by us _, err = d.retryOnConflictError(namespace, d.finalizeNamespace) if err != nil &#123; // in normal practice, this should not be possible, but if a deployment is running // two controllers to do namespace deletion that share a common finalizer token it's // possible that a not found could occur since the other controller would have finished the delete. if errors.IsNotFound(err) &#123; return nil &#125; return err &#125; return nil&#125;// 使用dynamic client删除命名空间下的所有资源// 返回一个时间(估计资源删除需要的时间)func (d *namespacedResourcesDeleter) deleteAllContent(ctx context.Context, ns *v1.Namespace) (int64, error) &#123; namespace := ns.Name namespaceDeletedAt := *ns.DeletionTimestamp var errs []error conditionUpdater := namespaceConditionUpdater&#123;&#125; estimate := int64(0) logger := klog.FromContext(ctx) logger.V(4).Info("namespace controller - deleteAllContent", "namespace", namespace) resources, err := d.discoverResourcesFn() if err != nil &#123; // discovery errors are not fatal. We often have some set of resources we can operate against even if we don't have a complete list errs = append(errs, err) conditionUpdater.ProcessDiscoverResourcesErr(err) &#125; // TODO(sttts): get rid of opCache and pass the verbs (especially "deletecollection") down into the deleter deletableResources := discovery.FilteredBy(discovery.SupportsAllVerbs&#123;Verbs: []string&#123;"delete"&#125;&#125;, resources) groupVersionResources, err := discovery.GroupVersionResources(deletableResources) if err != nil &#123; // discovery errors are not fatal. We often have some set of resources we can operate against even if we don't have a complete list errs = append(errs, err) conditionUpdater.ProcessGroupVersionErr(err) &#125; numRemainingTotals := allGVRDeletionMetadata&#123; gvrToNumRemaining: map[schema.GroupVersionResource]int&#123;&#125;, finalizersToNumRemaining: map[string]int&#123;&#125;, &#125; for gvr := range groupVersionResources &#123; gvrDeletionMetadata, err := d.deleteAllContentForGroupVersionResource(ctx, gvr, namespace, namespaceDeletedAt) if err != nil &#123; // If there is an error, hold on to it but proceed with all the remaining // groupVersionResources. errs = append(errs, err) conditionUpdater.ProcessDeleteContentErr(err) &#125; if gvrDeletionMetadata.finalizerEstimateSeconds &gt; estimate &#123; estimate = gvrDeletionMetadata.finalizerEstimateSeconds &#125; if gvrDeletionMetadata.numRemaining &gt; 0 &#123; numRemainingTotals.gvrToNumRemaining[gvr] = gvrDeletionMetadata.numRemaining for finalizer, numRemaining := range gvrDeletionMetadata.finalizersToNumRemaining &#123; if numRemaining == 0 &#123; continue &#125; numRemainingTotals.finalizersToNumRemaining[finalizer] = numRemainingTotals.finalizersToNumRemaining[finalizer] + numRemaining &#125; &#125; &#125; conditionUpdater.ProcessContentTotals(numRemainingTotals) // we always want to update the conditions because if we have set a condition to "it worked" after it was previously, "it didn't work", // we need to reflect that information. Recall that additional finalizers can be set on namespaces, so this finalizer may clear itself and // NOT remove the resource instance. if hasChanged := conditionUpdater.Update(ns); hasChanged &#123; if _, err = d.nsClient.UpdateStatus(context.TODO(), ns, metav1.UpdateOptions&#123;&#125;); err != nil &#123; utilruntime.HandleError(fmt.Errorf("couldn't update status condition for namespace %q: %v", namespace, err)) &#125; &#125; // if len(errs)==0, NewAggregate returns nil. err = utilerrors.NewAggregate(errs) logger.V(4).Info("namespace controller - deleteAllContent", "namespace", namespace, "estimate", estimate, "err", err) return estimate, err&#125; 总结：NamespaceController相对比较简单，只有一个删除的调谐操作。 REF:1.cmd/kube-controller-manager/app/core.go2.pkg/controller/namespace/namespace_controller.go3.pkg/controller/namespace/deletion/namespaced_resources_deleter.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之replicaset]]></title>
    <url>%2F2023%2F05%2F10%2Fkube-controller%E4%B9%8Breplicaset%2F</url>
    <content type="text"><![CDATA[ReplicaSet维护一组Pod并使Pod数量和状态达到预期的状态。Deployment就是基于ReplicaSet的一层封装，官方并不建议直接使用ReplicaSet,而是使用Deployment。 使用下面的yaml创建对应的RS.123456789101112131415161718192021222324apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontend labels: app: guestbook tier: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413type ReplicaSetController struct &#123; // GroupVersionKind indicates the controller type. // Different instances of this struct may handle different GVKs. // For example, this struct can be used (with adapters) to handle ReplicationController. schema.GroupVersionKind kubeClient clientset.Interface podControl controller.PodControlInterface eventBroadcaster record.EventBroadcaster // A ReplicaSet is temporarily suspended after creating/deleting these many replicas. // It resumes normal action after observing the watch events for them. burstReplicas int // To allow injection of syncReplicaSet for testing. syncHandler func(ctx context.Context, rsKey string) error // A TTLCache of pod creates/deletes each rc expects to see. expectations *controller.UIDTrackingControllerExpectations // A store of ReplicaSets, populated by the shared informer passed to NewReplicaSetController rsLister appslisters.ReplicaSetLister // rsListerSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. rsListerSynced cache.InformerSynced rsIndexer cache.Indexer // A store of pods, populated by the shared informer passed to NewReplicaSetController podLister corelisters.PodLister // podListerSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. podListerSynced cache.InformerSynced // Controllers that need to be synced queue workqueue.RateLimitingInterface&#125;func NewReplicaSetController(logger klog.Logger, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int) *ReplicaSetController &#123; eventBroadcaster := record.NewBroadcaster() if err := metrics.Register(legacyregistry.Register); err != nil &#123; logger.Error(err, "unable to register metrics") &#125; return NewBaseController(rsInformer, podInformer, kubeClient, burstReplicas, apps.SchemeGroupVersion.WithKind("ReplicaSet"), "replicaset_controller", "replicaset", controller.RealPodControl&#123; KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "replicaset-controller"&#125;), &#125;, eventBroadcaster, )&#125;// 创建一个ReplicaSetController// 监听ReplicaSet和Pod变化func NewBaseController(rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, kubeClient clientset.Interface, burstReplicas int, gvk schema.GroupVersionKind, metricOwnerName, queueName string, podControl controller.PodControlInterface, eventBroadcaster record.EventBroadcaster) *ReplicaSetController &#123; rsc := &amp;ReplicaSetController&#123; GroupVersionKind: gvk, kubeClient: kubeClient, podControl: podControl, eventBroadcaster: eventBroadcaster, burstReplicas: burstReplicas, expectations: controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), queueName), &#125; rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: rsc.addRS, UpdateFunc: rsc.updateRS, DeleteFunc: rsc.deleteRS, &#125;) rsInformer.Informer().AddIndexers(cache.Indexers&#123; controllerUIDIndex: func(obj interface&#123;&#125;) ([]string, error) &#123; rs, ok := obj.(*apps.ReplicaSet) if !ok &#123; return []string&#123;&#125;, nil &#125; controllerRef := metav1.GetControllerOf(rs) if controllerRef == nil &#123; return []string&#123;&#125;, nil &#125; return []string&#123;string(controllerRef.UID)&#125;, nil &#125;, &#125;) rsc.rsIndexer = rsInformer.Informer().GetIndexer() rsc.rsLister = rsInformer.Lister() rsc.rsListerSynced = rsInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: rsc.addPod, // This invokes the ReplicaSet for every pod change, eg: host assignment. Though this might seem like // overkill the most frequent pod update is status, and the associated ReplicaSet will only list from // local storage, so it should be ok. UpdateFunc: rsc.updatePod, DeleteFunc: rsc.deletePod, &#125;) rsc.podLister = podInformer.Lister() rsc.podListerSynced = podInformer.Informer().HasSynced rsc.syncHandler = rsc.syncReplicaSet return rsc&#125;// Run begins watching and syncing.func (rsc *ReplicaSetController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() // Start events processing pipeline. rsc.eventBroadcaster.StartStructuredLogging(0) rsc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: rsc.kubeClient.CoreV1().Events("")&#125;) defer rsc.eventBroadcaster.Shutdown() defer rsc.queue.ShutDown() controllerName := strings.ToLower(rsc.Kind) klog.FromContext(ctx).Info("Starting controller", "name", controllerName) defer klog.FromContext(ctx).Info("Shutting down controller", "name", controllerName) if !cache.WaitForNamedCacheSync(rsc.Kind, ctx.Done(), rsc.podListerSynced, rsc.rsListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, rsc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (rsc *ReplicaSetController) worker(ctx context.Context) &#123; for rsc.processNextWorkItem(ctx) &#123; &#125;&#125;func (rsc *ReplicaSetController) processNextWorkItem(ctx context.Context) bool &#123; key, quit := rsc.queue.Get() if quit &#123; return false &#125; defer rsc.queue.Done(key) err := rsc.syncHandler(ctx, key.(string)) if err == nil &#123; // 如果未发生错误,结束重试 rsc.queue.Forget(key) return true &#125; utilruntime.HandleError(fmt.Errorf("sync %q failed with %v", key, err)) // 发生错误则重新入队 rsc.queue.AddRateLimited(key) return true&#125;// syncHandler函数func (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error &#123; startTime := time.Now() // 记录此次syncReplicaSet耗时 defer func() &#123; klog.FromContext(ctx).V(4).Info("Finished syncing", "kind", rsc.Kind, "key", key, "duration", time.Since(startTime)) &#125;() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 获取对应的RS rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if apierrors.IsNotFound(err) &#123; klog.FromContext(ctx).V(4).Info("deleted", "kind", rsc.Kind, "key", key) rsc.expectations.DeleteExpectations(key) return nil &#125; if err != nil &#123; return err &#125; // 判断是否需要Sync rsNeedsSync := rsc.expectations.SatisfiedExpectations(key) selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("error converting pod selector to selector for rs %v/%v: %v", namespace, name, err)) return nil &#125; // 这里会获取RS对应命名空间下的所有pod // 这是一个优化项,将下面的FilterActivePods合并为一步 allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) if err != nil &#123; return err &#125; // 返回所有Active Pod filteredPods := controller.FilterActivePods(allPods) // NOTE: filteredPods are pointing to objects from cache - if you need to // modify them, you need to copy it first. // 新建的RS第一次调谐，filterdPod为nil filteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods) if err != nil &#123; return err &#125; var manageReplicasErr error if rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil &#123; // 操作pod manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs) &#125; rs = rs.DeepCopy() newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) // Always updates status as pods come up or die. updatedRS, err := updateReplicaSetStatus(klog.FromContext(ctx), rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) if err != nil &#123; // Multiple things could lead to this update failing. Requeuing the replica set ensures // Returning an error causes a requeue without forcing a hotloop return err &#125; // Resync the ReplicaSet after MinReadySeconds as a last line of defense to guard against clock-skew. if manageReplicasErr == nil &amp;&amp; updatedRS.Spec.MinReadySeconds &gt; 0 &amp;&amp; updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) &amp;&amp; updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) &#123; rsc.queue.AddAfter(key, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second) &#125; return manageReplicasErr&#125;func (rsc *ReplicaSetController) claimPods(ctx context.Context, rs *apps.ReplicaSet, selector labels.Selector, filteredPods []*v1.Pod) ([]*v1.Pod, error) &#123; // If any adoptions are attempted, we should first recheck for deletion with // an uncached quorum read sometime after listing Pods (see #42639). canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) &#123; // 获取最新的RS // 如果fresh.UID和rs.UID不相等. 说明rs不是最新的，直接返回error fresh, err := rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace).Get(ctx, rs.Name, metav1.GetOptions&#123;&#125;) if err != nil &#123; return nil, err &#125; if fresh.UID != rs.UID &#123; return nil, fmt.Errorf("original %v %v/%v is gone: got uid %v, wanted %v", rsc.Kind, rs.Namespace, rs.Name, fresh.UID, rs.UID) &#125; return fresh, nil &#125;) // 返回PodControllerRefManager // PodControllerRefManager实现了对Pod管理的一些方法 cm := controller.NewPodControllerRefManager(rsc.podControl, rs, selector, rsc.GroupVersionKind, canAdoptFunc) // 认领RS拥有的Pod return cm.ClaimPods(ctx, filteredPods)&#125;// 认领RS所拥有的Pod// 如果selector匹配,收养orphans pod// 如果selector不再匹配,释放对应的pod// 如果error返回为nil,则表示本次调谐成功或者不需要调谐func (m *PodControllerRefManager) ClaimPods(ctx context.Context, pods []*v1.Pod, filters ...func(*v1.Pod) bool) ([]*v1.Pod, error) &#123; var claimed []*v1.Pod var errlist []error // 判断pod.Labels和rs.Selector是否匹配 match := func(obj metav1.Object) bool &#123; pod := obj.(*v1.Pod) // Check selector first so filters only run on potentially matching Pods. if !m.Selector.Matches(labels.Set(pod.Labels)) &#123; return false &#125; for _, filter := range filters &#123; if !filter(pod) &#123; return false &#125; &#125; return true &#125; // 收养一个pod // 如果某个 Pod 没有 OwnerReference 或者其 OwnerReference 不是一个控制器， // 且其匹配到某 ReplicaSet 的选择算符，则该 Pod 立即被此 ReplicaSet 获得。 adopt := func(ctx context.Context, obj metav1.Object) error &#123; return m.AdoptPod(ctx, obj.(*v1.Pod)) &#125; // 如果不再匹配则释放对应的pod release := func(ctx context.Context, obj metav1.Object) error &#123; return m.ReleasePod(ctx, obj.(*v1.Pod)) &#125; for _, pod := range pods &#123; ok, err := m.ClaimObject(ctx, pod, match, adopt, release) if err != nil &#123; errlist = append(errlist, err) continue &#125; if ok &#123; claimed = append(claimed, pod) &#125; &#125; // 如果是新建的RS第一次调谐,claimed为nil return claimed, utilerrors.NewAggregate(errlist)&#125;// 对replicas进行操作// 不能直接对&lt;filteredPods&gt;进行修改// 因为这个过程可能失败，失败后需要重新入队func (rsc *ReplicaSetController) manageReplicas(ctx context.Context, filteredPods []*v1.Pod, rs *apps.ReplicaSet) error &#123; // 新建的RS,len(filteredPods)=0 // 如果replicas=3,则diff=-3 diff := len(filteredPods) - int(*(rs.Spec.Replicas)) // rsKey = default/frontend rsKey, err := controller.KeyFunc(rs) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("couldn't get key for %v %#v: %v", rsc.Kind, rs, err)) return nil &#125; // diff&lt;0,说明需要增加pod if diff &lt; 0 &#123; diff *= -1 if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; // TODO: Track UIDs of creates just like deletes. The problem currently // is we'd need to wait on the result of a create to record the pod's // UID, which would require locking *across* the create, which will turn // into a performance bottleneck. We should generate a UID for the pod // beforehand and store it via ExpectCreations. // 向Controller注册最的Expection rsc.expectations.ExpectCreations(rsKey, diff) klog.FromContext(ctx).V(2).Info("Too few replicas", "replicaSet", klog.KObj(rs), "need", *(rs.Spec.Replicas), "creating", diff) // Batch the pod creates. Batch sizes start at SlowStartInitialBatchSize // and double with each successful iteration in a kind of "slow start". // This handles attempts to start large numbers of pods that would // likely all fail with the same error. For example a project with a // low quota that attempts to create a large number of pods will be // prevented from spamming the API service with the pod create requests // after one of its pods fails. Conveniently, this also prevents the // event spam that those failures would generate. successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error &#123; // 调用接口创建pod err := rsc.podControl.CreatePods(ctx, rs.Namespace, &amp;rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind)) if err != nil &#123; if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) &#123; // if the namespace is being terminated, we don't have to do // anything because any creation will fail return nil &#125; &#125; return err &#125;) // Any skipped pods that we never attempted to start shouldn't be expected. // The skipped pods will be retried later. The next controller resync will // retry the slow start process. if skippedPods := diff - successfulCreations; skippedPods &gt; 0 &#123; klog.FromContext(ctx).V(2).Info("Slow-start failure. Skipping creation of pods, decrementing expectations", "podsSkipped", skippedPods, "kind", rsc.Kind, "replicaSet", klog.KObj(rs)) for i := 0; i &lt; skippedPods; i++ &#123; // Decrement the expected number of creates because the informer won't observe this pod rsc.expectations.CreationObserved(rsKey) &#125; &#125; return err &#125; else if diff &gt; 0 &#123; if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; klog.FromContext(ctx).V(2).Info("Too many replicas", "replicaSet", klog.KObj(rs), "need", *(rs.Spec.Replicas), "deleting", diff) relatedPods, err := rsc.getIndirectlyRelatedPods(klog.FromContext(ctx), rs) utilruntime.HandleError(err) // Choose which Pods to delete, preferring those in earlier phases of startup. podsToDelete := getPodsToDelete(filteredPods, relatedPods, diff) // Snapshot the UIDs (ns/name) of the pods we're expecting to see // deleted, so we know to record their expectations exactly once either // when we see it as an update of the deletion timestamp, or as a delete. // Note that if the labels on a pod/rs change in a way that the pod gets // orphaned, the rs will only wake up after the expectations have // expired even if other pods are deleted. rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete)) errCh := make(chan error, diff) var wg sync.WaitGroup wg.Add(diff) for _, pod := range podsToDelete &#123; go func(targetPod *v1.Pod) &#123; defer wg.Done() // 删除多余的pod if err := rsc.podControl.DeletePod(ctx, rs.Namespace, targetPod.Name, rs); err != nil &#123; // Decrement the expected number of deletes because the informer won't observe this deletion podKey := controller.PodKey(targetPod) rsc.expectations.DeletionObserved(rsKey, podKey) if !apierrors.IsNotFound(err) &#123; klog.FromContext(ctx).V(2).Info("Failed to delete pod, decremented expectations", "pod", podKey, "kind", rsc.Kind, "replicaSet", klog.KObj(rs)) errCh &lt;- err &#125; &#125; &#125;(pod) &#125; wg.Wait() select &#123; case err := &lt;-errCh: // all errors have been reported before and they're likely to be the same, so we'll only return the first one we hit. if err != nil &#123; return err &#125; default: &#125; &#125; return nil&#125; REF:1.pkg/controller/replicaset/replica_set.go2.pkg/controller/controller_ref_manager.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之deployment]]></title>
    <url>%2F2023%2F05%2F08%2Fkube-controller%E4%B9%8Bdeployment%2F</url>
    <content type="text"><![CDATA[Kubernetes中的Deployment Controller是一个负责管理Pod副本集的控制器。它在Kubernetes集群中负责确保Pod的副本数始终符合用户指定的期望值。Deployment Controller能够对Pod进行滚动更新(默认的更新策略)，通过逐步替换Pod的方式更新应用程序，以确保服务的高可用性和零停机时间。Deployment Controller使用控制循环对目标状态和当前状态进行比较，并对任何不同之处进行调整，以确保集群中的Pod数始终符合用户指定的期望值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// DeploymentController启动入口// cmd/kube-controller-manager/app/apps.gofunc startDeploymentController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) &#123; dc, err := deployment.NewDeploymentController( ctx, controllerContext.InformerFactory.Apps().V1().Deployments(), controllerContext.InformerFactory.Apps().V1().ReplicaSets(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.ClientBuilder.ClientOrDie("deployment-controller"), ) if err != nil &#123; return nil, true, fmt.Errorf("error creating Deployment controller: %v", err) &#125; go dc.Run(ctx, int(controllerContext.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs)) return nil, true, nil&#125;// 完成Controller注册// cmd/kube-controller-manager/app/controllermanager.gofunc NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123; controllers := map[string]InitFunc&#123;&#125; // All of the controllers must have unique names, or else we will explode. register := func(name string, fn InitFunc) &#123; if _, found := controllers[name]; found &#123; panic(fmt.Sprintf("controller name %q was registered twice", name)) &#125; controllers[name] = fn &#125; ... register("deployment", startDeploymentController) ...&#125;// 最后会在Run方法中调用func Run(ctx context.Context, c *config.CompletedConfig) error &#123; run := func(ctx context.Context, startSATokenController InitFunc, initializersFunc ControllerInitializersFunc) &#123; controllerContext, err := CreateControllerContext(logger, c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil &#123; logger.Error(err, "Error building controller context") klog.FlushAndExit(klog.ExitFlushTimeout, 1) &#125; controllerInitializers := initializersFunc(controllerContext.LoopMode) if err := StartControllers(ctx, controllerContext, startSATokenController, controllerInitializers, unsecuredMux, healthzHandler); err != nil &#123; logger.Error(err, "Error starting controllers") klog.FlushAndExit(klog.ExitFlushTimeout, 1) &#125; controllerContext.InformerFactory.Start(stopCh) controllerContext.ObjectOrMetadataInformerFactory.Start(stopCh) close(controllerContext.InformersStarted) &lt;-ctx.Done() &#125; // No leader election, run directly if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; // 启动NewControllerInitializers注册的controller run(ctx, saTokenControllerInitFunc, NewControllerInitializers) return nil &#125; ...&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242// pkg/controller/deployment/deployment_controller.go// 最大重试次数// 采用公式(5ms*2^(retryNum-1))对消息进行重新入队// 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82sconst maxRetries = 15type DeploymentController struct &#123; // rsControl用于添加/删除ReplicaSets. rsControl controller.RSControlInterface client clientset.Interface eventBroadcaster record.EventBroadcaster eventRecorder record.EventRecorder // To allow injection of syncDeployment for testing. // syncHandler默认为syncDeployment,测试时可进行替换 syncHandler func(ctx context.Context, dKey string) error // used for unit testing enqueueDeployment func(deployment *apps.Deployment) // dLister can list/get deployments from the shared informer's store dLister appslisters.DeploymentLister // rsLister can list/get replica sets from the shared informer's store rsLister appslisters.ReplicaSetLister // podLister can list/get pods from the shared informer's store podLister corelisters.PodLister // dListerSynced returns true if the Deployment store has been synced at least once. // Added as a member to the struct to allow injection for testing. dListerSynced cache.InformerSynced // rsListerSynced returns true if the ReplicaSet store has been synced at least once. // Added as a member to the struct to allow injection for testing. rsListerSynced cache.InformerSynced // podListerSynced returns true if the pod store has been synced at least once. // Added as a member to the struct to allow injection for testing. podListerSynced cache.InformerSynced // Deployments that need to be synced queue workqueue.RateLimitingInterface&#125;// 创建DeploymentController// 可以看到DeploymentController需要监听三种资源变化: Deployment, ReplicaSet, Podfunc NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) &#123; eventBroadcaster := record.NewBroadcaster() logger := klog.FromContext(ctx) dc := &amp;DeploymentController&#123; client: client, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "deployment-controller"&#125;), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "deployment"), &#125; dc.rsControl = controller.RealRSControl&#123; KubeClient: client, Recorder: dc.eventRecorder, &#125; dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; // 如果新建一个Deployment, 将key入队 dc.addDeployment(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dc.updateDeployment(logger, oldObj, newObj) &#125;, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface&#123;&#125;) &#123; dc.deleteDeployment(logger, obj) &#125;, &#125;) rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; dc.addReplicaSet(logger, obj) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; dc.updateReplicaSet(logger, oldObj, newObj) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; dc.deleteReplicaSet(logger, obj) &#125;, &#125;) podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; DeleteFunc: func(obj interface&#123;&#125;) &#123; dc.deletePod(logger, obj) &#125;, &#125;) // 设置syncHandler为syncDeployment. dc.syncHandler = dc.syncDeployment dc.enqueueDeployment = dc.enqueue dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil&#125;// func (dc *DeploymentController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() // Start events processing pipeline. dc.eventBroadcaster.StartStructuredLogging(0) dc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: dc.client.CoreV1().Events("")&#125;) defer dc.eventBroadcaster.Shutdown() defer dc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting controller", "controller", "deployment") defer logger.Info("Shutting down controller", "controller", "deployment") if !cache.WaitForNamedCacheSync("deployment", ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, dc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (dc *DeploymentController) worker(ctx context.Context) &#123; for dc.processNextWorkItem(ctx) &#123; &#125;&#125;func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool &#123; // 取出key,然后执行syncHandler,也就是syncDeployment key, quit := dc.queue.Get() if quit &#123; return false &#125; defer dc.queue.Done(key) // 执行调谐逻辑 err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true&#125;// DeploymentController真正的处理逻辑func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error &#123; logger := klog.FromContext(ctx) // 获取到资源的命名空间和名称 // 假如是在命名空间default创建Deployment(ep),则key为default/ep // namespace=default,name=ep namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; klog.ErrorS(err, "Failed to split meta namespace cache key", "cacheKey", key) return err &#125; startTime := time.Now() logger.V(4).Info("Started syncing deployment", "deployment", klog.KRef(namespace, name), "startTime", startTime) defer func() &#123; logger.V(4).Info("Finished syncing deployment", "deployment", klog.KRef(namespace, name), "duration", time.Since(startTime)) &#125;() // 从cache是获取deployment,如果不存在则直接退出 deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) &#123; logger.V(2).Info("Deployment has been deleted", "deployment", klog.KRef(namespace, name)) return nil &#125; if err != nil &#123; return err &#125; // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector&#123;&#125; // 如果Deployment的Selector为空，生成警告事件 // Selector不能为空(为空表示选择所有的pod) if reflect.DeepEqual(d.Spec.Selector, &amp;everything) &#123; dc.eventRecorder.Eventf(d, v1.EventTypeWarning, "SelectingAll", "This deployment is selecting all pods. A non-empty selector is required.") if d.Status.ObservedGeneration &lt; d.Generation &#123; d.Status.ObservedGeneration = d.Generation dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions&#123;&#125;) &#125; return nil &#125; // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. // 获取属于Deployment的ReplicaSet rsList, err := dc.getReplicaSetsForDeployment(ctx, d) if err != nil &#123; return err &#125; // 返回属于Deployment的pod,类型为map //map[ReplicaSet.UID][]*v1.Pod podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil &#123; return err &#125; // 如果DeletionTimestamp!=nil,表明Deployment已经被Delete if d.DeletionTimestamp != nil &#123; return dc.syncStatusOnly(ctx, d, rsList) &#125; // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(ctx, d); err != nil &#123; return err &#125; if d.Spec.Paused &#123; return dc.sync(ctx, d, rsList) &#125; // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won't proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if getRollbackTo(d) != nil &#123; return dc.rollback(ctx, d, rsList) &#125; scalingEvent, err := dc.isScalingEvent(ctx, d, rsList) if err != nil &#123; return err &#125; if scalingEvent &#123; return dc.sync(ctx, d, rsList) &#125; // Deployment的默认升级策略是RollingUpdate switch d.Spec.Strategy.Type &#123; case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(ctx, d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(ctx, d, rsList) &#125; return fmt.Errorf("unexpected deployment strategy type: %s", d.Spec.Strategy.Type)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129// pkg/controller/deployment/rolling.gofunc (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; // 获取最新的RS,和旧的RS newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil &#123; return err &#125; allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d) if err != nil &#123; return err &#125; if scaledUp &#123; // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) &#125; // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(ctx, allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil &#123; return err &#125; if scaledDown &#123; // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) &#125; if deploymentutil.DeploymentComplete(d, &amp;d.Status) &#123; if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil &#123; return err &#125; &#125; // Sync deployment status return dc.syncRolloutStatus(ctx, allRSs, newRS, d)&#125;func (dc *DeploymentController) reconcileNewReplicaSet(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) &#123; // 如果最新的RS副本数=deployment的副本数，则不进行更新 if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) &#123; // Scaling not required. return false, nil &#125; // 减少pod数量 if *(newRS.Spec.Replicas) &gt; *(deployment.Spec.Replicas) &#123; // Scale down. scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment) return scaled, err &#125; // 计算NewRS应该拥有的Replicas数目 newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil &#123; return false, err &#125; scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, newReplicasCount, deployment) return scaled, err&#125;func (dc *DeploymentController) reconcileOldReplicaSets(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) &#123; logger := klog.FromContext(ctx) oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 &#123; // Can't scale down further return false, nil &#125; allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) logger.V(4).Info("New replica set", "replicaSet", klog.KObj(newRS), "availableReplicas", newRS.Status.AvailableReplicas) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) // Check if we can scale down. We can scale down in the following 2 cases: // * Some old replica sets have unhealthy replicas, we could safely scale down those unhealthy replicas since that won't further // increase unavailability. // * New replica set has scaled up and it's replicas becomes ready, then we can scale down old replica sets in a further step. // // maxScaledDown := allPodsCount - minAvailable - newReplicaSetPodsUnavailable // take into account not only maxUnavailable and any surge pods that have been created, but also unavailable pods from // the newRS, so that the unavailable pods from the newRS would not make us scale down old replica sets in a further // step(that will increase unavailability). // // Concrete example: // // * 10 replicas // * 2 maxUnavailable (absolute number, not percent) // * 3 maxSurge (absolute number, not percent) // // case 1: // * Deployment is updated, newRS is created with 3 replicas, oldRS is scaled down to 8, and newRS is scaled up to 5. // * The new replica set pods crashloop and never become available. // * allPodsCount is 13. minAvailable is 8. newRSPodsUnavailable is 5. // * A node fails and causes one of the oldRS pods to become unavailable. However, 13 - 8 - 5 = 0, so the oldRS won't be scaled down. // * The user notices the crashloop and does kubectl rollout undo to rollback. // * newRSPodsUnavailable is 1, since we rolled back to the good replica set, so maxScaledDown = 13 - 8 - 1 = 4. 4 of the crashlooping pods will be scaled down. // * The total number of pods will then be 9 and the newRS can be scaled up to 10. // // case 2: // Same example, but pushing a new pod template instead of rolling back (aka "roll over"): // * The new replica set created must start with 0 replicas because allPodsCount is already at 13. // * However, newRSPodsUnavailable would also be 0, so the 2 old replica sets could be scaled down by 5 (13 - 8 - 0), which would then // allow the new replica set to be scaled up by 5. minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown &lt;= 0 &#123; return false, nil &#125; // Clean up unhealthy replicas first, otherwise unhealthy replicas will block deployment // and cause timeout. See https://github.com/kubernetes/kubernetes/issues/16737 oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(ctx, oldRSs, deployment, maxScaledDown) if err != nil &#123; return false, nil &#125; logger.V(4).Info("Cleaned up unhealthy replicas from old RSes", "count", cleanupCount) // Scale down old replica sets, need check maxUnavailable to ensure we can scale down allRSs = append(oldRSs, newRS) scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(ctx, allRSs, oldRSs, deployment) if err != nil &#123; return false, nil &#125; logger.V(4).Info("Scaled down old RSes", "deployment", klog.KObj(deployment), "count", scaledDownCount) totalScaledDown := cleanupCount + scaledDownCount return totalScaledDown &gt; 0, nil&#125; REF:1.cmd/kube-controller-manager/app/apps.go2.pkg/controller/deployment/deployment_controller.go3.pkg/controller/deployment/rolling.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之endpointslice]]></title>
    <url>%2F2023%2F05%2F07%2Fkube-controller%E4%B9%8Bendpointslice%2F</url>
    <content type="text"><![CDATA[Kubernetes EndpointSlice是在Kubernetes 1.16版本中引入的一种资源，它提供了一种更有效的方式来表示Kubernetes服务的网络端点。 EndpointSlice为现有的Endpoints资源提供了一种替代方案，对于具有大量网络端点的非常大型服务，Endpoints资源可能会成为性能瓶颈。 EndpointSlice将服务的网络端点分成更小、更易管理的块，每个块由单独的EndpointSlice资源表示。 每个EndpointSlice包含有关服务的一部分端点的信息，包括它们的IP地址、端口和附加的元数据，例如拓扑信息。这使得Kubernetes更容易有效地管理和更新服务的网络端点。 EndpointSlice还支持其他功能，例如支持多协议服务和支持加权负载平衡，这些功能在原始的Endpoints资源中不可用。 与 Endpoints 的比较原来的 Endpoints API 提供了在 Kubernetes 中跟踪网络端点的一种简单而直接的方法。随着 Kubernetes 集群和服务逐渐开始为更多的后端 Pod 处理和发送请求， 原来的 API 的局限性变得越来越明显。最明显的是那些因为要处理大量网络端点而带来的挑战。 由于任一 Service 的所有网络端点都保存在同一个 Endpoints 对象中，这些 Endpoints 对象可能变得非常巨大。对于保持稳定的服务（长时间使用同一组端点），影响不太明显； 即便如此，Kubernetes 的一些使用场景也没有得到很好的服务。 当某 Service存在很多后端端点并且该工作负载频繁扩缩或上线新更改时，对该 Service 的单个 Endpoints 对象的每次更新都意味着（在控制平面内以及在节点和 API 服务器之间）Kubernetes 集群组件之间会出现大量流量。 这种额外的流量在 CPU 使用方面也有开销。 使用 EndpointSlices 时，添加或移除单个 Pod 对于正监视变更的客户端会触发相同数量的更新， 但这些更新消息的大小在大规模场景下要小得多。 EndpointSlices 还支持围绕双栈网络和拓扑感知路由等新功能的创新。 可以使用下面的yaml文件创建Service观察对应的EndpointSlice. 123456789101112131415161718192021222324252627282930313233343536cat &gt; ep.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: epspec: replicas: 1 selector: matchLabels: app: ep template: metadata: labels: app: ep spec: containers: - name: ep image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: epspec: selector: app: ep ports: - name: http port: 8080 targetPort: 8080 type: NodePortEOF 123456➜ kubernetes git:(master) k get svc --show-labelsNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE LABELSep NodePort 10.96.112.72 &lt;none&gt; 8080:30143/TCP 23h &lt;none&gt;➜ kubernetes git:(master) k get endpointslice --show-labelsNAME ADDRESSTYPE PORTS ENDPOINTS AGE LABELSep-rsnnk IPv4 8080 10.244.0.183 23h endpointslice.kubernetes.io/managed-by=endpointslice-controller.k8s.io,kubernetes.io/service-name=ep 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132// staging/src/k8s.io/api/discovery/v1/types.go// EndpointSlice represents a subset of the endpoints that implement a service.// For a given service there may be multiple EndpointSlice objects, selected by// labels, which must be joined to produce the full set of endpoints.// EndpointSlice表示实现服务的一部分端点。对于给定的服务，可能会有多个EndpointSlice对象，// 通过标签进行选择，必须将它们合并以产生完整的端点集。type EndpointSlice struct &#123; metav1.TypeMeta `json:",inline"` // Standard object's metadata. // +optional metav1.ObjectMeta `json:"metadata,omitempty" protobuf:"bytes,1,opt,name=metadata"` // addressType specifies the type of address carried by this EndpointSlice. // All addresses in this slice must be the same type. This field is // immutable after creation. The following address types are currently // supported: // * IPv4: Represents an IPv4 Address. // * IPv6: Represents an IPv6 Address. // * FQDN: Represents a Fully Qualified Domain Name. AddressType AddressType `json:"addressType" protobuf:"bytes,4,rep,name=addressType"` // endpoints is a list of unique endpoints in this slice. Each slice may // include a maximum of 1000 endpoints. // +listType=atomic Endpoints []Endpoint `json:"endpoints" protobuf:"bytes,2,rep,name=endpoints"` // ports specifies the list of network ports exposed by each endpoint in // this slice. Each port must have a unique name. When ports is empty, it // indicates that there are no defined ports. When a port is defined with a // nil port value, it indicates "all ports". Each slice may include a // maximum of 100 ports. // +optional // +listType=atomic Ports []EndpointPort `json:"ports" protobuf:"bytes,3,rep,name=ports"`&#125;// pkg/controller/endpointslice/endpointslice_controller.go// 与endpoint_controller有一点区别，endpointslice监听了nodefunc NewController(podInformer coreinformers.PodInformer, serviceInformer coreinformers.ServiceInformer, nodeInformer coreinformers.NodeInformer, endpointSliceInformer discoveryinformers.EndpointSliceInformer, maxEndpointsPerSlice int32, client clientset.Interface, endpointUpdatesBatchPeriod time.Duration,) *Controller &#123; broadcaster := record.NewBroadcaster() recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "endpoint-slice-controller"&#125;) endpointslicemetrics.RegisterMetrics() c := &amp;Controller&#123; client: client, // This is similar to the DefaultControllerRateLimiter, just with a // significantly higher default backoff (1s vs 5ms). This controller // processes events that can require significant EndpointSlice changes, // such as an update to a Service or Deployment. A more significant // rate limit back off here helps ensure that the Controller does not // overwhelm the API Server. queue: workqueue.NewNamedRateLimitingQueue(workqueue.NewMaxOfRateLimiter( workqueue.NewItemExponentialFailureRateLimiter(defaultSyncBackOff, maxSyncBackOff), // 10 qps, 100 bucket size. This is only for retry speed and its // only the overall factor (not per item). &amp;workqueue.BucketRateLimiter&#123;Limiter: rate.NewLimiter(rate.Limit(10), 100)&#125;, ), "endpoint_slice"), workerLoopPeriod: time.Second, &#125; serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: c.onServiceUpdate, UpdateFunc: func(old, cur interface&#123;&#125;) &#123; c.onServiceUpdate(cur) &#125;, DeleteFunc: c.onServiceDelete, &#125;) c.serviceLister = serviceInformer.Lister() c.servicesSynced = serviceInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: c.addPod, UpdateFunc: c.updatePod, DeleteFunc: c.deletePod, &#125;) c.podLister = podInformer.Lister() c.podsSynced = podInformer.Informer().HasSynced c.nodeLister = nodeInformer.Lister() c.nodesSynced = nodeInformer.Informer().HasSynced endpointSliceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: c.onEndpointSliceAdd, UpdateFunc: c.onEndpointSliceUpdate, DeleteFunc: c.onEndpointSliceDelete, &#125;) c.endpointSliceLister = endpointSliceInformer.Lister() c.endpointSlicesSynced = endpointSliceInformer.Informer().HasSynced c.endpointSliceTracker = endpointsliceutil.NewEndpointSliceTracker() c.maxEndpointsPerSlice = maxEndpointsPerSlice c.triggerTimeTracker = endpointutil.NewTriggerTimeTracker() c.eventBroadcaster = broadcaster c.eventRecorder = recorder c.endpointUpdatesBatchPeriod = endpointUpdatesBatchPeriod if utilfeature.DefaultFeatureGate.Enabled(features.TopologyAwareHints) &#123; nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: c.addNode, UpdateFunc: c.updateNode, DeleteFunc: c.deleteNode, &#125;) c.topologyCache = topologycache.NewTopologyCache() &#125; c.reconciler = &amp;reconciler&#123; client: c.client, nodeLister: c.nodeLister, maxEndpointsPerSlice: c.maxEndpointsPerSlice, endpointSliceTracker: c.endpointSliceTracker, metricsCache: endpointslicemetrics.NewCache(maxEndpointsPerSlice), topologyCache: c.topologyCache, eventRecorder: c.eventRecorder, &#125; return c&#125; syncService是循环处理函数，从workqueue中取key然后进行处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321// pkg/controller/endpointslice/endpointslice_controller.gofunc (c *Controller) syncService(key string) error &#123; startTime := time.Now() defer func() &#123; klog.V(4).Infof("Finished syncing service %q endpoint slices. (%v)", key, time.Since(startTime)) &#125;() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; service, err := c.serviceLister.Services(namespace).Get(name) if err != nil &#123; if !apierrors.IsNotFound(err) &#123; return err &#125; c.triggerTimeTracker.DeleteService(namespace, name) c.reconciler.deleteService(namespace, name) c.endpointSliceTracker.DeleteService(namespace, name) // The service has been deleted, return nil so that it won't be retried. return nil &#125; if service.Spec.Type == v1.ServiceTypeExternalName &#123; // services with Type ExternalName receive no endpoints from this controller; // Ref: https://issues.k8s.io/105986 return nil &#125; if service.Spec.Selector == nil &#123; // services without a selector receive no endpoint slices from this controller; // these services will receive endpoint slices that are created out-of-band via the REST API. return nil &#125; klog.V(5).Infof("About to update endpoint slices for service %q", key) podLabelSelector := labels.Set(service.Spec.Selector).AsSelectorPreValidated() pods, err := c.podLister.Pods(service.Namespace).List(podLabelSelector) if err != nil &#123; // Since we're getting stuff from a local cache, it is basically // impossible to get this error. c.eventRecorder.Eventf(service, v1.EventTypeWarning, "FailedToListPods", "Error listing Pods for Service %s/%s: %v", service.Namespace, service.Name, err) return err &#125; esLabelSelector := labels.Set(map[string]string&#123; discovery.LabelServiceName: service.Name, discovery.LabelManagedBy: controllerName, &#125;).AsSelectorPreValidated() endpointSlices, err := c.endpointSliceLister.EndpointSlices(service.Namespace).List(esLabelSelector) if err != nil &#123; // Since we're getting stuff from a local cache, it is basically // impossible to get this error. c.eventRecorder.Eventf(service, v1.EventTypeWarning, "FailedToListEndpointSlices", "Error listing Endpoint Slices for Service %s/%s: %v", service.Namespace, service.Name, err) return err &#125; // Drop EndpointSlices that have been marked for deletion to prevent the controller from getting stuck. endpointSlices = dropEndpointSlicesPendingDeletion(endpointSlices) if c.endpointSliceTracker.StaleSlices(service, endpointSlices) &#123; return endpointsliceutil.NewStaleInformerCache("EndpointSlice informer cache is out of date") &#125; // We call ComputeEndpointLastChangeTriggerTime here to make sure that the // state of the trigger time tracker gets updated even if the sync turns out // to be no-op and we don't update the EndpointSlice objects. lastChangeTriggerTime := c.triggerTimeTracker. ComputeEndpointLastChangeTriggerTime(namespace, service, pods) err = c.reconciler.reconcile(service, pods, endpointSlices, lastChangeTriggerTime) if err != nil &#123; c.eventRecorder.Eventf(service, v1.EventTypeWarning, "FailedToUpdateEndpointSlices", "Error updating Endpoint Slices for Service %s/%s: %v", service.Namespace, service.Name, err) return err &#125; return nil&#125;// reconcile takes a set of pods currently matching a service selector and// compares them with the endpoints already present in any existing endpoint// slices for the given service. It creates, updates, or deletes endpoint slices// to ensure the desired set of pods are represented by endpoint slices.// pkg/controller/endpointslice/reconciler.go// 参数为service及匹配service selector的pods,已经存在的endpointslice// 作用:创建，更新或删除endpointslice,确保endpointslice能匹配所有预期的podfunc (r *reconciler) reconcile(service *corev1.Service, pods []*corev1.Pod, existingSlices []*discovery.EndpointSlice, triggerTime time.Time) error &#123; slicesToDelete := []*discovery.EndpointSlice&#123;&#125; // slices that are no longer matching any address the service has errs := []error&#123;&#125; // all errors generated in the process of reconciling slicesByAddressType := make(map[discovery.AddressType][]*discovery.EndpointSlice) // slices by address type // addresses that this service supports [o(1) find] // 获得server支持的AddressesTypes,serviceSupportedAddressesTypes是一个map实现的set // * IPv4: Represents an IPv4 Address. // * IPv6: Represents an IPv6 Address. // * FQDN: Represents a Fully Qualified Domain Name. serviceSupportedAddressesTypes := getAddressTypesForService(service) // loop through slices identifying their address type. // slices that no longer match address type supported by services // go to delete, other slices goes to the reconciler machinery // for further adjustment for _, existingSlice := range existingSlices &#123; // service no longer supports that address type, add it to deleted slices if _, ok := serviceSupportedAddressesTypes[existingSlice.AddressType]; !ok &#123; if r.topologyCache != nil &#123; svcKey, err := serviceControllerKey(existingSlice) if err != nil &#123; klog.Warningf("Couldn't get key to remove EndpointSlice from topology cache %+v: %v", existingSlice, err) &#125; else &#123; r.topologyCache.RemoveHints(svcKey, existingSlice.AddressType) &#125; &#125; // 如果是这个Service不支持的AddressType则加入删除列表 slicesToDelete = append(slicesToDelete, existingSlice) continue &#125; // add list if it is not on our map if _, ok := slicesByAddressType[existingSlice.AddressType]; !ok &#123; slicesByAddressType[existingSlice.AddressType] = make([]*discovery.EndpointSlice, 0, 1) &#125; slicesByAddressType[existingSlice.AddressType] = append(slicesByAddressType[existingSlice.AddressType], existingSlice) &#125; // reconcile for existing. for addressType := range serviceSupportedAddressesTypes &#123; existingSlices := slicesByAddressType[addressType] // 对不同的AddressType进行调谐 err := r.reconcileByAddressType(service, pods, existingSlices, triggerTime, addressType) if err != nil &#123; errs = append(errs, err) &#125; &#125; // delete those which are of addressType that is no longer supported // by the service for _, sliceToDelete := range slicesToDelete &#123; err := r.client.DiscoveryV1().EndpointSlices(service.Namespace).Delete(context.TODO(), sliceToDelete.Name, metav1.DeleteOptions&#123;&#125;) if err != nil &#123; errs = append(errs, fmt.Errorf("error deleting %s EndpointSlice for Service %s/%s: %w", sliceToDelete.Name, service.Namespace, service.Name, err)) &#125; else &#123; r.endpointSliceTracker.ExpectDeletion(sliceToDelete) metrics.EndpointSliceChanges.WithLabelValues("delete").Inc() &#125; &#125; return utilerrors.NewAggregate(errs)&#125;// reconcileByAddressType takes a set of pods currently matching a service selector and// compares them with the endpoints already present in any existing endpoint// slices (by address type) for the given service. It creates, updates, or deletes endpoint slices// to ensure the desired set of pods are represented by endpoint slices.func (r *reconciler) reconcileByAddressType(service *corev1.Service, pods []*corev1.Pod, existingSlices []*discovery.EndpointSlice, triggerTime time.Time, addressType discovery.AddressType) error &#123; errs := []error&#123;&#125; slicesToCreate := []*discovery.EndpointSlice&#123;&#125; slicesToUpdate := []*discovery.EndpointSlice&#123;&#125; slicesToDelete := []*discovery.EndpointSlice&#123;&#125; events := []*topologycache.EventBuilder&#123;&#125; // Build data structures for existing state. existingSlicesByPortMap := map[endpointutil.PortMapKey][]*discovery.EndpointSlice&#123;&#125; for _, existingSlice := range existingSlices &#123; if ownedBy(existingSlice, service) &#123; epHash := endpointutil.NewPortMapKey(existingSlice.Ports) existingSlicesByPortMap[epHash] = append(existingSlicesByPortMap[epHash], existingSlice) &#125; else &#123; slicesToDelete = append(slicesToDelete, existingSlice) &#125; &#125; // Build data structures for desired state. desiredMetaByPortMap := map[endpointutil.PortMapKey]*endpointMeta&#123;&#125; desiredEndpointsByPortMap := map[endpointutil.PortMapKey]endpointsliceutil.EndpointSet&#123;&#125; for _, pod := range pods &#123; if !endpointutil.ShouldPodBeInEndpoints(pod, true) &#123; continue &#125; endpointPorts := getEndpointPorts(service, pod) epHash := endpointutil.NewPortMapKey(endpointPorts) if _, ok := desiredEndpointsByPortMap[epHash]; !ok &#123; desiredEndpointsByPortMap[epHash] = endpointsliceutil.EndpointSet&#123;&#125; &#125; if _, ok := desiredMetaByPortMap[epHash]; !ok &#123; desiredMetaByPortMap[epHash] = &amp;endpointMeta&#123; addressType: addressType, ports: endpointPorts, &#125; &#125; node, err := r.nodeLister.Get(pod.Spec.NodeName) if err != nil &#123; // we are getting the information from the local informer, // an error different than IsNotFound should not happen if !errors.IsNotFound(err) &#123; return err &#125; // If the Node specified by the Pod doesn't exist we want to requeue the Service so we // retry later, but also update the EndpointSlice without the problematic Pod. // Theoretically, the pod Garbage Collector will remove the Pod, but we want to avoid // situations where a reference from a Pod to a missing node can leave the EndpointSlice // stuck forever. // On the other side, if the service.Spec.PublishNotReadyAddresses is set we just add the // Pod, since the user is explicitly indicating that the Pod address should be published. if !service.Spec.PublishNotReadyAddresses &#123; klog.Warningf("skipping Pod %s for Service %s/%s: Node %s Not Found", pod.Name, service.Namespace, service.Name, pod.Spec.NodeName) errs = append(errs, fmt.Errorf("skipping Pod %s for Service %s/%s: Node %s Not Found", pod.Name, service.Namespace, service.Name, pod.Spec.NodeName)) continue &#125; &#125; endpoint := podToEndpoint(pod, node, service, addressType) if len(endpoint.Addresses) &gt; 0 &#123; desiredEndpointsByPortMap[epHash].Insert(&amp;endpoint) &#125; &#125; spMetrics := metrics.NewServicePortCache() totalAdded := 0 totalRemoved := 0 // Determine changes necessary for each group of slices by port map. for portMap, desiredEndpoints := range desiredEndpointsByPortMap &#123; numEndpoints := len(desiredEndpoints) pmSlicesToCreate, pmSlicesToUpdate, pmSlicesToDelete, added, removed := r.reconcileByPortMapping( service, existingSlicesByPortMap[portMap], desiredEndpoints, desiredMetaByPortMap[portMap]) totalAdded += added totalRemoved += removed spMetrics.Set(portMap, metrics.EfficiencyInfo&#123; Endpoints: numEndpoints, Slices: len(existingSlicesByPortMap[portMap]) + len(pmSlicesToCreate) - len(pmSlicesToDelete), &#125;) slicesToCreate = append(slicesToCreate, pmSlicesToCreate...) slicesToUpdate = append(slicesToUpdate, pmSlicesToUpdate...) slicesToDelete = append(slicesToDelete, pmSlicesToDelete...) &#125; // If there are unique sets of ports that are no longer desired, mark // the corresponding endpoint slices for deletion. for portMap, existingSlices := range existingSlicesByPortMap &#123; if _, ok := desiredEndpointsByPortMap[portMap]; !ok &#123; slicesToDelete = append(slicesToDelete, existingSlices...) &#125; &#125; // When no endpoint slices would usually exist, we need to add a placeholder. if len(existingSlices) == len(slicesToDelete) &amp;&amp; len(slicesToCreate) &lt; 1 &#123; // Check for existing placeholder slice outside of the core control flow placeholderSlice := newEndpointSlice(service, &amp;endpointMeta&#123;ports: []discovery.EndpointPort&#123;&#125;, addressType: addressType&#125;) if len(slicesToDelete) == 1 &amp;&amp; placeholderSliceCompare.DeepEqual(slicesToDelete[0], placeholderSlice) &#123; // We are about to unnecessarily delete/recreate the placeholder, remove it now. slicesToDelete = slicesToDelete[:0] &#125; else &#123; slicesToCreate = append(slicesToCreate, placeholderSlice) &#125; spMetrics.Set(endpointutil.NewPortMapKey(placeholderSlice.Ports), metrics.EfficiencyInfo&#123; Endpoints: 0, Slices: 1, &#125;) &#125; metrics.EndpointsAddedPerSync.WithLabelValues().Observe(float64(totalAdded)) metrics.EndpointsRemovedPerSync.WithLabelValues().Observe(float64(totalRemoved)) serviceNN := types.NamespacedName&#123;Name: service.Name, Namespace: service.Namespace&#125; r.metricsCache.UpdateServicePortCache(serviceNN, spMetrics) // Topology hints are assigned per address type. This means it is // theoretically possible for endpoints of one address type to be assigned // hints while another endpoints of another address type are not. si := &amp;topologycache.SliceInfo&#123; ServiceKey: fmt.Sprintf("%s/%s", service.Namespace, service.Name), AddressType: addressType, ToCreate: slicesToCreate, ToUpdate: slicesToUpdate, Unchanged: unchangedSlices(existingSlices, slicesToUpdate, slicesToDelete), &#125; if r.topologyCache != nil &amp;&amp; hintsEnabled(service.Annotations) &#123; slicesToCreate, slicesToUpdate, events = r.topologyCache.AddHints(si) &#125; else &#123; if r.topologyCache != nil &#123; if r.topologyCache.HasPopulatedHints(si.ServiceKey) &#123; klog.InfoS("TopologyAwareHints annotation has changed, removing hints", "serviceKey", si.ServiceKey, "addressType", si.AddressType) events = append(events, &amp;topologycache.EventBuilder&#123; EventType: corev1.EventTypeWarning, Reason: "TopologyAwareHintsDisabled", Message: topologycache.FormatWithAddressType(topologycache.TopologyAwareHintsDisabled, si.AddressType), &#125;) &#125; r.topologyCache.RemoveHints(si.ServiceKey, addressType) &#125; slicesToCreate, slicesToUpdate = topologycache.RemoveHintsFromSlices(si) &#125; err := r.finalize(service, slicesToCreate, slicesToUpdate, slicesToDelete, triggerTime) if err != nil &#123; errs = append(errs, err) &#125; for _, event := range events &#123; r.eventRecorder.Event(service, event.EventType, event.Reason, event.Message) &#125; return utilerrors.NewAggregate(errs)&#125; 为了管理服务的EndpointSlices分发，Kubernetes维护了一个名为TopologyCache的EndpointSlices缓存。TopologyCache跟踪服务所有EndpointSlices的状态，并允许Kubernetes快速、高效地将流量路由到适当的端点。 TopologyCache是由服务控制器提供的端点拓扑信息构建的。控制器根据标签和选择器确定服务端点的拓扑，并生成一组基于拓扑信息的EndpointSlices，并使用新的EndpointSlices更新TopologyCache。 REF:1.https://kubernetes.io/zh-cn/docs/concepts/services-networking/endpoint-slices/]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller之endpoint]]></title>
    <url>%2F2023%2F05%2F03%2Fkube-controller%E4%B9%8Bendpoint%2F</url>
    <content type="text"><![CDATA[在 Kubernetes 中，一个 Endpoint 代表一个网络地址（IP地址和端口,是实现实际服务的端点的集合,通常用于将服务的客户端请求路由到后端 Pod 的 IP 地址和端口上。 Endpoint 可以手动创建，也可以由 Kubernetes 的 service 控制器自动创建和更新。当创建一个 Service 对象时，Kubernetes 会创建一个关联的 Endpoint 对象，自动添加 Pod 的 IP 地址和端口到 Endpoint 中。 当客户端通过 Service 访问后端 Pod 时，Kubernetes 会自动将客户端请求路由到后端 Pod 的 IP 地址和端口上。这种路由方式是通过 iptables 实现的，对于一个 Service 的每个端口，Kubernetes 会自动创建一条 iptables 规则将该端口上的请求转发到对应的 Endpoint 地址和端口上。 官方目前推荐使用endpointslice, 创建一个service会自动创建一个endpoint和endpointSlice.发现如果删除掉endpoint服务也能正常访问。 可以使用下面的yaml文件创建Service观察对应的endpoint. 123456789101112131415161718192021222324252627282930313233343536cat &gt; ep.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: name: epspec: replicas: 1 selector: matchLabels: app: ep template: metadata: labels: app: ep spec: containers: - name: ep image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: epspec: selector: app: ep ports: - name: http port: 8080 targetPort: 8080 type: NodePortEOF 12345I0503 15:23:44.009583 131717 round_trippers.go:463] POST apis/apps/v1/namespaces/default/deployments?fieldManager=kubectl-create&amp;fieldValidation=Strictdeployment.apps/ep createdI0503 15:23:44.016610 131717 round_trippers.go:463] POST api/v1/namespaces/default/services?fieldManager=kubectl-create&amp;fieldValidation=Strictservice/ep created 调用接口分别创建了deployment和service,假如关掉kube-controller-manager的话会发现kubectl get ep并不能看到对应的Endpoint,Endpoint是由kube-controller-manager创建的。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// pkg/apis/discovery/types.go// Endpoint represents a single logical "backend" implementing a service.type Endpoint struct &#123; // addresses of this endpoint. The contents of this field are interpreted // according to the corresponding EndpointSlice addressType field. Consumers // must handle different types of addresses in the context of their own // capabilities. This must contain at least one address but no more than // 100. // +listType=set Addresses []string // conditions contains information about the current status of the endpoint. Conditions EndpointConditions // hostname of this endpoint. This field may be used by consumers of // endpoints to distinguish endpoints from each other (e.g. in DNS names). // Multiple endpoints which use the same hostname should be considered // fungible (e.g. multiple A values in DNS). Must pass DNS Label (RFC 1123) // validation. // +optional Hostname *string // targetRef is a reference to a Kubernetes object that represents this // endpoint. // +optional TargetRef *api.ObjectReference // deprecatedTopology is deprecated and only retained for round-trip // compatibility with v1beta1 Topology field. When v1beta1 is removed, this // should be removed, too. // +optional DeprecatedTopology map[string]string // nodeName represents the name of the Node hosting this endpoint. This can // be used to determine endpoints local to a Node. // +optional NodeName *string // zone is the name of the Zone this endpoint exists in. // +optional Zone *string // hints contains information associated with how an endpoint should be // consumed. // +featureGate=TopologyAwareHints // +optional Hints *EndpointHints&#125;// 从代码中可以看该Controller通过Informer机制监听了三种资源// Service// Pod// endpoint// pkg/controller/endpoint/endpoints_controller.go// NewEndpointController returns a new *Controller.func NewEndpointController(podInformer coreinformers.PodInformer, serviceInformer coreinformers.ServiceInformer, endpointsInformer coreinformers.EndpointsInformer, client clientset.Interface, endpointUpdatesBatchPeriod time.Duration) *Controller &#123; broadcaster := record.NewBroadcaster() recorder := broadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: "endpoint-controller"&#125;) e := &amp;Controller&#123; client: client, queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "endpoint"), workerLoopPeriod: time.Second, &#125; // onServiceUpdate,onServiceDelete都是执行入队操作 serviceInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: e.onServiceUpdate, UpdateFunc: func(old, cur interface&#123;&#125;) &#123; e.onServiceUpdate(cur) &#125;, DeleteFunc: e.onServiceDelete, &#125;) e.serviceLister = serviceInformer.Lister() e.servicesSynced = serviceInformer.Informer().HasSynced podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: e.addPod, UpdateFunc: e.updatePod, DeleteFunc: e.deletePod, &#125;) e.podLister = podInformer.Lister() e.podsSynced = podInformer.Informer().HasSynced // e.onEndpointsDelete将key(default/ep)入队 endpointsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; DeleteFunc: e.onEndpointsDelete, &#125;) e.endpointsLister = endpointsInformer.Lister() e.endpointsSynced = endpointsInformer.Informer().HasSynced e.triggerTimeTracker = endpointutil.NewTriggerTimeTracker() e.eventBroadcaster = broadcaster e.eventRecorder = recorder e.endpointUpdatesBatchPeriod = endpointUpdatesBatchPeriod return e&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225func (e *Controller) processNextWorkItem(ctx context.Context) bool &#123; eKey, quit := e.queue.Get() if quit &#123; return false &#125; defer e.queue.Done(eKey) // 处理逻辑都在syncService中 err := e.syncService(ctx, eKey.(string)) e.handleErr(err, eKey) return true&#125;func (e *Controller) syncService(ctx context.Context, key string) error &#123; startTime := time.Now() defer func() &#123; klog.V(4).Infof("Finished syncing service %q endpoints. (%v)", key, time.Since(startTime)) &#125;() // 通过key获取资源的命名空间和名称 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 通过endpoint名称找到对应的service,这里endpoint与service应该是一一对应的 service, err := e.serviceLister.Services(namespace).Get(name) if err != nil &#123; if !errors.IsNotFound(err) &#123; return err &#125; // Delete the corresponding endpoint, as the service has been deleted. // TODO: Please note that this will delete an endpoint when a // service is deleted. However, if we're down at the time when // the service is deleted, we will miss that deletion, so this // doesn't completely solve the problem. See #6877. // 当kube-controller-manager处于宕机状态时删除Service会导致Endpoint可能会一直存在 err = e.client.CoreV1().Endpoints(namespace).Delete(ctx, name, metav1.DeleteOptions&#123;&#125;) if err != nil &amp;&amp; !errors.IsNotFound(err) &#123; return err &#125; e.triggerTimeTracker.DeleteService(namespace, name) return nil &#125; if service.Spec.Type == v1.ServiceTypeExternalName &#123; // services with Type ExternalName receive no endpoints from this controller; // Ref: https://issues.k8s.io/105986 return nil &#125; if service.Spec.Selector == nil &#123; // services without a selector receive no endpoints from this controller; // these services will receive the endpoints that are created out-of-band via the REST API. return nil &#125; klog.V(5).Infof("About to update endpoints for service %q", key) // 通过Selector找到对应的Pod pods, err := e.podLister.Pods(service.Namespace).List(labels.Set(service.Spec.Selector).AsSelectorPreValidated()) if err != nil &#123; // Since we're getting stuff from a local cache, it is // basically impossible to get this error. return err &#125; // We call ComputeEndpointLastChangeTriggerTime here to make sure that the // state of the trigger time tracker gets updated even if the sync turns out // to be no-op and we don't update the endpoints object. endpointsLastChangeTriggerTime := e.triggerTimeTracker. ComputeEndpointLastChangeTriggerTime(namespace, service, pods) subsets := []v1.EndpointSubset&#123;&#125; var totalReadyEps int var totalNotReadyEps int for _, pod := range pods &#123; if !endpointutil.ShouldPodBeInEndpoints(pod, service.Spec.PublishNotReadyAddresses) &#123; klog.V(5).Infof("Pod %s/%s is not included on endpoints for Service %s/%s", pod.Namespace, pod.Name, service.Namespace, service.Name) continue &#125; ep, err := podToEndpointAddressForService(service, pod) if err != nil &#123; // this will happen, if the cluster runs with some nodes configured as dual stack and some as not // such as the case of an upgrade.. klog.V(2).Infof("Failed to find endpoint for service:%s with ClusterIP:%s on pod:%s with error:%v", service.Name, service.Spec.ClusterIP, klog.KObj(pod), err) continue &#125; epa := *ep if endpointutil.ShouldSetHostname(pod, service) &#123; epa.Hostname = pod.Spec.Hostname &#125; // Allow headless service not to have ports. if len(service.Spec.Ports) == 0 &#123; if service.Spec.ClusterIP == api.ClusterIPNone &#123; subsets, totalReadyEps, totalNotReadyEps = addEndpointSubset(subsets, pod, epa, nil, service.Spec.PublishNotReadyAddresses) // No need to repack subsets for headless service without ports. &#125; &#125; else &#123; for i := range service.Spec.Ports &#123; servicePort := &amp;service.Spec.Ports[i] portNum, err := podutil.FindPort(pod, servicePort) if err != nil &#123; klog.V(4).Infof("Failed to find port for service %s/%s: %v", service.Namespace, service.Name, err) continue &#125; epp := endpointPortFromServicePort(servicePort, portNum) var readyEps, notReadyEps int subsets, readyEps, notReadyEps = addEndpointSubset(subsets, pod, epa, epp, service.Spec.PublishNotReadyAddresses) totalReadyEps = totalReadyEps + readyEps totalNotReadyEps = totalNotReadyEps + notReadyEps &#125; &#125; &#125; subsets = endpoints.RepackSubsets(subsets) // See if there's actually an update here. // 通过命名空间和资源名称找对应的Endpoint // 如果Endpoint存在则是一个更新操作 currentEndpoints, err := e.endpointsLister.Endpoints(service.Namespace).Get(service.Name) if err != nil &#123; if !errors.IsNotFound(err) &#123; return err &#125; currentEndpoints = &amp;v1.Endpoints&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: service.Name, Labels: service.Labels, &#125;, &#125; &#125; // 表示Endpoint不存在，需要创建新的EndPoint // 当删除Endpoint后createEndpoints = true // 当创建Service后(Endpoint创建之前)createEndpoints = true createEndpoints := len(currentEndpoints.ResourceVersion) == 0 // Compare the sorted subsets and labels // Remove the HeadlessService label from the endpoints if it exists, // as this won't be set on the service itself // and will cause a false negative in this diff check. // But first check if it has that label to avoid expensive copies. compareLabels := currentEndpoints.Labels if _, ok := currentEndpoints.Labels[v1.IsHeadlessService]; ok &#123; compareLabels = utillabels.CloneAndRemoveLabel(currentEndpoints.Labels, v1.IsHeadlessService) &#125; // When comparing the subsets, we ignore the difference in ResourceVersion of Pod to avoid unnecessary Endpoints // updates caused by Pod updates that we don't care, e.g. annotation update. // 不必更新Endpoint if !createEndpoints &amp;&amp; endpointutil.EndpointSubsetsEqualIgnoreResourceVersion(currentEndpoints.Subsets, subsets) &amp;&amp; apiequality.Semantic.DeepEqual(compareLabels, service.Labels) &amp;&amp; capacityAnnotationSetCorrectly(currentEndpoints.Annotations, currentEndpoints.Subsets) &#123; klog.V(5).Infof("endpoints are equal for %s/%s, skipping update", service.Namespace, service.Name) return nil &#125; // Endpoint发生变化，更新对应的字段 newEndpoints := currentEndpoints.DeepCopy() newEndpoints.Subsets = subsets newEndpoints.Labels = service.Labels if newEndpoints.Annotations == nil &#123; newEndpoints.Annotations = make(map[string]string) &#125; if !endpointsLastChangeTriggerTime.IsZero() &#123; newEndpoints.Annotations[v1.EndpointsLastChangeTriggerTime] = endpointsLastChangeTriggerTime.UTC().Format(time.RFC3339Nano) &#125; else &#123; // No new trigger time, clear the annotation. delete(newEndpoints.Annotations, v1.EndpointsLastChangeTriggerTime) &#125; // subset的最大长度为1000,所以理论上一个service最多支持1000个pod // 大于1000则会发生截断 if truncateEndpoints(newEndpoints) &#123; newEndpoints.Annotations[v1.EndpointsOverCapacity] = truncated &#125; else &#123; delete(newEndpoints.Annotations, v1.EndpointsOverCapacity) &#125; if newEndpoints.Labels == nil &#123; newEndpoints.Labels = make(map[string]string) &#125; if !helper.IsServiceIPSet(service) &#123; newEndpoints.Labels = utillabels.CloneAndAddLabel(newEndpoints.Labels, v1.IsHeadlessService, "") &#125; else &#123; newEndpoints.Labels = utillabels.CloneAndRemoveLabel(newEndpoints.Labels, v1.IsHeadlessService) &#125; klog.V(4).Infof("Update endpoints for %v/%v, ready: %d not ready: %d", service.Namespace, service.Name, totalReadyEps, totalNotReadyEps) if createEndpoints &#123; // No previous endpoints, create them _, err = e.client.CoreV1().Endpoints(service.Namespace).Create(ctx, newEndpoints, metav1.CreateOptions&#123;&#125;) &#125; else &#123; // Pre-existing _, err = e.client.CoreV1().Endpoints(service.Namespace).Update(ctx, newEndpoints, metav1.UpdateOptions&#123;&#125;) &#125; if err != nil &#123; if createEndpoints &amp;&amp; errors.IsForbidden(err) &#123; // A request is forbidden primarily for two reasons: // 1. namespace is terminating, endpoint creation is not allowed by default. // 2. policy is misconfigured, in which case no service would function anywhere. // Given the frequency of 1, we log at a lower level. klog.V(5).Infof("Forbidden from creating endpoints: %v", err) // If the namespace is terminating, creates will continue to fail. Simply drop the item. if errors.HasStatusCause(err, v1.NamespaceTerminatingCause) &#123; return nil &#125; &#125; if createEndpoints &#123; e.eventRecorder.Eventf(newEndpoints, v1.EventTypeWarning, "FailedToCreateEndpoint", "Failed to create endpoint for service %v/%v: %v", service.Namespace, service.Name, err) &#125; else &#123; e.eventRecorder.Eventf(newEndpoints, v1.EventTypeWarning, "FailedToUpdateEndpoint", "Failed to update endpoint %v/%v: %v", service.Namespace, service.Name, err) &#125; return err &#125; return nil&#125; REF:1.pkg/controller/endpoint/endpoints_controller.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apiserver-builder-alpha]]></title>
    <url>%2F2023%2F04%2F29%2Fapiserver-builder-alpha%2F</url>
    <content type="text"><![CDATA[k8s有两种扩展kubernetes API的方式Custom Resources和Kubernetes API Aggregation这两者最大的区别在于CRD通过在kubernetes API内部添加资源来扩展API,而server aggregation则是通过外部服务。 Aggregation Layer: 运行在kube-apiserver并为新的API types代理请求。APIService Resources: 新的API通过 APIService进行动态 注册。Extension API Servers：响应aggregation layer代理的请求。 apiserver-boot安装https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/installing.md 初始化项目必须在$GOPATH/src目录下,这有点坑。apiserver-boot init repo --domain &lt;your-domain&gt; apiserver-boot init repo --domain extend-k8s.hysyeah.io 创建API resourceAPI resource为CRUD操作提供了REST访问点API resources通过group(package),a version,a kind(type)来定义。 apiserver-boot create group version resource --group animal --version v1beta1 --kind Cat 本地运行apiserver + controller-managerapiserver-boot run local文档上说会自动执行code generators和编译，然而事实上并没有。运行到这一步失败。apiserver-boot build executables编译二进制文件。 看下有没有注册成功kubectl api-versions 结论：apiserver-builder-alpha处于试验阶段，根据文档并不能成功运行项目并且很久没有更新不推荐使用。 REF:1.https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/2.https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/3.https://github.com/kubernetes/sample-apiserver4.https://github.com/kubernetes-sigs/apiserver-builder-alpha5.https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/api_building_overview.md6.https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/tools_user_guide.md]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-controller-manager]]></title>
    <url>%2F2023%2F04%2F27%2Fkube-controller-manager%2F</url>
    <content type="text"><![CDATA[kube-controller-manager是k8s中一个非常重要的组件,它主要负责管理k8s集群中的各种资源调节到预期状态。kube-controller-manager中有多种内置的controller，它们其实就是一个永不停止的循环，不断的调节系统达到预期的状态。k8s中有多种内置的controller，接下来这段时间会对这些controller一个一个分析。X certificate-controllerX cronjob-controllerdaemon-controllerdeployment-controllerX disruption-controllerendpoint-controllerendpointslice-controllerX endpointslicemirroring-controllerX garbagecollector-controllerhistory-controllerjob-controllernamespace-controllerX nodeipam-controllerX nodelifecycle-controllerpodautoscaler-controllerpodgc-controllerreplicaset-controllerX replication-controllerresourceclaim-controllerresourcequota-controllerserviceaccount-controllerstatefulset-controllerstorageversiongc-controllerttl-controllerttlafterfinished-controllerX volume-controller]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kube-controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[how to write a k8s admission webhook]]></title>
    <url>%2F2023%2F04%2F21%2Fk8s-webhook%2F</url>
    <content type="text"><![CDATA[什么是admission webhookKubernetes Ａdmission Webhook是一种HTTP回调机制，它允许Kubernetes调用外部Web服务，以便在某些事件发生时执行自定义代码。 Admission Webhook是Kubernetes提供的一种扩展机制，用于在资源被持久化到etcd之前，对资源进行验证或修改。Admission Webhook可以分为两种类型：Validation Webhook和Mutating Webhook。 Validation Webhook用于验证资源是否符合预期的规则，如果资源不符合规则，则会拒绝资源被持久化到etcd。 Mutating Webhook则可以对资源进行修改，在资源被持久化到etcd之前，将资源修改为期望的状态。允许对请求进行更改，例如对Pod进行注入，以添加一些特定于应用程序的设置，如日志记录、密钥管理、监视等。 Webhook机制使得用户可以根据自己的需求编写和部署自己的代码，以扩展和定制Kubernetes平台的行为。 Webhook何时调用,图片来源于网络 如何编写一个webhookWebhook请求与响应123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384// 结构体定义了请求和响应的字段// staging/src/k8s.io/api/admission/v1/types.gotype AdmissionReview struct &#123; metav1.TypeMeta `json:",inline"` // Request describes the attributes for the admission request. // +optional Request *AdmissionRequest `json:"request,omitempty" protobuf:"bytes,1,opt,name=request"` // Response describes the attributes for the admission response. // +optional Response *AdmissionResponse `json:"response,omitempty" protobuf:"bytes,2,opt,name=response"`&#125;type AdmissionRequest struct &#123; UID types.UID `json:"uid" protobuf:"bytes,1,opt,name=uid"` // Kind is the fully-qualified type of object being submitted (for example, v1.Pod or autoscaling.v1.Scale) Kind metav1.GroupVersionKind `json:"kind" protobuf:"bytes,2,opt,name=kind"` // Resource is the fully-qualified resource being requested (for example, v1.pods) Resource metav1.GroupVersionResource `json:"resource" protobuf:"bytes,3,opt,name=resource"` // SubResource is the subresource being requested, if any (for example, "status" or "scale") // +optional SubResource string `json:"subResource,omitempty" protobuf:"bytes,4,opt,name=subResource"` // +optional RequestKind *metav1.GroupVersionKind `json:"requestKind,omitempty" protobuf:"bytes,13,opt,name=requestKind"` // +optional RequestResource *metav1.GroupVersionResource `json:"requestResource,omitempty" protobuf:"bytes,14,opt,name=requestResource"` // +optional RequestSubResource string `json:"requestSubResource,omitempty" protobuf:"bytes,15,opt,name=requestSubResource"` // Name is the name of the object as presented in the request. On a CREATE operation, the client may omit name and // rely on the server to generate the name. If that is the case, this field will contain an empty string. // +optional Name string `json:"name,omitempty" protobuf:"bytes,5,opt,name=name"` // Namespace is the namespace associated with the request (if any). // +optional Namespace string `json:"namespace,omitempty" protobuf:"bytes,6,opt,name=namespace"` // Operation is the operation being performed. This may be different than the operation // requested. e.g. a patch can result in either a CREATE or UPDATE Operation. Operation Operation `json:"operation" protobuf:"bytes,7,opt,name=operation"` // UserInfo is information about the requesting user UserInfo authenticationv1.UserInfo `json:"userInfo" protobuf:"bytes,8,opt,name=userInfo"` // Object is the object from the incoming request. // +optional Object runtime.RawExtension `json:"object,omitempty" protobuf:"bytes,9,opt,name=object"` // OldObject is the existing object. Only populated for DELETE and UPDATE requests. // +optional OldObject runtime.RawExtension `json:"oldObject,omitempty" protobuf:"bytes,10,opt,name=oldObject"` // +optional DryRun *bool `json:"dryRun,omitempty" protobuf:"varint,11,opt,name=dryRun"` // +optional Options runtime.RawExtension `json:"options,omitempty" protobuf:"bytes,12,opt,name=options"`&#125;type AdmissionResponse struct &#123; // UID is an identifier for the individual request/response. // This must be copied over from the corresponding AdmissionRequest. UID types.UID `json:"uid" protobuf:"bytes,1,opt,name=uid"` // Allowed indicates whether or not the admission request was permitted. Allowed bool `json:"allowed" protobuf:"varint,2,opt,name=allowed"` // Result contains extra details into why an admission request was denied. // This field IS NOT consulted in any way if "Allowed" is "true". // +optional Result *metav1.Status `json:"status,omitempty" protobuf:"bytes,3,opt,name=status"` // The patch body. Currently we only support "JSONPatch" which implements RFC 6902. // +optional Patch []byte `json:"patch,omitempty" protobuf:"bytes,4,opt,name=patch"` // The type of Patch. Currently we only allow "JSONPatch". // +optional PatchType *PatchType `json:"patchType,omitempty" protobuf:"bytes,5,opt,name=patchType"` // +optional AuditAnnotations map[string]string `json:"auditAnnotations,omitempty" protobuf:"bytes,6,opt,name=auditAnnotations"` // +optional Warnings []string `json:"warnings,omitempty" protobuf:"bytes,7,rep,name=warnings"`&#125; 请求: Webhook 发送 POST 请求时，请设置 Content-Type: application/json 并对 admission.k8s.io API 组中的 AdmissionReview 对象进行序列化，将所得到的 JSON 作为请求的主体。 响应: Webhook 使用 HTTP 200 状态码、Content-Type: application/json 和一个包含 AdmissionReview 对象的 JSON 序列化格式来发送响应。该 AdmissionReview 对象与发送的版本相同，且其中包含的 response 字段已被有效填充。响应示例:12345678&#123; "apiVersion": "admission.k8s.io/v1", "kind": "AdmissionReview", "response": &#123; "uid": "&lt;value from request.uid&gt;", "allowed": true &#125;&#125; 开始编写Webhook 配置ValidatingAdmissionWebhook,此准入控制器调用与请求匹配的所有验证性 Webhook。 匹配的 Webhook 将被并行调用。如果其中任何一个拒绝请求，则整个请求将失败。 该准入控制器仅在验证（Validating）阶段运行 123456789101112131415161718192021222324apiVersion: admissionregistration.k8s.io/v1kind: ValidatingWebhookConfigurationmetadata: name: pod-webhookwebhooks: - name: pod.webhook.hysyeah.com # 定义访问的服务,如果是外部服务则指定对应的URL clientConfig: service: name: pod-webhook namespace: default path: "/configmaps" caBundle: "update &lt;&gt;" # 匹配规则,如果传入请求与rules的指定operations,groups,version,resources匹配 # 则该请求将发送到webhook rules: - operations: [ "CREATE", "UPDATE", "DELETE"] apiGroups: ["apps", ""] apiVersions: ["v1"] resources: ["configmaps"] failurePolicy: Ignore sideEffects: None admissionReviewVersions: - v1 配置MutatingWebhookConfiguration 123456789101112131415161718192021222324252627apiVersion: admissionregistration.k8s.io/v1kind: MutatingWebhookConfigurationmetadata: name: pod-webhookwebhooks: - name: pod.webhook.hysyeah.com rules: - apiGroups: - "" apiVersions: - v1 operations: - CREATE - UPDATE resources: - pods failurePolicy: Ignore sideEffects: None admissionReviewVersions: - v1 clientConfig: # base64 -w 0 ca.crt caBundle: "update &lt;&gt;" service: name: pod-webhook namespace: default path: "/add-label" 构建镜像nerdctl build -t hysyeah/pod-webhook:v3 . 12345678910111213FROM golang:1.18-alpineWORKDIR /appCOPY . .RUN go env -w GO111MODULE=onRUN go env -w GOPROXY=https://goproxy.cn,directRUN go build -o mainRUN chmod +x mainEXPOSE 443CMD ["./main"] 编写对应的deployment 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647apiVersion: apps/v1kind: Deploymentmetadata: name: pod-webhookspec: replicas: 1 selector: matchLabels: app: pod-webhook template: metadata: labels: app: pod-webhook spec: containers: - name: pod-webhook image: hysyeah/pod-webhook:v3 imagePullPolicy: IfNotPresent command: ["/app/main"] args: ["-tls-cert-file=/keys/tls.crt","-tls-private-key-file=/keys/tls.key"] ports: - containerPort: 443 volumeMounts: - name: tls-keys mountPath: /keys volumes: - name: tls-keys secret: secretName: pod-webhook-tls items: - key: tls.crt path: tls.crt - key: tls.key path: tls.key---apiVersion: v1kind: Servicemetadata: name: pod-webhookspec: selector: app: pod-webhook ports: - name: tcp port: 443 targetPort: 443 部署webhook执行kubectl apply -f deployment.yaml 验证webhook 新建一个pod,发现pod添加了label: added-label=yes 12NAME READY STATUS RESTARTS AGE LABELSmy-curl 1/1 Running 0 170m added-label=yes,app=my-curl 新建一个configmap,包含webhook-e2e-test: webhook-disallow结果如下: 1Error from server (the configmap contains unwanted key and value): error when creating "configmap.yaml": admission webhook "pod.webhook.hysyeah.com" denied the request: the configmap contains unwanted key and value 完整的代码可查看完整代码 注意事项 确保启用 MutatingAdmissionWebhook 和ValidatingAdmissionWebhook控制器 确保启用了 admissionregistration.k8s.io/v1 API 确保您的代码不会影响Kubernetes集群的稳定性和安全性 当用户尝试创建的对象与返回的对象不同时，用户可能会感到困惑。 与覆盖原始请求中设置的字段相比，使用原始请求未设置的字段会引起问题的可能性较小。 应尽量避免覆盖原始请求中的字段设置 REF:1.https://github.com/jpeeler/podpreset-crd/tree/master/webhook2.https://github.com/kubernetes/kubernetes/tree/release-1.21/test/images/agnhost/webhook3.https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/4.https://banzaicloud.com/blog/k8s-admission-webhooks/]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s Deployment滚动升级可可靠吗]]></title>
    <url>%2F2023%2F04%2F19%2Fk8s%E6%97%A0%E6%8D%9F%E5%8D%87%E7%BA%A7%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Kubernetes Deployment使用滚动升级策略进行升级，可以保证升级过程中的可靠性和容错性。 滚动升级通过逐步将新版本的Pod逐步添加到服务中，同时逐步停止旧版本的Pod，实现了服务的无停机升级。在升级过程中，Kubernetes会监控新版本Pod的运行状态，并在满足指定的健康检查条件之后才继续升级下一个Pod，确保服务的稳定性和可靠性。使用Deployment滚动升级虽说可以实现无停机服务，但是它是完全客户端无感吗?答案是否定的!!! 我们来做个实验,使用go提供一个简单的接口。超时时间为30s,方便实验。 编写代码123456789101112131415161718192021func main() &#123; // 创建一个HTTP服务器 server := &amp;http.Server&#123; Addr: ":8080", Handler: http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) &#123; // 处理请求 time.Sleep(time.Second * 10) fmt.Fprintln(w, "Hello, World!") &#125;), &#125; // 设置超时时间为30秒 server.ReadTimeout = 30 * time.Second server.WriteTimeout = 30 * time.Second // 启动服务器 err := server.ListenAndServe() if err != nil &#123; fmt.Println("服务器启动失败:", err) &#125;&#125; 制作镜像123456789101112FROM golang:1.17-alpine AS builderWORKDIR /appCOPY . .RUN go build -o mainRUN chmod +x mainEXPOSE 8080CMD [&quot;./main&quot;] 12➜ curl http://127.0.0.1:8080curl: (52) Empty reply from server 构建一个镜像将下面的代码保存为Dockerfile,然后使用nerdctl构建镜像,也可以直接拉取。nerdctl的安装可参考附录。sudo nerdctl build -t hysyeah/my-curl:v1 . 123456789101112FROM golang:1.17-alpine AS builderWORKDIR /appCOPY . .RUN go build -o mainRUN chmod +x mainEXPOSE 8080CMD [&quot;./main&quot;] 以Deployment形式启动设置3个复本，策略为RollingUpdate。1234567891011121314151617181920212223242526272829303132333435363738# deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: my-curlspec: replicas: 3 selector: matchLabels: app: my-curl strategy: type: RollingUpdate template: metadata: labels: app: my-curl spec: containers: - name: my-curl image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: my-curlspec: selector: app: my-curl ports: - name: http port: 8080 targetPort: 8080 type: NodePort--- 测试滚动升级启动Deploymentkubectl apply -f deployment.yaml 访问服务curl http://127.0.0.1:31918 升级Deployment在服务返回之前，升级Deployment(直接修改image,然后apply)，然后你会得到如下错误。说明Deployment升级对客户端是有影响。为什么会有影响可以参考一个pod的终止流程 12➜ curl http://127.0.0.1:31918curl: (52) Empty reply from server 原因是虽说是滚动升级，Deployment只保证在升级过程中服务不会中止并不会保证，已有的请求它并不会保证处理完。 如何对客户端无感在应用中优雅退出在退出中使用优雅退出机制，监听SIGTERM信号。可以参考一个pod的终止流程，了解pod是如何退出的。 使用pre-stop hookPod钩子是在Pod生命周期的不同阶段中执行的命令或脚本。使用Pod钩子，可以在Pod启动之前或之后，容器启动之前或之后，容器退出之前或之后等时刻执行定制化的逻辑。 Pod钩子支持的钩子类型有以下四种： PostStart: Pod创建后，容器创建前执行。PreStop: 容器退出前执行，可用于准备容器退出前的工作，如保存状态、备份数据等。PreStart: Pod创建后，容器创建前执行，但只在容器是第一个启动的容器时才执行。PostStop: 容器退出后执行，用于清理容器退出后的工作。 可以使用pre-stop hook在容器退出前执行一段代码或脚本。添加preStop在容器退出前休眠20s,再次执行curl命令和升级操作发现接口能被正常处理。 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deploymentmetadata: name: my-curlspec: replicas: 3 selector: matchLabels: app: my-curl strategy: type: RollingUpdate template: metadata: labels: app: my-curl spec: containers: - name: my-curl image: hysyeah/my-curl:gracefulv2 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: ["sleep","20"] ports: - containerPort: 8080 附录-nerdctl套装安装下载对应的压缩包,解压执行如下命令。 1234567➜ cp lib/systemd/system/buildkit.service /lib/systemd/system/➜ sudo cp bin/buildkitd /usr/local/bin/➜ sudo cp bin/nerdctl /usr/local/bin/➜ sudo cp bin/buildctl /usr/local/bin/➜ sudo systemctl start buildkit ➜ systemctl status buildkit REF:1.https://iximiuz.com/en/posts/containerd-command-line-clients/]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个pod的正常终止流程]]></title>
    <url>%2F2023%2F04%2F18%2F%E4%B8%80%E4%B8%AApod%E7%9A%84%E6%AD%A3%E5%B8%B8%E7%BB%88%E6%AD%A2%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[当你使用命令kubectl delete pod pod_name来删除一个pod,你有了解过整个流程是怎么样的吗?pod是如何被删除的，kubelet做了什么，容器运行时又做了什么?今天就来分析一下一个pod正常的终止流程。 新建一个pod你可以执行命令kubectl apply -f mycurl.yaml创建一个pod12345678910111213apiVersion: v1kind: Podmetadata: name: my-curl labels: app: my-curlspec: containers: - name: my-curl image: hysyeah/my-curl:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 可以看到pod已经运行起来123➜ k get podNAME READY STATUS RESTARTS AGEmy-curl 1/1 Running 0 3s 删除pod执行命令kubectl delete pod my-curl --v=6,从日志中可以看到kubectl发送了一个DELETE请求给apiserver,请求路径为api/v1/namespaces/default/pods/my-curl。然后输出日志表示pod已经被删除,看似只发送了一个请求就把pod删除了，但删除一个其实是需要多个组件的协作的12I0418 round_trippers.go:553] DELETE https://192.168.2.123:6443/api/v1/namespaces/default/pods/my-curl 200 OK in 12 millisecondspod &quot;my-curl&quot; deleted apiserver删除podkubectl发送DELETE请求给apiserver,apiserver调用etcd接口删除对应的记录。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go// 对etcd接口的一层封装，比如会执行删除前的一些策略func (e *Store) Delete(ctx context.Context, name string, deleteValidation rest.ValidateObjectFunc, options *metav1.DeleteOptions) (runtime.Object, bool, error) &#123; key, err := e.KeyFunc(ctx, name) if err != nil &#123; return nil, false, err &#125; obj := e.NewFunc() qualifiedResource := e.qualifiedResourceFromContext(ctx) if err = e.Storage.Get(ctx, key, storage.GetOptions&#123;&#125;, obj); err != nil &#123; return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name) &#125; // support older consumers of delete by treating "nil" as delete immediately if options == nil &#123; options = metav1.NewDeleteOptions(0) &#125; var preconditions storage.Preconditions if options.Preconditions != nil &#123; preconditions.UID = options.Preconditions.UID preconditions.ResourceVersion = options.Preconditions.ResourceVersion &#125; // BeforeDelete函数用于测试对象是否可以进行优雅删除。 // 如果设置了graceful参数，表示对象应该进行优雅删除。如果设置了gracefulPending参数，表示对象已经被优雅删除。 // BeforeDelete会设置对象的DeletionTimestamp graceful, pendingGraceful, err := rest.BeforeDelete(e.DeleteStrategy, ctx, obj, options) if err != nil &#123; return nil, false, err &#125; // this means finalizers cannot be updated via DeleteOptions if a deletion is already pending if pendingGraceful &#123; out, err := e.finalizeDelete(ctx, obj, false, options) return out, false, err &#125; // check if obj has pending finalizers accessor, err := meta.Accessor(obj) if err != nil &#123; return nil, false, apierrors.NewInternalError(err) &#125; pendingFinalizers := len(accessor.GetFinalizers()) != 0 var ignoreNotFound bool var deleteImmediately bool = true var lastExisting, out runtime.Object // Handle combinations of graceful deletion and finalization by issuing // the correct updates. shouldUpdateFinalizers, _ := deletionFinalizersForGarbageCollection(ctx, e, accessor, options) // TODO: remove the check, because we support no-op updates now. if graceful || pendingFinalizers || shouldUpdateFinalizers &#123; err, ignoreNotFound, deleteImmediately, out, lastExisting = e.updateForGracefulDeletionAndFinalizers(ctx, name, key, options, preconditions, deleteValidation, obj) // Update the preconditions.ResourceVersion if set since we updated the object. if err == nil &amp;&amp; deleteImmediately &amp;&amp; preconditions.ResourceVersion != nil &#123; accessor, err = meta.Accessor(out) if err != nil &#123; return out, false, apierrors.NewInternalError(err) &#125; resourceVersion := accessor.GetResourceVersion() preconditions.ResourceVersion = &amp;resourceVersion &#125; &#125; // !deleteImmediately covers all cases where err != nil. We keep both to be future-proof. if !deleteImmediately || err != nil &#123; return out, false, err &#125; // Going further in this function is not useful when we are // performing a dry-run request. Worse, it will actually // override "out" with the version of the object in database // that doesn't have the finalizer and deletiontimestamp set // (because the update above was dry-run too). If we already // have that version available, let's just return it now, // otherwise, we can call dry-run delete that will get us the // latest version of the object. if dryrun.IsDryRun(options.DryRun) &amp;&amp; out != nil &#123; return out, true, nil &#125; // delete immediately, or no graceful deletion supported klog.V(6).InfoS("Going to delete object from registry", "object", klog.KRef(genericapirequest.NamespaceValue(ctx), name)) out = e.NewFunc() // 只有能立即删除的情况下才会直接删除etcd中的数据 if err := e.Storage.Delete(ctx, key, out, &amp;preconditions, storage.ValidateObjectFunc(deleteValidation), dryrun.IsDryRun(options.DryRun), nil); err != nil &#123; // Please refer to the place where we set ignoreNotFound for the reason // why we ignore the NotFound error . if storage.IsNotFound(err) &amp;&amp; ignoreNotFound &amp;&amp; lastExisting != nil &#123; // The lastExisting object may not be the last state of the object // before its deletion, but it's the best approximation. out, err := e.finalizeDelete(ctx, lastExisting, true, options) return out, true, err &#125; return nil, false, storeerr.InterpretDeleteError(err, qualifiedResource, name) &#125; out, err = e.finalizeDelete(ctx, out, true, options) return out, true, err&#125;// staging/src/k8s.io/apiserver/pkg/registry/rest/delete.gofunc BeforeDelete(strategy RESTDeleteStrategy, ctx context.Context, obj runtime.Object, options *metav1.DeleteOptions) (graceful, gracefulPending bool, err error) &#123; objectMeta, gvk, kerr := objectMetaAndKind(strategy, obj) if kerr != nil &#123; return false, false, kerr &#125; if errs := validation.ValidateDeleteOptions(options); len(errs) &gt; 0 &#123; return false, false, errors.NewInvalid(schema.GroupKind&#123;Group: metav1.GroupName, Kind: "DeleteOptions"&#125;, "", errs) &#125; // Checking the Preconditions here to fail early. They'll be enforced later on when we actually do the deletion, too. if options.Preconditions != nil &#123; if options.Preconditions.UID != nil &amp;&amp; *options.Preconditions.UID != objectMeta.GetUID() &#123; return false, false, errors.NewConflict(schema.GroupResource&#123;Group: gvk.Group, Resource: gvk.Kind&#125;, objectMeta.GetName(), fmt.Errorf("the UID in the precondition (%s) does not match the UID in record (%s). The object might have been deleted and then recreated", *options.Preconditions.UID, objectMeta.GetUID())) &#125; if options.Preconditions.ResourceVersion != nil &amp;&amp; *options.Preconditions.ResourceVersion != objectMeta.GetResourceVersion() &#123; return false, false, errors.NewConflict(schema.GroupResource&#123;Group: gvk.Group, Resource: gvk.Kind&#125;, objectMeta.GetName(), fmt.Errorf("the ResourceVersion in the precondition (%s) does not match the ResourceVersion in record (%s). The object might have been modified", *options.Preconditions.ResourceVersion, objectMeta.GetResourceVersion())) &#125; &#125; // Negative values will be treated as the value `1s` on the delete path. if gracePeriodSeconds := options.GracePeriodSeconds; gracePeriodSeconds != nil &amp;&amp; *gracePeriodSeconds &lt; 0 &#123; options.GracePeriodSeconds = utilpointer.Int64(1) &#125; if deletionGracePeriodSeconds := objectMeta.GetDeletionGracePeriodSeconds(); deletionGracePeriodSeconds != nil &amp;&amp; *deletionGracePeriodSeconds &lt; 0 &#123; objectMeta.SetDeletionGracePeriodSeconds(utilpointer.Int64(1)) &#125; gracefulStrategy, ok := strategy.(RESTGracefulDeleteStrategy) if !ok &#123; // If we're not deleting gracefully there's no point in updating Generation, as we won't update // the obcject before deleting it. return false, false, nil &#125; // if the object is already being deleted, no need to update generation. if objectMeta.GetDeletionTimestamp() != nil &#123; // if we are already being deleted, we may only shorten the deletion grace period // this means the object was gracefully deleted previously but deletionGracePeriodSeconds was not set, // so we force deletion immediately // IMPORTANT: // The deletion operation happens in two phases. // 1. Update to set DeletionGracePeriodSeconds and DeletionTimestamp // 2. Delete the object from storage. // If the update succeeds, but the delete fails (network error, internal storage error, etc.), // a resource was previously left in a state that was non-recoverable. We // check if the existing stored resource has a grace period as 0 and if so // attempt to delete immediately in order to recover from this scenario. if objectMeta.GetDeletionGracePeriodSeconds() == nil || *objectMeta.GetDeletionGracePeriodSeconds() == 0 &#123; return false, false, nil &#125; // only a shorter grace period may be provided by a user if options.GracePeriodSeconds != nil &#123; period := int64(*options.GracePeriodSeconds) if period &gt;= *objectMeta.GetDeletionGracePeriodSeconds() &#123; return false, true, nil &#125; newDeletionTimestamp := metav1.NewTime( objectMeta.GetDeletionTimestamp().Add(-time.Second * time.Duration(*objectMeta.GetDeletionGracePeriodSeconds())). Add(time.Second * time.Duration(*options.GracePeriodSeconds))) objectMeta.SetDeletionTimestamp(&amp;newDeletionTimestamp) objectMeta.SetDeletionGracePeriodSeconds(&amp;period) return true, false, nil &#125; // graceful deletion is pending, do nothing options.GracePeriodSeconds = objectMeta.GetDeletionGracePeriodSeconds() return false, true, nil &#125; // `CheckGracefulDelete` will be implemented by specific strategy if !gracefulStrategy.CheckGracefulDelete(ctx, obj, options) &#123; return false, false, nil &#125; if options.GracePeriodSeconds == nil &#123; return false, false, errors.NewInternalError(fmt.Errorf("options.GracePeriodSeconds should not be nil")) &#125; now := metav1.NewTime(metav1.Now().Add(time.Second * time.Duration(*options.GracePeriodSeconds))) objectMeta.SetDeletionTimestamp(&amp;now) objectMeta.SetDeletionGracePeriodSeconds(options.GracePeriodSeconds) // If it's the first graceful deletion we are going to set the DeletionTimestamp to non-nil. // Controllers of the object that's being deleted shouldn't take any nontrivial actions, hence its behavior changes. // Thus we need to bump object's Generation (if set). This handles generation bump during graceful deletion. // The bump for objects that don't support graceful deletion is handled in pkg/registry/generic/registry/store.go. if objectMeta.GetGeneration() &gt; 0 &#123; objectMeta.SetGeneration(objectMeta.GetGeneration() + 1) &#125; return true, false, nil&#125; kubelet开始工作当apiserver删除pod后，因为kubelet对pod资源进行了监听，所以当pod被删除后，kubelet可以获得pod删除的事件。123456789101112131415161718192021222324252627282930313233343536373839// 会进入kubetypes.DELETE分支func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123; select &#123; case u, open := &lt;-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open &#123; klog.ErrorS(nil, "Update channel is closed, exiting the sync loop") return false &#125; switch u.Op &#123; case kubetypes.ADD: klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: klog.V(2).InfoS("SyncLoop UPDATE", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: klog.V(2).InfoS("SyncLoop REMOVE", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: klog.V(4).InfoS("SyncLoop RECONCILE", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: klog.V(2).InfoS("SyncLoop DELETE", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) // DELETE is treated as a UPDATE because of graceful deletion. handler.HandlePodUpdates(u.Pods) case kubetypes.SET: // TODO: Do we want to support this? klog.ErrorS(nil, "Kubelet does not support snapshot update") default: klog.ErrorS(nil, "Invalid operation type received", "operation", u.Op) &#125; 然后会进入到容器删除流程(这里是进入了另一个goroutine里)，函数调用流程如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134// killPod instructs the container runtime to kill the pod. This method requires that// the pod status contains the result of the last syncPod, otherwise it may fail to// terminate newly created containers and sandboxes.// 调用容器运行删除对应的容器func (kl *Kubelet) killPod(ctx context.Context, pod *v1.Pod, p kubecontainer.Pod, gracePeriodOverride *int64) error &#123; // Call the container runtime KillPod method which stops all known running containers of the pod if err := kl.containerRuntime.KillPod(ctx, pod, p, gracePeriodOverride); err != nil &#123; return err &#125; if err := kl.containerManager.UpdateQOSCgroups(); err != nil &#123; klog.V(2).InfoS("Failed to update QoS cgroups while killing pod", "err", err) &#125; return nil&#125;// 删除pod内的所有容器func (m *kubeGenericRuntimeManager) KillPod(ctx context.Context, pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) error &#123; err := m.killPodWithSyncResult(ctx, pod, runningPod, gracePeriodOverride) return err.Error()&#125;// 删除一个Runing状态的podfunc (m *kubeGenericRuntimeManager) killPodWithSyncResult(ctx context.Context, pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (result kubecontainer.PodSyncResult) &#123; killContainerResults := m.killContainersWithSyncResult(ctx, pod, runningPod, gracePeriodOverride) for _, containerResult := range killContainerResults &#123; result.AddSyncResult(containerResult) &#125; // stop sandbox, the sandbox will be removed in GarbageCollect killSandboxResult := kubecontainer.NewSyncResult(kubecontainer.KillPodSandbox, runningPod.ID) result.AddSyncResult(killSandboxResult) // Stop all sandboxes belongs to same pod for _, podSandbox := range runningPod.Sandboxes &#123; if err := m.runtimeService.StopPodSandbox(ctx, podSandbox.ID.ID); err != nil &amp;&amp; !crierror.IsNotFound(err) &#123; killSandboxResult.Fail(kubecontainer.ErrKillPodSandbox, err.Error()) klog.ErrorS(nil, "Failed to stop sandbox", "podSandboxID", podSandbox.ID) &#125; &#125; return&#125;// killContainersWithSyncResult kills all pod's containers with sync results.func (m *kubeGenericRuntimeManager) killContainersWithSyncResult(ctx context.Context, pod *v1.Pod, runningPod kubecontainer.Pod, gracePeriodOverride *int64) (syncResults []*kubecontainer.SyncResult) &#123; containerResults := make(chan *kubecontainer.SyncResult, len(runningPod.Containers)) wg := sync.WaitGroup&#123;&#125; wg.Add(len(runningPod.Containers)) for _, container := range runningPod.Containers &#123; go func(container *kubecontainer.Container) &#123; defer utilruntime.HandleCrash() defer wg.Done() killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name) if err := m.killContainer(ctx, pod, container.ID, container.Name, "", reasonUnknown, gracePeriodOverride); err != nil &#123; killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) // Use runningPod for logging as the pod passed in could be *nil*. klog.ErrorS(err, "Kill container failed", "pod", klog.KRef(runningPod.Namespace, runningPod.Name), "podUID", runningPod.ID, "containerName", container.Name, "containerID", container.ID) &#125; containerResults &lt;- killContainerResult &#125;(container) &#125; wg.Wait() close(containerResults) for containerResult := range containerResults &#123; syncResults = append(syncResults, containerResult) &#125; return&#125;// kill一个容器// 如果设置了pre-stop则执行对应的hooks// 调用cri停止容器func (m *kubeGenericRuntimeManager) killContainer(ctx context.Context, pod *v1.Pod, containerID kubecontainer.ContainerID, containerName string, message string, reason containerKillReason, gracePeriodOverride *int64) error &#123; var containerSpec *v1.Container if pod != nil &#123; if containerSpec = kubecontainer.GetContainerSpec(pod, containerName); containerSpec == nil &#123; return fmt.Errorf("failed to get containerSpec %q (id=%q) in pod %q when killing container for reason %q", containerName, containerID.String(), format.Pod(pod), message) &#125; &#125; else &#123; // Restore necessary information if one of the specs is nil. restoredPod, restoredContainer, err := m.restoreSpecsFromContainerLabels(ctx, containerID) if err != nil &#123; return err &#125; pod, containerSpec = restoredPod, restoredContainer &#125; // From this point, pod and container must be non-nil. // gracePeriod默认为30s gracePeriod := setTerminationGracePeriod(pod, containerSpec, containerName, containerID, reason) if len(message) == 0 &#123; message = fmt.Sprintf("Stopping container %s", containerSpec.Name) &#125; m.recordContainerEvent(pod, containerSpec, containerID.ID, v1.EventTypeNormal, events.KillingContainer, message) // Run the pre-stop lifecycle hooks if applicable and if there is enough time to run it // 执行pre-stop hooks并更新gracePeriod,gracePeriod会作为超时参数传递给cri if containerSpec.Lifecycle != nil &amp;&amp; containerSpec.Lifecycle.PreStop != nil &amp;&amp; gracePeriod &gt; 0 &#123; gracePeriod = gracePeriod - m.executePreStopHook(ctx, pod, containerID, containerSpec, gracePeriod) &#125; // always give containers a minimal shutdown window to avoid unnecessary SIGKILLs if gracePeriod &lt; minimumGracePeriodInSeconds &#123; gracePeriod = minimumGracePeriodInSeconds &#125; if gracePeriodOverride != nil &#123; gracePeriod = *gracePeriodOverride klog.V(3).InfoS("Killing container with a grace period override", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerName, "containerID", containerID.String(), "gracePeriod", gracePeriod) &#125; klog.V(2).InfoS("Killing container with a grace period", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerName, "containerID", containerID.String(), "gracePeriod", gracePeriod) // 真正进行cri调用的地方,如果要看StopContainer具体做了什么需要查看对应cri的代码 // 因为使用的是containerd,因此可以在containerd项目中查看对应的源码 // 如果没有设置pre-stop hooks，gracePeriod默认为30s(这其实也就是stopContainer的超时时间) // 如果你的应用没有设置处理SIGTERM信号的话，一般来说进程会马上退出(也就是容器会被删除) err := m.runtimeService.StopContainer(ctx, containerID.ID, gracePeriod) if err != nil &amp;&amp; !crierror.IsNotFound(err) &#123; klog.ErrorS(err, "Container termination failed with gracePeriod", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerName, "containerID", containerID.String(), "gracePeriod", gracePeriod) return err &#125; klog.V(3).InfoS("Container exited normally", "pod", klog.KObj(pod), "podUID", pod.UID, "containerName", containerName, "containerID", containerID.String()) return nil&#125; 调用Containerd接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164// pkg/cri/sbserver/container_stop.gofunc (c *criService) StopContainer(ctx context.Context, r *runtime.StopContainerRequest) (*runtime.StopContainerResponse, error) &#123; start := time.Now() // Get container config from container store. container, err := c.containerStore.Get(r.GetContainerId()) if err != nil &#123; return nil, fmt.Errorf("an error occurred when try to find container %q: %w", r.GetContainerId(), err) &#125; // 停止一个容器，可以直接看这个函数 if err := c.stopContainer(ctx, container, time.Duration(r.GetTimeout())*time.Second); err != nil &#123; return nil, err &#125; sandbox, err := c.sandboxStore.Get(container.SandboxID) // 删除对应的sandbox if err != nil &#123; err = c.nri.StopContainer(ctx, nil, &amp;container) &#125; else &#123; err = c.nri.StopContainer(ctx, &amp;sandbox, &amp;container) &#125; if err != nil &#123; log.G(ctx).WithError(err).Error("NRI failed to stop container") &#125; i, err := container.Container.Info(ctx) if err != nil &#123; return nil, fmt.Errorf("get container info: %w", err) &#125; containerStopTimer.WithValues(i.Runtime.Name).UpdateSince(start) return &amp;runtime.StopContainerResponse&#123;&#125;, nil&#125;func (c *criService) stopContainer(ctx context.Context, container containerstore.Container, timeout time.Duration) error &#123; id := container.ID sandboxID := container.SandboxID state := container.Status.Get().State() if state != runtime.ContainerState_CONTAINER_RUNNING &amp;&amp; state != runtime.ContainerState_CONTAINER_UNKNOWN &#123; log.G(ctx).Infof("Container to stop %q must be in running or unknown state, current state %q", id, criContainerStateToString(state)) return nil &#125; task, err := container.Container.Task(ctx, nil) if err != nil &#123; if !errdefs.IsNotFound(err) &#123; return fmt.Errorf("failed to get task for container %q: %w", id, err) &#125; // Don't return for unknown state, some cleanup needs to be done. if state == runtime.ContainerState_CONTAINER_UNKNOWN &#123; return cleanupUnknownContainer(ctx, id, container, sandboxID, c) &#125; return nil &#125; // Handle unknown state. if state == runtime.ContainerState_CONTAINER_UNKNOWN &#123; // Start an exit handler for containers in unknown state. waitCtx, waitCancel := context.WithCancel(ctrdutil.NamespacedContext()) defer waitCancel() exitCh, err := task.Wait(waitCtx) if err != nil &#123; if !errdefs.IsNotFound(err) &#123; return fmt.Errorf("failed to wait for task for %q: %w", id, err) &#125; return cleanupUnknownContainer(ctx, id, container, sandboxID, c) &#125; exitCtx, exitCancel := context.WithCancel(context.Background()) stopCh := c.eventMonitor.startContainerExitMonitor(exitCtx, id, task.Pid(), exitCh) defer func() &#123; exitCancel() // This ensures that exit monitor is stopped before // `Wait` is cancelled, so no exit event is generated // because of the `Wait` cancellation. &lt;-stopCh &#125;() &#125; // We only need to kill the task. The event handler will Delete the // task from containerd after it handles the Exited event. if timeout &gt; 0 &#123; // 如果timeout &gt; 0 相当于是执行SIGTERM命令 stopSignal := "SIGTERM" if container.StopSignal != "" &#123; stopSignal = container.StopSignal &#125; else &#123; // The image may have been deleted, and the `StopSignal` field is // just introduced to handle that. // However, for containers created before the `StopSignal` field is // introduced, still try to get the stop signal from the image config. // If the image has been deleted, logging an error and using the // default SIGTERM is still better than returning error and leaving // the container unstoppable. (See issue #990) // TODO(random-liu): Remove this logic when containerd 1.2 is deprecated. image, err := c.GetImage(container.ImageRef) if err != nil &#123; if !errdefs.IsNotFound(err) &#123; return fmt.Errorf("failed to get image %q: %w", container.ImageRef, err) &#125; log.G(ctx).Warningf("Image %q not found, stop container with signal %q", container.ImageRef, stopSignal) &#125; else &#123; if image.ImageSpec.Config.StopSignal != "" &#123; stopSignal = image.ImageSpec.Config.StopSignal &#125; &#125; &#125; sig, err := signal.ParseSignal(stopSignal) if err != nil &#123; return fmt.Errorf("failed to parse stop signal %q: %w", stopSignal, err) &#125; var sswt bool if container.IsStopSignaledWithTimeout == nil &#123; log.G(ctx).Infof("unable to ensure stop signal %v was not sent twice to container %v", sig, id) sswt = true &#125; else &#123; sswt = atomic.CompareAndSwapUint32(container.IsStopSignaledWithTimeout, 0, 1) &#125; if sswt &#123; log.G(ctx).Infof("Stop container %q with signal %v", id, sig) // 执行kill操作，这里其实就是发送一个信号给对应的进程 if err = task.Kill(ctx, sig); err != nil &amp;&amp; !errdefs.IsNotFound(err) &#123; return fmt.Errorf("failed to stop container %q: %w", id, err) &#125; &#125; else &#123; log.G(ctx).Infof("Skipping the sending of signal %v to container %q because a prior stop with timeout&gt;0 request already sent the signal", sig, id) &#125; sigTermCtx, sigTermCtxCancel := context.WithTimeout(ctx, timeout) defer sigTermCtxCancel() // 等待进程退出直到超时，如果超时了也会直接执行下面SIGKILL操作，直接kill进程 err = c.waitContainerStop(sigTermCtx, container) if err == nil &#123; // Container stopped on first signal no need for SIGKILL return nil &#125; // If the parent context was cancelled or exceeded return immediately if ctx.Err() != nil &#123; return ctx.Err() &#125; // sigTermCtx was exceeded. Send SIGKILL log.G(ctx).Debugf("Stop container %q with signal %v timed out", id, sig) &#125; log.G(ctx).Infof("Kill container %q", id) // 如果timeout&lt;=0,则发送SIGKILL信号，强制退出 if err = task.Kill(ctx, syscall.SIGKILL); err != nil &amp;&amp; !errdefs.IsNotFound(err) &#123; return fmt.Errorf("failed to kill container %q: %w", id, err) &#125; // Wait for a fixed timeout until container stop is observed by event monitor. err = c.waitContainerStop(ctx, container) if err != nil &#123; return fmt.Errorf("an error occurs during waiting for container %q to be killed: %w", id, err) &#125; return nil&#125; 容器终止后，kubelet会将pod的信息从缓存中删除，至此整个删除流程就走完了。 小结: kubectl发送请求给apiserver kubelet执行pre-stop hook,调用cri删除容器 kubelet将pod信息从缓存的删除 从上面的内容中我们可以得到如何使pod如何进行优雅退出 设置pre-stop hook 在应用中处理SIGTERM信号 以上两种方法都能实现优雅退出但都不能超过最大退出时间(默认30s)，否则进程将会被强制kill。 REF:1.pkg/cri/sbserver/container_stop.go2.pkg/kubelet/kubelet.go3.pkg/kubelet/kubelet_pods.go]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Iterator]]></title>
    <url>%2F2023%2F04%2F17%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BIterator%2F</url>
    <content type="text"><![CDATA[迭代器模式也叫Iterator迭代器模式是一种行为设计模式， 让你能在不暴露集合底层表现形式 （列表、 栈和树等） 的情况下遍历集合中所有的元素。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114// plugins/ipam/host-local/backend/allocator/allocator.go// 对RangeIter实现Next()方法以实现对RangeSet的遍历type RangeIter struct &#123; rangeset *RangeSet // The current range id // 记录当前访问的索引 rangeIdx int // Our current position // 当前的ip cur net.IP // The IP where we started iterating; if we hit this again, we&apos;re done. // 开始访问的IP,如果再次访问表示已迭代完成 startIP net.IP&#125;func (i *RangeIter) Next() (*net.IPNet, net.IP) &#123; // 获取索引为rangeIdx的元素(Range) r := (*i.rangeset)[i.rangeIdx] // 第一次开始迭代 if i.cur == nil &#123; i.cur = r.RangeStart i.startIP = i.cur if i.cur.Equal(r.Gateway) &#123; return i.Next() &#125; return &amp;net.IPNet&#123;IP: i.cur, Mask: r.Subnet.Mask&#125;, r.Gateway &#125; // If we&apos;ve reached the end of this range, we need to advance the range // RangeEnd is inclusive as well if i.cur.Equal(r.RangeEnd) &#123; i.rangeIdx++ i.rangeIdx %= len(*i.rangeset) r = (*i.rangeset)[i.rangeIdx] i.cur = r.RangeStart &#125; else &#123; i.cur = ip.NextIP(i.cur) &#125; if i.startIP == nil &#123; i.startIP = i.cur &#125; else if i.cur.Equal(i.startIP) &#123; // IF we&apos;ve looped back to where we started, give up return nil, nil &#125; if i.cur.Equal(r.Gateway) &#123; return i.Next() &#125; return &amp;net.IPNet&#123;IP: i.cur, Mask: r.Subnet.Mask&#125;, r.Gateway&#125;func (a *IPAllocator) GetIter() (*RangeIter, error) &#123; iter := RangeIter&#123; rangeset: a.rangeset, &#125; // Round-robin by trying to allocate from the last reserved IP + 1 startFromLastReservedIP := false // We might get a last reserved IP that is wrong if the range indexes changed. // This is not critical, we just lose round-robin this one time. lastReservedIP, err := a.store.LastReservedIP(a.rangeID) if err != nil &amp;&amp; !os.IsNotExist(err) &#123; log.Printf(&quot;Error retrieving last reserved ip: %v&quot;, err) &#125; else if lastReservedIP != nil &#123; startFromLastReservedIP = a.rangeset.Contains(lastReservedIP) &#125; // Find the range in the set with this IP if startFromLastReservedIP &#123; for i, r := range *a.rangeset &#123; if r.Contains(lastReservedIP) &#123; iter.rangeIdx = i // We advance the cursor on every Next(), so the first call // to next() will return lastReservedIP + 1 iter.cur = lastReservedIP break &#125; &#125; &#125; else &#123; iter.rangeIdx = 0 iter.startIP = (*a.rangeset)[0].RangeStart &#125; return &amp;iter, nil&#125;// 调用Iterfunc (a *IPAllocator) Get(id string, ifname string, requestedIP net.IP) (*current.IPConfig, error) &#123; ... iter, err := a.GetIter() for &#123; reservedIP, gw := iter.Next() ... &#125; ...&#125;// plugins/ipam/host-local/backend/allocator/config.gotype RangeSet []Rangetype Range struct &#123; RangeStart net.IP `json:&quot;rangeStart,omitempty&quot;` // The first ip, inclusive RangeEnd net.IP `json:&quot;rangeEnd,omitempty&quot;` // The last ip, inclusive Subnet types.IPNet `json:&quot;subnet&quot;` Gateway net.IP `json:&quot;gateway,omitempty&quot;`&#125; REF:1.plugins/ipam/host-local/backend/allocator/config.go2.plugins/ipam/host-local/backend/allocator/allocator.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cni-plugins之ipam-host-local]]></title>
    <url>%2F2023%2F04%2F16%2Fcni-plugins%E4%B9%8Bipam-host-local%2F</url>
    <content type="text"><![CDATA[host-local是一种IP地址管理(IPAM)插件。它从一组地址范围中分配IP地址,也可以从主机上的resolv.conf文件中获取DNS配置。它在主机文件系统上存储本地状态，因此可以确保单个主机上IP地址的唯一性。该分配器可以分配多个地址范围，并支持多个(不相交的)子网集。分配策略是在每个范围集中松散的轮询。在 CNI 调用链中，每个插件都会返回一个 JSON 格式的结果，其中会包含网络配置信息，如 IP 地址、子网掩码、网关等。当一个插件执行完毕，将其返回的结果传递给下一个插件，下一个插件将使用上一个插件返回的网络配置信息来配置网络。当最后一个插件执行完毕后，kubelet 会检查所有插件返回的结果，只有当所有插件都成功设置了 IP 地址，才会认为整个 CNI 调用链执行成功。如果其中一个插件失败，后续的插件将不会被执行，同时整个 CNI 调用链也会被标记为失败。 配置示例(多个地址范围)ranges表示一个列表，元素是rangeSet,下面的ranges包含两个rangeSet。ranges的长度表示返回多少ip。rangeSet表示ip地址的可选范围。12345678910111213141516171819202122232425262728293031&#123; "ipam": &#123; "type": "host-local", "ranges": [ [ &#123; "subnet": "10.10.0.0/16", "rangeStart": "10.10.1.20", "rangeEnd": "10.10.3.50", "gateway": "10.10.0.254" &#125;, &#123; "subnet": "172.16.5.0/24" &#125; ], [ &#123; "subnet": "3ffe:ffff:0:01ff::/64", "rangeStart": "3ffe:ffff:0:01ff::0010", "rangeEnd": "3ffe:ffff:0:01ff::0020" &#125; ] ], "routes": [ &#123; "dst": "0.0.0.0/0" &#125;, &#123; "dst": "192.168.0.0/16", "gw": "10.10.5.1" &#125;, &#123; "dst": "3ffe:ffff:0:01ff::1/64" &#125; ], "dataDir": "/run/my-orchestrator/container-ipam-state" &#125;&#125; ip地址分配对于每一个请求的自定义 IP，host-local 分配器会在它所管理的地址范围中进行请求。因此可以指定多个自定义 IP 和多个地址范围。如果一个IP在使用中或者不在范围内将会分配失败。 文件存储host-local将已分配的 IP 地址作为文件存储在 /var/lib/cni/networks/$NETWORK_NAME 目录下。 源码分析host-local入口1234567891011121314// PluginMain 是plugin的main函数，自动进行了错误处理func main() &#123; skel.PluginMain(cmdAdd, cmdCheck, cmdDel, version.All, bv.BuildString("host-local"))&#125;// vendor/github.com/containernetworking/cni/pkg/skel/skel.gofunc PluginMain(cmdAdd, cmdCheck, cmdDel func(_ *CmdArgs) error, versionInfo version.PluginInfo, about string) &#123; if e := PluginMainWithError(cmdAdd, cmdCheck, cmdDel, versionInfo, about); e != nil &#123; if err := e.Print(); err != nil &#123; log.Print("Error writing error JSON to stdout: ", err) &#125; os.Exit(1) &#125;&#125; 三个方法cmdCheck,cmdAdd, cmdDel。cmdCheck12345678910111213141516171819202122232425// 根据提供的ContainerID和IfName，判断ip是否存在。如不存在则返回一个错误。func cmdCheck(args *skel.CmdArgs) error &#123; // 通过传过来的参数(也就是第二步中的json)构建一个IPAMConfig ipamConf, _, err := allocator.LoadIPAMConfig(args.StdinData, args.Args) if err != nil &#123; return err &#125; // Look to see if there is at least one IP address allocated to the container // in the data dir, irrespective of what that address actually is // 创建一个store对象，包含一个FileLock和目录.用于文件的操作 store, err := disk.New(ipamConf.Name, ipamConf.DataDir) if err != nil &#123; return err &#125; defer store.Close() // 通过容器ID和网络接口的名称(如eth0) containerIPFound := store.FindByID(args.ContainerID, args.IfName) if !containerIPFound &#123; return fmt.Errorf("host-local: Failed to find address added by container %v", args.ContainerID) &#125; return nil&#125; cmdAdd12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// 从可选分配一个ipfunc cmdAdd(args *skel.CmdArgs) error &#123; // 通过传过来的参数(也就是第二步中的json)构建一个IPAMConfig ipamConf, confVersion, err := allocator.LoadIPAMConfig(args.StdinData, args.Args) if err != nil &#123; return err &#125; result := &amp;current.Result&#123;CNIVersion: current.ImplementedSpecVersion&#125; if ipamConf.ResolvConf != "" &#123; dns, err := parseResolvConf(ipamConf.ResolvConf) if err != nil &#123; return err &#125; result.DNS = *dns &#125; // 创建一个store对象，包含一个FileLock和目录.用于文件的操作 store, err := disk.New(ipamConf.Name, ipamConf.DataDir) if err != nil &#123; return err &#125; defer store.Close() // Keep the allocators we used, so we can release all IPs if an error // occurs after we start allocating // 保存分配的ip,如果在分配过程中出现错误可以进行释放 allocs := []*allocator.IPAllocator&#123;&#125; // Store all requested IPs in a map, so we can easily remove ones we use // and error if some remain requestedIPs := map[string]net.IP&#123;&#125; // net.IP cannot be a key // ipamConf.IPArgs为请求的ip列表 for _, ip := range ipamConf.IPArgs &#123; requestedIPs[ip.String()] = ip &#125; // 遍历Ranges for idx, rangeset := range ipamConf.Ranges &#123; // 返回IPAllocator指针 // IPAllocator实现了Get方法获取一个IP // Release方法释放为容器分配的IP allocator := allocator.NewIPAllocator(&amp;rangeset, store, idx) // Check to see if there are any custom IPs requested in this range. var requestedIP net.IP for k, ip := range requestedIPs &#123; if rangeset.Contains(ip) &#123; requestedIP = ip // 在一个rangeset找到了ip,则删除requestedIPs中的记录 delete(requestedIPs, k) break &#125; &#125; // 根据容器ID,网络接口的名称和IP返回IPConfig ipConf, err := allocator.Get(args.ContainerID, args.IfName, requestedIP) if err != nil &#123; // Deallocate all already allocated IPs // 如果获取IP出错，释放对应的IP for _, alloc := range allocs &#123; _ = alloc.Release(args.ContainerID, args.IfName) &#125; return fmt.Errorf("failed to allocate for range %d: %v", idx, err) &#125; allocs = append(allocs, allocator) result.IPs = append(result.IPs, ipConf) &#125; // If an IP was requested that wasn't fulfilled, fail // 如果requestedIPs长度不为0,表示有些IP不符合要求 // 进行回退操作，释放已分配IP if len(requestedIPs) != 0 &#123; for _, alloc := range allocs &#123; _ = alloc.Release(args.ContainerID, args.IfName) &#125; errstr := "failed to allocate all requested IPs:" for _, ip := range requestedIPs &#123; errstr = errstr + " " + ip.String() &#125; return fmt.Errorf(errstr) &#125; result.Routes = ipamConf.Routes return types.PrintResult(result, confVersion)&#125; cmdDel123456789101112131415161718192021222324252627282930// 释放ip// 通过对cmdAdd的分析，这里应该比较清晰了func cmdDel(args *skel.CmdArgs) error &#123; ipamConf, _, err := allocator.LoadIPAMConfig(args.StdinData, args.Args) if err != nil &#123; return err &#125; store, err := disk.New(ipamConf.Name, ipamConf.DataDir) if err != nil &#123; return err &#125; defer store.Close() // Loop through all ranges, releasing all IPs, even if an error occurs var errors []string for idx, rangeset := range ipamConf.Ranges &#123; ipAllocator := allocator.NewIPAllocator(&amp;rangeset, store, idx) err := ipAllocator.Release(args.ContainerID, args.IfName) if err != nil &#123; errors = append(errors, err.Error()) &#125; &#125; if errors != nil &#123; return fmt.Errorf(strings.Join(errors, ";")) &#125; return nil&#125; 对应的一些结构体1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// plugins/ipam/host-local/backend/allocator/allocator.gotype IPAllocator struct &#123; rangeset *RangeSet store backend.Store rangeID string // Used for tracking last reserved ip&#125;// plugins/ipam/host-local/backend/disk/backend.gotype Store struct &#123; *FileLock dataDir string&#125;// plugins/ipam/host-local/backend/allocator/config.go// The top-level network config - IPAM plugins are passed the full configuration// of the calling plugin, not just the IPAM section.type Net struct &#123; Name string `json:"name"` CNIVersion string `json:"cniVersion"` IPAM *IPAMConfig `json:"ipam"` RuntimeConfig struct &#123; // The capability arg IPRanges []RangeSet `json:"ipRanges,omitempty"` IPs []*ip.IP `json:"ips,omitempty"` &#125; `json:"runtimeConfig,omitempty"` Args *struct &#123; A *IPAMArgs `json:"cni"` &#125; `json:"args"`&#125;// IPAMConfig represents the IP related network configuration.// This nests Range because we initially only supported a single// range directly, and wish to preserve backwards compatibilitytype IPAMConfig struct &#123; *Range Name string Type string `json:"type"` Routes []*types.Route `json:"routes"` DataDir string `json:"dataDir"` ResolvConf string `json:"resolvConf"` Ranges []RangeSet `json:"ranges"` IPArgs []net.IP `json:"-"` // Requested IPs from CNI_ARGS, args and capabilities&#125;type IPAMEnvArgs struct &#123; types.CommonArgs IP ip.IP `json:"ip,omitempty"`&#125;type IPAMArgs struct &#123; IPs []*ip.IP `json:"ips"`&#125;type RangeSet []Rangetype Range struct &#123; RangeStart net.IP `json:"rangeStart,omitempty"` // The first ip, inclusive RangeEnd net.IP `json:"rangeEnd,omitempty"` // The last ip, inclusive Subnet types.IPNet `json:"subnet"` Gateway net.IP `json:"gateway,omitempty"`&#125; REF:1.https://www.cni.dev/plugins/current/ipam/host-local/2.plugins/ipam/host-local/main.go3.plugins/ipam/host-local/backend/allocator/allocator.go4.plugins/ipam/host-local/backend/disk/backend.go5. plugins/ipam/host-local/backend/allocator/config.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>cni</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s设计模式之Command]]></title>
    <url>%2F2023%2F04%2F16%2Fk8s%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BCommand%2F</url>
    <content type="text"><![CDATA[命令模式也叫动作,事务,Action,Transaction,Command。 命令模式是一种行为设计模式， 它可将请求转换为一个包含与请求相关的所有信息的独立对象。 该转换让你能根据不同的请求将方法参数化、 延迟请求执行或将其放入队列中， 且能实现可撤销操作。 12345678910111213141516171819202122232425262728293031323334353637383940// pkg/kubelet/config/config.go// 处理Pod配置的相关操作type PodConfig struct &#123; // 存储当前kubelet所管理的所有的Pod信息 pods *podStorage // 用于合并多个Pod配置来源的更新, mux *config.Mux // the channel of denormalized changes passed to listeners // 通常被用于将不同来源的更新合并并发送给订阅者。订阅者可以通过该通道获取最新的 Pod 配置信息 updates chan kubetypes.PodUpdate // contains the list of all configured sources sourcesLock sync.Mutex // Pod配置来源，i.e.(file, api) sources sets.String&#125;// podStorage管理当前Pod状态,确保更新信息在updates通道是严格有序的传递type podStorage struct &#123; podLock sync.RWMutex // map of source name to pod uid to pod reference pods map[string]map[types.UID]*v1.Pod mode PodConfigNotificationMode // ensures that updates are delivered in strict order // on the updates channel updateLock sync.Mutex // 表示的是Pod状态的更新 updates chan&lt;- kubetypes.PodUpdate // contains the set of all sources that have sent at least one SET sourcesSeenLock sync.RWMutex sourcesSeen sets.String // the EventRecorder to use recorder record.EventRecorder startupSLIObserver podStartupSLIObserver&#125; PodConfig 结构体使用了命令模式的思想，将多个来源的 Pod 配置信息抽象为一个接口，通过 mux 对象实现了命令对象的统一管理和执行，同时通过 updates 通道将执行结果传递给其他组件。这样做的好处是可以很方便地扩展 Pod 配置来源的种类，也可以保证不同来源的更新按顺序执行，避免了竞态问题。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114// pkg/kubelet/kubelet.gofunc startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123; // start the kubelet go k.Run(podCfg.Updates()) // start the kubelet server if enableServer &#123; go k.ListenAndServe(kubeCfg, kubeDeps.TLSOptions, kubeDeps.Auth, kubeDeps.TracerProvider) &#125; if kubeCfg.ReadOnlyPort &gt; 0 &#123; go k.ListenAndServeReadOnly(netutils.ParseIPSloppy(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) &#125; if utilfeature.DefaultFeatureGate.Enabled(features.KubeletPodResources) &#123; go k.ListenAndServePodResources() &#125;&#125;// Run starts the kubelet reacting to config updatesfunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; ctx := context.Background() if kl.logServer == nil &#123; file := http.FileServer(http.Dir(nodeLogDir)) if utilfeature.DefaultFeatureGate.Enabled(features.NodeLogQuery) &amp;&amp; kl.kubeletConfiguration.EnableSystemLogQuery &#123; kl.logServer = http.StripPrefix("/logs/", http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) &#123; if nlq, errs := newNodeLogQuery(req.URL.Query()); len(errs) &gt; 0 &#123; http.Error(w, errs.ToAggregate().Error(), http.StatusBadRequest) return &#125; else if nlq != nil &#123; if req.URL.Path != "/" &amp;&amp; req.URL.Path != "" &#123; http.Error(w, "path not allowed in query mode", http.StatusNotAcceptable) return &#125; if errs := nlq.validate(); len(errs) &gt; 0 &#123; http.Error(w, errs.ToAggregate().Error(), http.StatusNotAcceptable) return &#125; // Validation ensures that the request does not query services and files at the same time if len(nlq.Services) &gt; 0 &#123; journal.ServeHTTP(w, req) return &#125; // Validation ensures that the request does not explicitly query multiple files at the same time if len(nlq.Files) == 1 &#123; // Account for the \ being used on Windows clients req.URL.Path = filepath.ToSlash(nlq.Files[0]) &#125; &#125; // Fall back in case the caller is directly trying to query a file // Example: kubectl get --raw /api/v1/nodes/$name/proxy/logs/foo.log file.ServeHTTP(w, req) &#125;)) &#125; else &#123; kl.logServer = http.StripPrefix("/logs/", file) &#125; &#125; if kl.kubeClient == nil &#123; klog.InfoS("No API server defined - no node status update will be sent") &#125; // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil &#123; go kl.cloudResourceSyncManager.Run(wait.NeverStop) &#125; if err := kl.initializeModules(); err != nil &#123; kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.ErrorS(err, "Failed to initialize internal modules") os.Exit(1) &#125; // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil &#123; // Start two go-routines to update the status. // // The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals, // while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver // once the node becomes ready, then exits afterwards. // // Introduce some small jittering to ensure that over time the requests won't start // accumulating at approximately the same time from the set of nodes due to priority and // fairness effect. go wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, 0.04, true, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease go kl.nodeLeaseController.Run(context.Background()) &#125; go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Set up iptables util rules if kl.makeIPTablesUtilChains &#123; kl.initNetworkUtil() &#125; // Start component sync loops. kl.statusManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil &#123; kl.runtimeClassManager.Start(wait.NeverStop) &#125; // Start the pod lifecycle event generator. kl.pleg.Start() // Start eventedPLEG only if EventedPLEG feature gate is enabled. if utilfeature.DefaultFeatureGate.Enabled(features.EventedPLEG) &#123; kl.eventedPleg.Start() &#125; // syncLoop 是一个处理变化的主循环,监听三个通道(file, apiserver, http) kl.syncLoop(ctx, updates, kl)&#125; REF:1.https://refactoringguru.cn/design-patterns/command2.pkg/kubelet/config/config.go3.pkg/kubelet/kubelet.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo添加评论系统giscus]]></title>
    <url>%2F2023%2F04%2F15%2Fhexo%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9Fgiscus%2F</url>
    <content type="text"><![CDATA[如何给hexo搭建的博客添加评论系统,经过对比之后决定使用giscus。giscus有如下优点，参考官网。 本人使用的hexo的一些信息,使用的主题为next,版本为7.1.2。查看文件themes/next/package.json获取版本信息。12345678910111213141516171819➜ hexo versionhexo: 3.9.0hexo-cli: 4.2.0os: Linux 5.15.0-67-generic linux x64http_parser: 2.9.3node: 10.19.0v8: 6.8.275.32-node.55uv: 1.34.2zlib: 1.2.11brotli: 1.0.7ares: 1.15.0modules: 64nghttp2: 1.40.0napi: 5openssl: 1.1.1dicu: 66.1unicode: 13.0cldr: 36.1tz: 2022g 本来是参照https://github.com/next-theme/hexo-next-giscus进行操作的，但是根据上面的方法修改_config.yml部署后并没有生效，所以才有了下面的(5,6,7)步骤。如果修改_config.yml的方式生效则不必执行下面的5,6,7。 1.新建一个github仓库,此仓库必须是公开的。2.安装giscus app,否则访客将无法评论和回应。3.在新建的仓库中启用Discussions功能4.安装hexo-next-giscusnpm install hexo-next-giscus --save5.在themes/next/layout/_macro/目录下,新建文件discus.swig。内容如下123456789101112131415&lt;script src=&quot;https://giscus.app/client.js&quot; data-repo=&quot;[在此输入仓库]&quot; data-repo-id=&quot;[在此输入仓库 ID]&quot; data-category=&quot;[在此输入分类名]&quot; data-category-id=&quot;[在此输入分类 ID]&quot; data-mapping=&quot;pathname&quot; data-strict=&quot;0&quot; data-reactions-enabled=&quot;1&quot; data-emit-metadata=&quot;0&quot; data-input-position=&quot;bottom&quot; data-theme=&quot;preferred_color_scheme&quot; data-lang=&quot;zh-CN&quot; crossorigin=&quot;anonymous&quot; async&gt;&lt;/script&gt; 如果获取上面的data-repo,data-repo-id,data-category,data-category-id`呢? 打开giscus.app,==&gt; 配置 ==&gt; 仓库,输入第一步新建的仓库名称,选择对应的分类，就会自动生成&lt;scrip&gt;...&lt;/script&gt;中的内容，直接复制就可以。6.编辑themes/next/layout/post.swig,带+号的行为新增的。 123456789101112131415161718192021222324252627&#123;% extends &apos;_layout.swig&apos; %&#125;&#123;% import &apos;_macro/post.swig&apos; as post_template %&#125;&#123;% import &apos;_macro/sidebar.swig&apos; as sidebar_template %&#125;+ &#123;% import &apos;_macro/discus.swig&apos; as discus_template %&#125;&#123;% block title %&#125;&#123;&#123; page.title &#125;&#125; | &#123;&#123; title &#125;&#125;&#123;% endblock %&#125;&#123;% block page_class %&#125;page-post-detail&#123;% endblock %&#125;&#123;% block content %&#125; &lt;div id=&quot;posts&quot; class=&quot;posts-expand&quot;&gt; &#123;&#123; post_template.render(page) &#125;&#125; &lt;/div&gt;+ &lt;div&gt;+ &#123;&#123; discus_template.render(true)&#125;&#125;+ &lt;/div&gt;&#123;% endblock %&#125;&#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(true) &#125;&#125;&#123;% endblock %&#125;&#123;% block script_extra %&#125; &#123;% include &apos;_scripts/pages/post-details.swig&apos; %&#125;&#123;% endblock %&#125; 7.重新部署 123hexo cleanhexo generatehexo deploy REF:1.https://github.com/next-theme/hexo-next-giscus2.https://giscus.app/zh-CN]]></content>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之State]]></title>
    <url>%2F2023%2F04%2F13%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BState%2F</url>
    <content type="text"><![CDATA[State也就是状态模式是一种行为设计模式， 让你能在一个对象的内部状态变化时改变其行为， 使其看上去就像改变了自身所属的类一样。 在Kubernetes 中，DeploymentController 作为控制器之一，主要负责维护 Deployment 对象的状态，即根据用户定义的 Deployment 规范，创建、更新、删除 ReplicaSet 和 Pod 等资源，确保应用的正确运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// pkg/apis/apps/types.go// DeploymentConditionType有三种类型,Available,Progressing,ReplicaFailure.const ( // Available means the deployment is available, ie. at least the minimum available // replicas required are up and running for at least minReadySeconds. DeploymentAvailable DeploymentConditionType = "Available" // Progressing means the deployment is progressing. Progress for a deployment is // considered when a new replica set is created or adopted, and when new pods scale // up or old pods scale down. Progress is not estimated for paused deployments or // when progressDeadlineSeconds is not specified. DeploymentProgressing DeploymentConditionType = "Progressing" // ReplicaFailure is added in a deployment when one of its pods fails to be created // or deleted. DeploymentReplicaFailure DeploymentConditionType = "ReplicaFailure")// DeploymentCondition 记录了Deployment的状态// DeploymentCondition describes the state of a deployment at a certain point.type DeploymentCondition struct &#123; // Type of deployment condition. Type DeploymentConditionType // Status of the condition, one of True, False, Unknown. Status api.ConditionStatus // The last time this condition was updated. LastUpdateTime metav1.Time // Last time the condition transitioned from one status to another. LastTransitionTime metav1.Time // The reason for the condition's last transition. Reason string // A human readable message indicating details about the transition. Message string&#125;// DeploymentStatus holds information about the observed status of a deployment.type DeploymentStatus struct &#123; // The generation observed by the deployment controller. // +optional ObservedGeneration int64 // Total number of non-terminated pods targeted by this deployment (their labels match the selector). // +optional Replicas int32 // Total number of non-terminated pods targeted by this deployment that have the desired template spec. // +optional UpdatedReplicas int32 // Total number of ready pods targeted by this deployment. // +optional ReadyReplicas int32 // Total number of available pods (ready for at least minReadySeconds) targeted by this deployment. // +optional AvailableReplicas int32 // Total number of unavailable pods targeted by this deployment. This is the total number of // pods that are still required for the deployment to have 100% available capacity. They may // either be pods that are running but not yet available or pods that still have not been created. // +optional UnavailableReplicas int32 // Represents the latest available observations of a deployment's current state. Conditions []DeploymentCondition // Count of hash collisions for the Deployment. The Deployment controller uses this // field as a collision avoidance mechanism when it needs to create the name for the // newest ReplicaSet. // +optional CollisionCount *int32&#125; 下面这段代码是 DeploymentController 在检测 Deployment 的部署状态时，使用状态模式进行状态转换的一个例子。该控制器使用 DeploymentCondition 对象来跟踪 Deployment 的状态。在这里，根据 Deployment 是否已经完成（DeploymentComplete）、是否在进行中（DeploymentProgressing）以及是否已经超时（DeploymentTimedOut）等三种状态，来执行不同的操作。 首先，如果 Deployment 已经完成，则会创建一个新的 DeploymentCondition 对象，将其状态设置为 True，原因设置为 NewReplicaSetAvailable，具体是哪个取决于新创建的 ReplicaSet 是否存在。然后，使用 SetDeploymentCondition 方法将其设置为Deployment 的新状态。 其次，如果 Deployment 正在进行中，则会创建一个的 DeploymentCondition 对象。如果currentCond不为nil且该对象的状态为 True，则将其上次转换时间 condition.LastTransitionTime设置为currentCond.LastTransitionTime。然后使用 SetDeploymentCondition 方法更新。 最后，如果 Deployment 已经超时，则会创建一个新的 DeploymentCondition 对象，将其状态设置为 False，原因设置为 TimedOutReason，并使用 SetDeploymentCondition 方法将其设置为 Deployment 的新状态。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384// pkg/controller/deployment/progress.gofunc (dc *DeploymentController) syncRolloutStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error &#123; newStatus := calculateStatus(allRSs, newRS, d) // If there is no progressDeadlineSeconds set, remove any Progressing condition. if !util.HasProgressDeadline(d) &#123; util.RemoveDeploymentCondition(&amp;newStatus, apps.DeploymentProgressing) &#125; // If there is only one replica set that is active then that means we are not running // a new rollout and this is a resync where we don't need to estimate any progress. // In such a case, we should simply not estimate any progress for this deployment. currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) isCompleteDeployment := newStatus.Replicas == newStatus.UpdatedReplicas &amp;&amp; currentCond != nil &amp;&amp; currentCond.Reason == util.NewRSAvailableReason // Check for progress only if there is a progress deadline set and the latest rollout // hasn't completed yet. if util.HasProgressDeadline(d) &amp;&amp; !isCompleteDeployment &#123; switch &#123; case util.DeploymentComplete(d, &amp;newStatus): // Update the deployment conditions with a message for the new replica set that // was successfully deployed. If the condition already exists, we ignore this update. msg := fmt.Sprintf("Deployment %q has successfully progressed.", d.Name) if newRS != nil &#123; msg = fmt.Sprintf("ReplicaSet %q has successfully progressed.", newRS.Name) &#125; condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg) util.SetDeploymentCondition(&amp;newStatus, *condition) case util.DeploymentProgressing(d, &amp;newStatus): // If there is any progress made, continue by not checking if the deployment failed. This // behavior emulates the rolling updater progressDeadline check. msg := fmt.Sprintf("Deployment %q is progressing.", d.Name) if newRS != nil &#123; msg = fmt.Sprintf("ReplicaSet %q is progressing.", newRS.Name) &#125; condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.ReplicaSetUpdatedReason, msg) // Update the current Progressing condition or add a new one if it doesn't exist. // If a Progressing condition with status=true already exists, we should update // everything but lastTransitionTime. SetDeploymentCondition already does that but // it also is not updating conditions when the reason of the new condition is the // same as the old. The Progressing condition is a special case because we want to // update with the same reason and change just lastUpdateTime iff we notice any // progress. That's why we handle it here. if currentCond != nil &#123; if currentCond.Status == v1.ConditionTrue &#123; condition.LastTransitionTime = currentCond.LastTransitionTime &#125; util.RemoveDeploymentCondition(&amp;newStatus, apps.DeploymentProgressing) &#125; util.SetDeploymentCondition(&amp;newStatus, *condition) case util.DeploymentTimedOut(ctx, d, &amp;newStatus): // Update the deployment with a timeout condition. If the condition already exists, // we ignore this update. msg := fmt.Sprintf("Deployment %q has timed out progressing.", d.Name) if newRS != nil &#123; msg = fmt.Sprintf("ReplicaSet %q has timed out progressing.", newRS.Name) &#125; condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, util.TimedOutReason, msg) util.SetDeploymentCondition(&amp;newStatus, *condition) &#125; &#125; // Move failure conditions of all replica sets in deployment conditions. For now, // only one failure condition is returned from getReplicaFailures. if replicaFailureCond := dc.getReplicaFailures(allRSs, newRS); len(replicaFailureCond) &gt; 0 &#123; // There will be only one ReplicaFailure condition on the replica set. util.SetDeploymentCondition(&amp;newStatus, replicaFailureCond[0]) &#125; else &#123; util.RemoveDeploymentCondition(&amp;newStatus, apps.DeploymentReplicaFailure) &#125; // Do not update if there is nothing new to add. if reflect.DeepEqual(d.Status, newStatus) &#123; // Requeue the deployment if required. dc.requeueStuckDeployment(ctx, d, newStatus) return nil &#125; newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions&#123;&#125;) return err&#125; 这种方式可以将状态转换的逻辑从主控制器中分离出来，这样做可以使代码更加清晰、易于维护。 REF:1.pkg/controller/deployment/progress.go2.hpkg/apis/apps/types.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Proxy]]></title>
    <url>%2F2023%2F04%2F11%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BProxy%2F</url>
    <content type="text"><![CDATA[Proxy也就是代理模式 代理模式是一种结构型设计模式， 让你能够提供对象的替代品或其占位符。 代理控制着对于原对象的访问， 并允许在将请求提交给对象前后进行一些处理。 在upgradeawarehandler.go文件中还定义了一个UpgradeAwareHandler结构体，它可以将HTTP请求转换为WebSocket请求，并将转换后的WebSocket请求传递给目标组件，以支持使用WebSocket协议与目标组件进行通信。 123456789101112131415161718192021222324252627282930313233343536373839404142// vendor/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go// UpgradeAwareHandler实现了ServeHTTP方法type UpgradeAwareHandler struct &#123; // UpgradeRequired will reject non-upgrade connections if true. UpgradeRequired bool // Location is the location of the upstream proxy. It is used as the location to Dial on the upstream server // for upgrade requests unless UseRequestLocationOnUpgrade is true. Location *url.URL // AppendLocationPath determines if the original path of the Location should be appended to the upstream proxy request path AppendLocationPath bool // Transport provides an optional round tripper to use to proxy. If nil, the default proxy transport is used Transport http.RoundTripper // UpgradeTransport, if specified, will be used as the backend transport when upgrade requests are provided. // This allows clients to disable HTTP/2. UpgradeTransport UpgradeRequestRoundTripper // WrapTransport indicates whether the provided Transport should be wrapped with default proxy transport behavior (URL rewriting, X-Forwarded-* header setting) WrapTransport bool // UseRequestLocation will use the incoming request URL when talking to the backend server. UseRequestLocation bool // UseLocationHost overrides the HTTP host header in requests to the backend server to use the Host from Location. // This will override the req.Host field of a request, while UseRequestLocation will override the req.URL field // of a request. The req.URL.Host specifies the server to connect to, while the req.Host field // specifies the Host header value to send in the HTTP request. If this is false, the incoming req.Host header will // just be forwarded to the backend server. UseLocationHost bool // FlushInterval controls how often the standard HTTP proxy will flush content from the upstream. FlushInterval time.Duration // MaxBytesPerSec controls the maximum rate for an upstream connection. No rate is imposed if the value is zero. MaxBytesPerSec int64 // Responder is passed errors that occur while setting up proxying. Responder ErrorResponder // Reject to forward redirect response RejectForwardingRedirects bool&#125;// proxyStream proxies stream to url.// pkg/kubelet/server/server.gofunc proxyStream(w http.ResponseWriter, r *http.Request, url *url.URL) &#123; // TODO(random-liu): Set MaxBytesPerSec to throttle the stream. handler := proxy.NewUpgradeAwareHandler(url, nil /*transport*/, false /*wrapTransport*/, true /*upgradeRequired*/, &amp;responder&#123;&#125;) handler.ServeHTTP(w, r)&#125; REF:1.staging/src/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go2.pkg/kubelet/server/server.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Flyweight]]></title>
    <url>%2F2023%2F04%2F11%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BFlyweight%2F</url>
    <content type="text"><![CDATA[Flyweight也叫缓存，Cache,享元模式。 享元模式是一种结构型设计模式， 它摒弃了在每个对象中保存所有数据的方式， 通过共享多个对象所共有的相同状态， 让你能在有限的内存容量中载入更多对象。 在 Kubernetes 源码中，有多个地方使用了享元模式（Flyweight Pattern），其中最常见的就是 client-go 中的 cache 包。该包实现了 Kubernetes 中的各种资源对象的本地缓存，并使用享元模式来尽可能地重用缓存对象，以降低内存消耗和提高效率。 在Informers Factory中，每个类型的资源（如Pods，Services，Deployments等）只需要创建一个Informer即可，因为每个Informer可以观察到同一类型的所有资源对象。因此，在创建Informer之前，需要首先判断该类型的Informer是否已经存在。如果存在，则直接返回已经存在的Informer；否则，创建新的Informer并存储到f.informers中供以后使用。 这种设计方式减少了Informer的创建次数，提高了Informer的重用率，减少了系统开销，同时也降低了代码的复杂度。这正是享元模式的优点所在。 k8s中informer创建k8s中的informer创建使用了享元模式。12345678910111213141516171819202122// staging/src/k8s.io/client-go/informers/factory.gofunc (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer &#123; f.lock.Lock() defer f.lock.Unlock() informerType := reflect.TypeOf(obj) // 判断是否已经存在 informer, exists := f.informers[informerType] if exists &#123; return informer &#125; resyncPeriod, exists := f.customResync[informerType] if !exists &#123; resyncPeriod = f.defaultResync &#125; informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer&#125; REF:1.client-go/informers/factory.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Composite]]></title>
    <url>%2F2023%2F04%2F11%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BComposite%2F</url>
    <content type="text"><![CDATA[组合模式是一种结构型设计模式， 你可以使用它将对象组合成树状结构， 并且能像使用独立对象一样使用它们。在Golang中可以使用多个interface组合在一起实现组合的功能。 12345678910111213141516171819// vendor/k8s.io/client-go/kubernetes/typed/core/v1/pod.go// 下面是k8s中的一个示例type PodInterface interface &#123; Create(ctx context.Context, pod *v1.Pod, opts metav1.CreateOptions) (*v1.Pod, error) Update(ctx context.Context, pod *v1.Pod, opts metav1.UpdateOptions) (*v1.Pod, error) UpdateStatus(ctx context.Context, pod *v1.Pod, opts metav1.UpdateOptions) (*v1.Pod, error) Delete(ctx context.Context, name string, opts metav1.DeleteOptions) error DeleteCollection(ctx context.Context, opts metav1.DeleteOptions, listOpts metav1.ListOptions) error Get(ctx context.Context, name string, opts metav1.GetOptions) (*v1.Pod, error) List(ctx context.Context, opts metav1.ListOptions) (*v1.PodList, error) Watch(ctx context.Context, opts metav1.ListOptions) (watch.Interface, error) Patch(ctx context.Context, name string, pt types.PatchType, data []byte, opts metav1.PatchOptions, subresources ...string) (result *v1.Pod, err error) Apply(ctx context.Context, pod *corev1.PodApplyConfiguration, opts metav1.ApplyOptions) (result *v1.Pod, err error) ApplyStatus(ctx context.Context, pod *corev1.PodApplyConfiguration, opts metav1.ApplyOptions) (result *v1.Pod, err error) UpdateEphemeralContainers(ctx context.Context, podName string, pod *v1.Pod, opts metav1.UpdateOptions) (*v1.Pod, error) PodExpansion&#125; REF:1.taging/src/k8s.io/client-go/kubernetes/typed/core/v1/pod.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Clone]]></title>
    <url>%2F2023%2F04%2F10%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BClone%2F</url>
    <content type="text"><![CDATA[原型模式是一种创建型设计模式， 使你能够复制已有对象， 而又无需使代码依赖它们所属的类。 k8s使用代码生成的方式为每个资源对象实现了DeepCopy()方法。 1234567891011121314151617// pkg/apis/core/types.go// Pod的定义// Pod is a collection of containers, used as either input (create, update) or as output (list, get).type Pod struct &#123; metav1.TypeMeta // +optional metav1.ObjectMeta // Spec defines the behavior of a pod. // +optional Spec PodSpec // Status represents the current information about a pod. This data may not be up // to date. // +optional Status PodStatus&#125; 123456789101112131415161718// newPod := pod.DeepCopy()可以调用DeepCopy()复制一个对象// pkg/apis/core/zz_generated.deepcopy.gofunc (in *Pod) DeepCopy() *Pod &#123; if in == nil &#123; return nil &#125; out := new(Pod) in.DeepCopyInto(out) return out&#125;func (in *Pod) DeepCopyInto(out *Pod) &#123; *out = *in out.TypeMeta = in.TypeMeta in.ObjectMeta.DeepCopyInto(&amp;out.ObjectMeta) in.Spec.DeepCopyInto(&amp;out.Spec) in.Status.DeepCopyInto(&amp;out.Status) return&#125; REF:1.pkg/apis/core/types.go2.pkg/apis/core/zz_generated.deepcopy.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Template Method]]></title>
    <url>%2F2023%2F04%2F10%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BTemplate-Method%2F</url>
    <content type="text"><![CDATA[模板方法是一种行为设计模式，用于定义一个操作的算法骨架，而将一些步骤的实现推迟到子类中。它可以使子类在不改变算法结构的情况下重定义算法中的某些步骤。 k8s源码中client-go中模版方法 123456789101112131415161718192021222324252627282930313233343536// staging/src/k8s.io/client-go/rest/client.gotype Interface interface &#123; GetRateLimiter() flowcontrol.RateLimiter Verb(verb string) *Request Post() *Request Put() *Request Patch(pt types.PatchType) *Request Get() *Request Delete() *Request APIVersion() schema.GroupVersion&#125;// RESTClient实现了Interfacetype RESTClient struct &#123; // base is the root URL for all invocations of the client base *url.URL // versionedAPIPath is a path segment connecting the base URL to the resource root versionedAPIPath string // content describes how a RESTClient encodes and decodes responses. content ClientContentConfig // creates BackoffManager that is passed to requests. createBackoffMgr func() BackoffManager // rateLimiter is shared among all requests created by this client unless specifically // overridden. rateLimiter flowcontrol.RateLimiter // warningHandler is shared among all requests created by this client. // If not set, defaultWarningHandler is used. warningHandler WarningHandler // Set specific behavior of the client. If not set http.DefaultClient will be used. Client *http.Client&#125; 1234567891011// staging/src/k8s.io/client-go/dynamic/simple.gotype DynamicClient struct &#123; client rest.Interface&#125;// dynamicResourceClient对需要重写的方法进行了重写type dynamicResourceClient struct &#123; client *DynamicClient namespace string resource schema.GroupVersionResource&#125; REF:1.staging/src/k8s.io/client-go/rest/client.go2.staging/src/k8s.io/client-go/dynamic/simple.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Observer]]></title>
    <url>%2F2023%2F04%2F10%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BObserver%2F</url>
    <content type="text"><![CDATA[观察者模式,也叫事件订阅,监听者,Listener,Observer。观察者模式是一种行为设计模式， 允许你定义一种订阅机制， 可在对象事件发生时通知多个 “观察” 该对象的其他对象。 k8s中的观察者模式Kubernetes中观察者模式主要是通过watch机制实现的。watch机制是一种持续性的HTTP请求(通过块传输)，当资源的状态发生变化时，服务器会返回该资源的新状态。客户端通过不断发起HTTP请求获取新状态，从而实现对资源的观察。watch机制是基于长轮询机制实现的，即服务器在有新状态时才会立即返回结果，否则会一直等待一段时间再返回结果。 在Kubernetes中，kube-apiserver负责处理客户端的watch请求。当客户端订阅一个资源时，kube-apiserver会在内部创建一个watcher对象，然后将其添加到对应资源的watcher列表中。当资源状态发生变化时，kube-apiserver会遍历watcher列表，并向所有订阅该资源的客户端发送新状态。客户端在收到新状态后，需要通过比较前后两个状态的差异来判断资源状态是否发生变化，并进行相应的处理。 执行命令kubectl get deployment use -n default -w可以监听名称为use的deployment的变化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 当这条语句执行会调用Watch// newCacheWatcher创建一个watcher// staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.gofunc (c *Cacher) Watch(ctx context.Context, key string, opts storage.ListOptions) (watch.Interface, error) &#123; ... watcher := newCacheWatcher( chanSize, filterWithAttrsFunction(key, pred), emptyFunc, c.versioner, deadline, pred.AllowWatchBookmarks, c.groupResource, identifier, ) ... func() &#123; c.Lock() defer c.Unlock() if generation, ok := c.ready.checkAndReadGeneration(); generation != readyGeneration || !ok &#123; // We went unready or are already on a different generation. // Avoid registering and starting the watch as it will have to be // terminated immediately anyway. return &#125; // Update watcher.forget function once we can compute it. watcher.forget = forgetWatcher(c, watcher, c.watcherIdx, scope, triggerValue, triggerSupported) // Update the bookMarkAfterResourceVersion watcher.setBookmarkAfterResourceVersion(bookmarkAfterResourceVersionFn()) // 将创建的watcher添加到Cacher中的watchers c.watchers.addWatcher(watcher, c.watcherIdx, scope, triggerValue, triggerSupported) addedWatcher = true // Add it to the queue only when the client support watch bookmarks. if watcher.allowWatchBookmarks &#123; c.bookmarkWatchers.addWatcher(watcher) &#125; c.watcherIdx++ &#125;() ... // 启动一个goroutine go watcher.processInterval(ctx, cacheInterval, startWatchRV) return watcher, nil 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889// staging/src/k8s.io/apiserver/pkg/storage/cacher/cache_watcher.gotype cacheWatcher struct &#123; input chan *watchCacheEvent result chan watch.Event done chan struct&#123;&#125; filter filterWithAttrsFunc stopped bool forget func(bool) versioner storage.Versioner deadline time.Time allowWatchBookmarks bool groupResource schema.GroupResource identifier string drainInputBuffer bool bookmarkAfterResourceVersion uint64 // stateMutex protects state stateMutex sync.Mutex state int&#125;func (c *cacheWatcher) processInterval(ctx context.Context, cacheInterval *watchCacheInterval, resourceVersion uint64) &#123; defer utilruntime.HandleCrash() defer close(c.result) defer c.Stop() // Check how long we are processing initEvents. // As long as these are not processed, we are not processing // any incoming events, so if it takes long, we may actually // block all watchers for some time. // TODO: From the logs it seems that there happens processing // times even up to 1s which is very long. However, this doesn't // depend that much on the number of initEvents. E.g. from the // 2000-node Kubemark run we have logs like this, e.g.: // ... processing 13862 initEvents took 66.808689ms // ... processing 14040 initEvents took 993.532539ms // We should understand what is blocking us in those cases (e.g. // is it lack of CPU, network, or sth else) and potentially // consider increase size of result buffer in those cases. const initProcessThreshold = 500 * time.Millisecond startTime := time.Now() initEventCount := 0 for &#123; event, err := cacheInterval.Next() if err != nil &#123; // An error indicates that the cache interval // has been invalidated and can no longer serve // events. // // Initially we considered sending an "out-of-history" // Error event in this case, but because historically // such events weren't sent out of the watchCache, we // decided not to. This is still ok, because on watch // closure, the watcher will try to re-instantiate the // watch and then will get an explicit "out-of-history" // window. There is potential for optimization, but for // now, in order to be on the safe side and not break // custom clients, the cost of it is something that we // are fully accepting. klog.Warningf("couldn't retrieve watch event to serve: %#v", err) return &#125; if event == nil &#123; break &#125; // 转换成watchEvent,并发送到c.result c.sendWatchCacheEvent(event) if event.ResourceVersion &gt; resourceVersion &#123; resourceVersion = event.ResourceVersion &#125; initEventCount++ &#125; if initEventCount &gt; 0 &#123; metrics.InitCounter.WithLabelValues(c.groupResource.String()).Add(float64(initEventCount)) &#125; processingTime := time.Since(startTime) if processingTime &gt; initProcessThreshold &#123; klog.V(2).Infof("processing %d initEvents of %s (%s) took %v", initEventCount, c.groupResource, c.identifier, processingTime) &#125; c.process(ctx, resourceVersion)&#125; 事件通知1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// dispatchEvents --&gt; dispatchEvent --&gt; watcher.addfunc (c *Cacher) dispatchEvent(event *watchCacheEvent) &#123; c.startDispatching(event) defer c.finishDispatching() // Watchers stopped after startDispatching will be delayed to finishDispatching, // Since add() can block, we explicitly add when cacher is unlocked. // Dispatching event in nonblocking way first, which make faster watchers // not be blocked by slower ones. if event.Type == watch.Bookmark &#123; for _, watcher := range c.watchersBuffer &#123; watcher.nonblockingAdd(event) &#125; &#125; else &#123; wcEvent := *event setCachingObjects(&amp;wcEvent, c.versioner) event = &amp;wcEvent c.blockedWatchers = c.blockedWatchers[:0] for _, watcher := range c.watchersBuffer &#123; if !watcher.nonblockingAdd(event) &#123; c.blockedWatchers = append(c.blockedWatchers, watcher) &#125; &#125; if len(c.blockedWatchers) &gt; 0 &#123; // dispatchEvent is called very often, so arrange // to reuse timers instead of constantly allocating. startTime := time.Now() timeout := c.dispatchTimeoutBudget.takeAvailable() c.timer.Reset(timeout) // Send event to all blocked watchers. As long as timer is running, // `add` will wait for the watcher to unblock. After timeout, // `add` will not wait, but immediately close a still blocked watcher. // Hence, every watcher gets the chance to unblock itself while timer // is running, not only the first ones in the list. timer := c.timer for _, watcher := range c.blockedWatchers &#123; if !watcher.add(event, timer) &#123; // fired, clean the timer by set it to nil. timer = nil &#125; &#125; // Stop the timer if it is not fired if timer != nil &amp;&amp; !timer.Stop() &#123; // Consume triggered (but not yet received) timer event // so that future reuse does not get a spurious timeout. &lt;-timer.C &#125; c.dispatchTimeoutBudget.returnUnused(timeout - time.Since(startTime)) &#125; &#125;&#125; REF:1.staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go2.staging/src/k8s.io/apiserver/pkg/storage/cacher/cache_watcher.go)3.staging/src/k8s.io/apiserver/pkg/storage/cacher/watch_cache.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Adapter]]></title>
    <url>%2F2023%2F04%2F10%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BAdapter%2F</url>
    <content type="text"><![CDATA[什么是AdapterAdapter也就是适配器模式 适配器模式是一种结构型设计模式，它将一个接口转换成另一个客户端所期望的接口，从而使得原本不兼容的接口能够协同工作。适配器模式常常用于将一个旧接口适配成新接口，或将一个外部库的接口适配成当前应用程序所需的接口 适配器模式由三个主要角色组成：客户端、适配器和被适配者。其中，客户端使用目标接口来与适配器交互，适配器将客户端的请求转换为被适配者的请求并进行处理，被适配者则是适配器所需要的接口。 k8s中适配器模式k8s中有很多不同的资源，对于创建资源，k8s实现了createHandler方法来处理 12345678910111213141516171819202122232425262728293031323334353637383940414243// staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go// 这里返回http.HandlerFuncfunc createHandler(r rest.NamedCreater, scope *RequestScope, admit admission.Interface, includeName bool) http.HandlerFunc &#123; return func(w http.ResponseWriter, req *http.Request) &#123; ctx := req.Context() // For performance tracking purposes. ctx, span := tracing.Start(ctx, "Create", traceFields(req)...) defer span.End(500 * time.Millisecond) ... namespace, name, err := scope.Namer.Name(req) requestFunc := func() (runtime.Object, error) &#123; // 调用r实现的Create方法，上层可以实现这个接口实现适配 return r.Create( ctx, name, obj, rest.AdmissionToValidateObjectFunc(admit, admissionAttributes, scope), options, ) &#125; ... &#125;&#125;// CreateNamedResource returns a function that will handle a resource creation with name.func CreateNamedResource(r rest.NamedCreater, scope *RequestScope, admission admission.Interface) http.HandlerFunc &#123; return createHandler(r, scope, admission, true)&#125;// CreateResource returns a function that will handle a resource creation.func CreateResource(r rest.Creater, scope *RequestScope, admission admission.Interface) http.HandlerFunc &#123; return createHandler(&amp;namedCreaterAdapter&#123;r&#125;, scope, admission, false)&#125;// 通过namedCreaterAdapter实现Create方法，适配createHandler. createHandler将调用对应的Create方法type namedCreaterAdapter struct &#123; rest.Creater&#125;func (c *namedCreaterAdapter) Create(ctx context.Context, name string, obj runtime.Object, createValidatingAdmission rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) &#123; return c.Creater.Create(ctx, obj, createValidatingAdmission, options)&#125; REF:1.staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Chan of Responsibility]]></title>
    <url>%2F2023%2F04%2F09%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BChan-of-Responsibility%2F</url>
    <content type="text"><![CDATA[Chan of Responsibility我们常说的责任链模式k8s中的实现kube-apiserver中的admission控制器通过责任链模式实现。每个admission控制器都实现了一个AdmissionHandler接口，该接口包含一个名为Admit的方法，用于验证请求是否符合规则。多个admission控制器按照注册顺序形成一个责任链。当请求进来时，apiserver会遍历这个责任链，逐一调用每个admission控制器的Admit方法，只有当所有控制器都通过验证后，请求才会被通过。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// vendor/k8s.io/apiserver/pkg/admission/chain.go// 下面定义了两种性质的webhook,修改性质在验证性质的之前被调用// chainAdmissionHandler is an instance of admission.NamedHandler that performs admission control using// a chain of admission handlerstype chainAdmissionHandler []Interface// NewChainHandler creates a new chain handler from an array of handlers. Used for testing.func NewChainHandler(handlers ...Interface) chainAdmissionHandler &#123; return chainAdmissionHandler(handlers)&#125;// Admit performs an admission control check using a chain of handlers, and returns immediately on first error// 修改性质的webhook，此类handler实现了MutationInterfacefunc (admissionHandler chainAdmissionHandler) Admit(ctx context.Context, a Attributes, o ObjectInterfaces) error &#123; for _, handler := range admissionHandler &#123; if !handler.Handles(a.GetOperation()) &#123; continue &#125; if mutator, ok := handler.(MutationInterface); ok &#123; err := mutator.Admit(ctx, a, o) if err != nil &#123; return err &#125; &#125; &#125; return nil&#125;// Validate performs an admission control check using a chain of handlers, and returns immediately on first error// 验证性质的webhook, 此类handler实现了ValidationInterfacefunc (admissionHandler chainAdmissionHandler) Validate(ctx context.Context, a Attributes, o ObjectInterfaces) error &#123; for _, handler := range admissionHandler &#123; if !handler.Handles(a.GetOperation()) &#123; continue &#125; if validator, ok := handler.(ValidationInterface); ok &#123; err := validator.Validate(ctx, a, o) if err != nil &#123; return err &#125; &#125; &#125; return nil&#125;// Handles will return true if any of the handlers handles the given operationfunc (admissionHandler chainAdmissionHandler) Handles(operation Operation) bool &#123; for _, handler := range admissionHandler &#123; if handler.Handles(operation) &#123; return true &#125; &#125; return false&#125; admission Interface123456789101112131415161718192021222324// vendor/k8s.io/apiserver/pkg/admission/interfaces.go// Interface is an abstract, pluggable interface for Admission Control decisions.type Interface interface &#123; // Handles returns true if this admission controller can handle the given operation // where operation can be one of CREATE, UPDATE, DELETE, or CONNECT Handles(operation Operation) bool&#125;type MutationInterface interface &#123; Interface // Admit makes an admission decision based on the request attributes. // Context is used only for timeout/deadline/cancellation and tracing information. Admit(ctx context.Context, a Attributes, o ObjectInterfaces) (err error)&#125;// ValidationInterface is an abstract, pluggable interface for Admission Control decisions.type ValidationInterface interface &#123; Interface // Validate makes an admission decision based on the request attributes. It is NOT allowed to mutate // Context is used only for timeout/deadline/cancellation and tracing information. Validate(ctx context.Context, a Attributes, o ObjectInterfaces) (err error)&#125; 调用时机Matation1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go// 把有Post请求都会经过CreateHandler方法，这里使用了装饰器模式返回http.HandlerFunc// 代码过长删除了部分逻辑func createHandler(r rest.NamedCreater, scope *RequestScope, admit admission.Interface, includeName bool) http.HandlerFunc &#123; return func(w http.ResponseWriter, req *http.Request) &#123; ctx := req.Context() ... result, err := finisher.FinishRequest(ctx, func() (runtime.Object, error) &#123; if scope.FieldManager != nil &#123; liveObj, err := scope.Creater.New(scope.Kind) if err != nil &#123; return nil, fmt.Errorf("failed to create new object (Create for %v): %v", scope.Kind, err) &#125; obj = scope.FieldManager.UpdateNoErrors(liveObj, obj, managerOrUserAgent(options.FieldManager, req.UserAgent())) admit = fieldmanager.NewManagedFieldsValidatingAdmissionController(admit) &#125; // 执行admission handler if mutatingAdmission, ok := admit.(admission.MutationInterface); ok &amp;&amp; mutatingAdmission.Handles(admission.Create) &#123; if err := mutatingAdmission.Admit(ctx, admissionAttributes, scope); err != nil &#123; return nil, err &#125; &#125; // Dedup owner references again after mutating admission happens dedupOwnerReferencesAndAddWarning(obj, req.Context(), true) result, err := requestFunc() // If the object wasn't committed to storage because it's serialized size was too large, // it is safe to remove managedFields (which can be large) and try again. if isTooLargeError(err) &#123; if accessor, accessorErr := meta.Accessor(obj); accessorErr == nil &#123; accessor.SetManagedFields(nil) result, err = requestFunc() &#125; &#125; return result, err &#125;) ... // 调用Store中的Create方法，这里的Create方法是对etcd的一层封装 // 可以看到AdmissionToValidateObjectFunc requestFunc := func() (runtime.Object, error) &#123; return r.Create( ctx, name, obj, rest.AdmissionToValidateObjectFunc(admit, admissionAttributes, scope), options, ) &#125; ... result, err := requestFunc() &#125;&#125; Validate1234567891011// vendor/k8s.io/apiserver/pkg/registry/generic/registry/store.gofunc (e *Store) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) &#123; ... // 这里将会执行validate if createValidation != nil &#123; if err := createValidation(ctx, obj.DeepCopyObject()); err != nil &#123; return nil, err &#125; &#125; ...&#125; REF:1.staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go2.staging/src/k8s.io/apiserver/pkg/admission/chain.go3.staging/src/k8s.io/apiserver/pkg/admission/interfaces.go4.staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Visitor]]></title>
    <url>%2F2023%2F04%2F07%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BVisitor%2F</url>
    <content type="text"><![CDATA[Visitor模式也叫访问模式 123456// vendor/k8s.io/cli-runtime/pkg/resource/interfaces.gotype Visitor interface &#123; Visit(VisitorFunc) error&#125;type VisitorFunc func(*Info, error) error 12345678910111213141516// vendor/k8s.io/cli-runtime/pkg/resource/visitor.gotype Info struct &#123; Client RESTClient Mapping *meta.RESTMapping Namespace string Name string Source string Object runtime.Object ResourceVersion string Subresource string&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// 从io.Reader读取数据type StreamVisitor struct &#123; io.Reader *mapper Source string Schema ContentValidator&#125;// 不同的Visitor处理不同的对象，比如URLVisitor处理URL,FileVisitor处理文件// 从URL中下载文件type URLVisitor struct &#123; URL *url.URL *StreamVisitor HttpAttemptCount int&#125;// FileVisitor is wrapping around a StreamVisitor, to handle open/close filestype FileVisitor struct &#123; Path string *StreamVisitor&#125;// Visit函数在一个流中实现了Visitor接口。StreamVisitor可以在一个流中区分多个资源func (v *StreamVisitor) Visit(fn VisitorFunc) error &#123; d := yaml.NewYAMLOrJSONDecoder(v.Reader, 4096) for &#123; ext := runtime.RawExtension&#123;&#125; if err := d.Decode(&amp;ext); err != nil &#123; if err == io.EOF &#123; return nil &#125; return fmt.Errorf("error parsing %s: %v", v.Source, err) &#125; // TODO: This needs to be able to handle object in other encodings and schemas. ext.Raw = bytes.TrimSpace(ext.Raw) if len(ext.Raw) == 0 || bytes.Equal(ext.Raw, []byte("null")) &#123; continue &#125; if err := ValidateSchema(ext.Raw, v.Schema); err != nil &#123; return fmt.Errorf("error validating %q: %v", v.Source, err) &#125; info, err := v.infoForData(ext.Raw, v.Source) if err != nil &#123; if fnErr := fn(info, err); fnErr != nil &#123; return fnErr &#125; continue &#125; if err := fn(info, nil); err != nil &#123; return err &#125; &#125;&#125;// Visit in a FileVisitor is just taking care of opening/closing filesfunc (v *FileVisitor) Visit(fn VisitorFunc) error &#123; var f *os.File if v.Path == constSTDINstr &#123; f = os.Stdin &#125; else &#123; var err error f, err = os.Open(v.Path) if err != nil &#123; return err &#125; defer f.Close() &#125; // TODO: Consider adding a flag to force to UTF16, apparently some // Windows tools don't write the BOM utf16bom := unicode.BOMOverride(unicode.UTF8.NewDecoder()) v.StreamVisitor.Reader = transform.NewReader(f, utf16bom) return v.StreamVisitor.Visit(fn)&#125; 12345678910111213141516171819202122232425262728// 构造r.visitor// 这里有两层嵌套，ContinueOnErrorVisitor,NewDecoratedVisitorfunc (b *Builder) Do() *Result &#123; r := b.visitorResult() r.mapper = b.Mapper() if r.err != nil &#123; return r &#125; if b.flatten &#123; r.visitor = NewFlattenListVisitor(r.visitor, b.objectTyper, b.mapper) &#125; helpers := []VisitorFunc&#123;&#125; if b.defaultNamespace &#123; helpers = append(helpers, SetNamespace(b.namespace)) &#125; if b.requireNamespace &#123; helpers = append(helpers, RequireNamespace(b.namespace)) &#125; helpers = append(helpers, FilterNamespace) if b.requireObject &#123; helpers = append(helpers, RetrieveLazy) &#125; if b.continueOnError &#123; r.visitor = ContinueOnErrorVisitor&#123;Visitor: r.visitor&#125; &#125; r.visitor = NewDecoratedVisitor(r.visitor, helpers...) return r&#125; 调用过程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func (o *CreateOptions) RunCreate(f cmdutil.Factory, cmd *cobra.Command) error &#123; // 这里会构造一个嵌套的visitor r := f.NewBuilder(). Unstructured(). Schema(schema). ContinueOnError(). NamespaceParam(cmdNamespace).DefaultNamespace(). FilenameParam(enforceNamespace, &amp;o.FilenameOptions). LabelSelectorParam(o.Selector). Flatten(). Do() err = r.Err() if err != nil &#123; return err &#125; count := 0 // 调用Result的Vistor // 会调用不同的Visitor并填充info结构体 err = r.Visit(func(info *resource.Info, err error) error &#123; if err != nil &#123; return err &#125; if err := util.CreateOrUpdateAnnotation(cmdutil.GetFlagBool(cmd, cmdutil.ApplyAnnotationsFlag), info.Object, scheme.DefaultJSONEncoder()); err != nil &#123; return cmdutil.AddSourceToErr("creating", info.Source, err) &#125; if err := o.Recorder.Record(info.Object); err != nil &#123; klog.V(4).Infof("error recording current command: %v", err) &#125; if o.DryRunStrategy != cmdutil.DryRunClient &#123; // 构造请求通过RESTClient向apiserver发送一个Post请求 obj, err := resource. NewHelper(info.Client, info.Mapping). DryRun(o.DryRunStrategy == cmdutil.DryRunServer). WithFieldManager(o.fieldManager). WithFieldValidation(o.ValidationDirective). Create(info.Namespace, true, info.Object) if err != nil &#123; return cmdutil.AddSourceToErr("creating", info.Source, err) &#125; info.Refresh(obj, true) &#125; count++ return o.PrintObj(info.Object) &#125;) if err != nil &#123; return err &#125; if count == 0 &#123; return fmt.Errorf("no objects passed to create") &#125; return nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183// 下面是一层一层Visit方法的调用// DecoratedVisitor.Visit// ContinueOnErrorVisitor.Visit// FlattenListVisitor.Visit// FlattenListVisitor.Visit// EagerVisitorList.Visit// FileVisitor.Visit// StreamVisitor.Visitfunc (r *Result) Visit(fn VisitorFunc) error &#123; if r.err != nil &#123; return r.err &#125; err := r.visitor.Visit(fn) return utilerrors.FilterOut(err, r.ignoreErrors...)&#125;// DecoratedVistor,这里使用的是装饰器模式// golang不像python那样提供语法糖，所以只能通过这种方式实现func (v DecoratedVisitor) Visit(fn VisitorFunc) error &#123; return v.visitor.Visit(func(info *Info, err error) error &#123; if err != nil &#123; return err &#125; for i := range v.decorators &#123; if err := v.decorators[i](info, nil); err != nil &#123; return err &#125; &#125; r := fn(info, nil) return r &#125;)&#125;func (v ContinueOnErrorVisitor) Visit(fn VisitorFunc) error &#123; var errs []error err := v.Visitor.Visit(func(info *Info, err error) error &#123; if err != nil &#123; errs = append(errs, err) return nil &#125; if err := fn(info, nil); err != nil &#123; errs = append(errs, err) &#125; return nil &#125;) if err != nil &#123; errs = append(errs, err) &#125; if len(errs) == 1 &#123; return errs[0] &#125; return utilerrors.NewAggregate(errs)&#125;func (v FlattenListVisitor) Visit(fn VisitorFunc) error &#123; return v.visitor.Visit(func(info *Info, err error) error &#123; if err != nil &#123; return err &#125; if info.Object == nil &#123; return fn(info, nil) &#125; if !meta.IsListType(info.Object) &#123; return fn(info, nil) &#125; items := []runtime.Object&#123;&#125; itemsToProcess := []runtime.Object&#123;info.Object&#125; for i := 0; i &lt; len(itemsToProcess); i++ &#123; currObj := itemsToProcess[i] if !meta.IsListType(currObj) &#123; items = append(items, currObj) continue &#125; currItems, err := meta.ExtractList(currObj) if err != nil &#123; return err &#125; if errs := runtime.DecodeList(currItems, v.mapper.decoder); len(errs) &gt; 0 &#123; return utilerrors.NewAggregate(errs) &#125; itemsToProcess = append(itemsToProcess, currItems...) &#125; // If we have a GroupVersionKind on the list, prioritize that when asking for info on the objects contained in the list var preferredGVKs []schema.GroupVersionKind if info.Mapping != nil &amp;&amp; !info.Mapping.GroupVersionKind.Empty() &#123; preferredGVKs = append(preferredGVKs, info.Mapping.GroupVersionKind) &#125; var errs []error for i := range items &#123; item, err := v.mapper.infoForObject(items[i], v.typer, preferredGVKs) if err != nil &#123; errs = append(errs, err) continue &#125; if len(info.ResourceVerEagerVisitorListsion) != 0 &#123; item.ResourceVersion = info.ResourceVersion &#125; // propagate list source to items source if len(info.Source) != 0 &#123; item.Source = info.Source &#125; if err := fn(item, nil); err != nil &#123; errs = append(errs, err) &#125; &#125; return utilerrors.NewAggregate(errs) &#125;)&#125;func (l EagerVisitorList) Visit(fn VisitorFunc) error &#123; var errs []error for i := range l &#123; err := l[i].Visit(func(info *Info, err error) error &#123; if err != nil &#123; errs = append(errs, err) return nil &#125; if err := fn(info, nil); err != nil &#123; errs = append(errs, err) &#125; return nil &#125;) if err != nil &#123; errs = append(errs, err) &#125; &#125; return utilerrors.NewAggregate(errs)&#125;func (v *FileVisitor) Visit(fn VisitorFunc) error &#123; var f *os.File if v.Path == constSTDINstr &#123; f = os.Stdin &#125; else &#123; var err error f, err = os.Open(v.Path) if err != nil &#123; return err &#125; defer f.Close() &#125; utf16bom := unicode.BOMOverride(unicode.UTF8.NewDecoder()) v.StreamVisitor.Reader = transform.NewReader(f, utf16bom) return v.StreamVisitor.Visit(fn)&#125;func (v *StreamVisitor) Visit(fn VisitorFunc) error &#123; d := yaml.NewYAMLOrJSONDecoder(v.Reader, 4096) for &#123; ext := runtime.RawExtension&#123;&#125; if err := d.Decode(&amp;ext); err != nil &#123; if err == io.EOF &#123; return nil &#125; return fmt.Errorf("error parsing %s: %v", v.Source, err) &#125; // TODO: This needs to be able to handle object in other encodings and schemas. ext.Raw = bytes.TrimSpace(ext.Raw) if len(ext.Raw) == 0 || bytes.Equal(ext.Raw, []byte("null")) &#123; continue &#125; if err := ValidateSchema(ext.Raw, v.Schema); err != nil &#123; return fmt.Errorf("error validating %q: %v", v.Source, err) &#125; info, err := v.infoForData(ext.Raw, v.Source) if err != nil &#123; if fnErr := fn(info, err); fnErr != nil &#123; return fnErr &#125; continue &#125; if err := fn(info, nil); err != nil &#123; return err &#125; &#125;&#125; REF:1.https://coolshell.cn/articles/21263.html2.staging/src/k8s.io/cli-runtime/pkg/resource/builder.go3.staging/src/k8s.io/cli-runtime/pkg/resource/visitor.go4.staging/src/k8s.io/kubectl/pkg/cmd/create/create.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Strategy]]></title>
    <url>%2F2023%2F04%2F07%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BStrategy%2F</url>
    <content type="text"><![CDATA[Strategy也就是策略模式 k8s中策略模式的实现k8s中为不同的资源对象实现了不同的策略。 123456789101112131415// vendor/k8s.io/apiserver/pkg/registry/generic/registry/store.go// Store实现了k8s.io/apiserver/pkg/registry/rest.StandardStorage接口,// 它允许使用者实现任何需要的非通用函数type Store struct &#123; NewFunc func() runtime.Object NewListFunc func() runtime.Object ... // 对应的策略 CreateStrategy rest.RESTCreateStrategy UpdateStrategy rest.RESTUpdateStrategy DeleteStrategy rest.RESTDeleteStrategy ...&#125; Deployment实现的策略1234567891011121314151617181920212223242526//pkg/registry/apps/deployment/strategy.gotype deploymentStrategy struct &#123; runtime.ObjectTyper names.NameGenerator&#125;// Strategy is the default logic that applies when creating and updating Deployment// objects via the REST API.var Strategy = deploymentStrategy&#123;legacyscheme.Scheme, names.SimpleNameGenerator&#125;// PrepareForCreate clears fields that are not allowed to be set by end users on creation.func (deploymentStrategy) PrepareForCreate(ctx context.Context, obj runtime.Object) &#123; deployment := obj.(*apps.Deployment) deployment.Status = apps.DeploymentStatus&#123;&#125; deployment.Generation = 1 pod.DropDisabledTemplateFields(&amp;deployment.Spec.Template, nil)&#125;// Validate validates a new deployment.func (deploymentStrategy) Validate(ctx context.Context, obj runtime.Object) field.ErrorList &#123; deployment := obj.(*apps.Deployment) opts := pod.GetValidationOptionsFromPodTemplate(&amp;deployment.Spec.Template, nil) return appsvalidation.ValidateDeployment(deployment, opts)&#125; ReplicaSet实现的策略123456789101112131415161718192021222324// pkg/registry/apps/replicaset/strategy.gotype rsStrategy struct &#123; runtime.ObjectTyper names.NameGenerator&#125;// Strategy is the default logic that applies when creating and updating ReplicaSet objects.var Strategy = rsStrategy&#123;legacyscheme.Scheme, names.SimpleNameGenerator&#125;func (rsStrategy) Validate(ctx context.Context, obj runtime.Object) field.ErrorList &#123; rs := obj.(*apps.ReplicaSet) opts := pod.GetValidationOptionsFromPodTemplate(&amp;rs.Spec.Template, nil) return appsvalidation.ValidateReplicaSet(rs, opts)&#125;https://groups.google.com/g/kubernetes-sig-apps/c/IGuguCg-vv8rnings for the creation of the given object.func (rsStrategy) WarningsOnCreate(ctx context.Context, obj runtime.Object) []string &#123; newRS := obj.(*apps.ReplicaSet) var warnings []string if msgs := utilvalidation.IsDNS1123Label(newRS.Name); len(msgs) != 0 &#123; warnings = append(warnings, fmt.Sprintf("metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: %v", msgs)) &#125; warnings = append(warnings, pod.GetWarningsForPodTemplate(ctx, field.NewPath("spec", "template"), &amp;newRS.Spec.Template, nil)...) return warnings&#125; Store如何与对应的Strategy关联12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 创建对应store时指定了Deployment对应的strategy// pkg/registry/apps/deployment/storage/storage.go// NewREST returns a RESTStorage object that will work against deployments.func NewREST(optsGetter generic.RESTOptionsGetter) (*REST, *StatusREST, *RollbackREST, error) &#123; store := &amp;genericregistry.Store&#123; NewFunc: func() runtime.Object &#123; return &amp;apps.Deployment&#123;&#125; &#125;, NewListFunc: func() runtime.Object &#123; return &amp;apps.DeploymentList&#123;&#125; &#125;, DefaultQualifiedResource: apps.Resource("deployments"), SingularQualifiedResource: apps.Resource("deployment"), CreateStrategy: deployment.Strategy, UpdateStrategy: deployment.Strategy, DeleteStrategy: deployment.Strategy, ResetFieldsStrategy: deployment.Strategy, TableConvertor: printerstorage.TableConvertor&#123;TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)&#125;, &#125; options := &amp;generic.StoreOptions&#123;RESTOptions: optsGetter&#125; if err := store.CompleteWithOptions(options); err != nil &#123; return nil, nil, nil, err &#125; statusStore := *store statusStore.UpdateStrategy = deployment.StatusStrategy statusStore.ResetFieldsStrategy = deployment.StatusStrategy return &amp;REST&#123;store&#125;, &amp;StatusREST&#123;store: &amp;statusStore&#125;, &amp;RollbackREST&#123;store: store&#125;, nil&#125;// 创建对应store时指定了ReplicaSet对应的strategy// pkg/registry/apps/replicaset/storage/storage.go// NewREST returns a RESTStorage object that will work against ReplicaSet.func NewREST(optsGetter generic.RESTOptionsGetter) (*REST, *StatusREST, error) &#123; store := &amp;genericregistry.Store&#123; NewFunc: func() runtime.Object &#123; return &amp;apps.ReplicaSet&#123;&#125; &#125;, NewListFunc: func() runtime.Object &#123; return &amp;apps.ReplicaSetList&#123;&#125; &#125;, PredicateFunc: replicaset.MatchReplicaSet, DefaultQualifiedResource: apps.Resource("replicasets"), SingularQualifiedResource: apps.Resource("replicaset"), CreateStrategy: replicaset.Strategy, UpdateStrategy: replicaset.Strategy, DeleteStrategy: replicaset.Strategy, ResetFieldsStrategy: replicaset.Strategy, TableConvertor: printerstorage.TableConvertor&#123;TableGenerator: printers.NewTableGenerator().With(printersinternal.AddHandlers)&#125;, &#125; options := &amp;generic.StoreOptions&#123;RESTOptions: optsGetter, AttrFunc: replicaset.GetAttrs&#125; if err := store.CompleteWithOptions(options); err != nil &#123; return nil, nil, err &#125; statusStore := *store statusStore.UpdateStrategy = replicaset.StatusStrategy statusStore.ResetFieldsStrategy = replicaset.StatusStrategy return &amp;REST&#123;store&#125;, &amp;StatusREST&#123;store: &amp;statusStore&#125;, nil&#125; 何时调用Strategy例如CreateStrategy,在创建Deployment的时候会执行123456789101112131415161718192021// staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.gofunc (e *Store) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) &#123; var finishCreate FinishFunc = finishNothing // Init metadata as early as possible. if objectMeta, err := meta.Accessor(obj); err != nil &#123; return nil, err &#125; else &#123; rest.FillObjectMetaSystemFields(objectMeta) if len(objectMeta.GetGenerateName()) &gt; 0 &amp;&amp; len(objectMeta.GetName()) == 0 &#123; objectMeta.SetName(e.CreateStrategy.GenerateName(objectMeta.GetGenerateName())) &#125; &#125; ... if err := rest.BeforeCreate(e.CreateStrategy, ctx, obj); err != nil &#123; return nil, err &#125; ... return out, nil&#125; REF:1.pkg/registry/generic/registry/store.go2.pkg/registry/apps/replicaset/strategy.go3.pkg/registry/apps/replicaset/storage/storage.go4.pkg/registry/apps/deployment/strategy.go5.pkg/registry/apps/deployment/storage/storage.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Factory]]></title>
    <url>%2F2023%2F04%2F06%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BFactory%2F</url>
    <content type="text"><![CDATA[工厂方法是一种创建型设计模式，详情可参考工厂方法如何创建一个informer123 // 创建对应资源的informerfactory := informers.NewSharedInformerFactory(clientset, time.Minute)informer := factory.Core().V1().Pods().Informer() 工厂方法构造123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596// 返回一个结构体sharedInformerFactory指针,该结构体实现了接口SharedInformerFactoryfunc NewSharedInformerFactory(client kubernetes.Interface, defaultResync time.Duration) SharedInformerFactory &#123; return NewSharedInformerFactoryWithOptions(client, defaultResync)&#125;func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory &#123; factory := &amp;sharedInformerFactory&#123; client: client, namespace: v1.NamespaceAll, defaultResync: defaultResync, informers: make(map[reflect.Type]cache.SharedIndexInformer), startedInformers: make(map[reflect.Type]bool), customResync: make(map[reflect.Type]time.Duration), &#125; // Apply all options for _, opt := range options &#123; factory = opt(factory) &#125; return factory&#125;type SharedInformerFactory interface &#123; internalinterfaces.SharedInformerFactory ForResource(resource schema.GroupVersionResource) (GenericInformer, error) WaitForCacheSync(stopCh &lt;-chan struct&#123;&#125;) map[reflect.Type]bool Admissionregistration() admissionregistration.Interface Internal() apiserverinternal.Interface Apps() apps.Interface Autoscaling() autoscaling.Interface Batch() batch.Interface Certificates() certificates.Interface Coordination() coordination.Interface Core() core.Interface Discovery() discovery.Interface Events() events.Interface Extensions() extensions.Interface Flowcontrol() flowcontrol.Interface Networking() networking.Interface Node() node.Interface Policy() policy.Interface Rbac() rbac.Interface Scheduling() scheduling.Interface Storage() storage.Interface&#125;// 以Core()方法为例，实现了V1()方法，返回值为一个接口type Interface interface &#123; // V1 provides access to shared informers for resources in V1. V1() v1.Interface&#125;// v1.Interfacetype Interface interface &#123; // ComponentStatuses returns a ComponentStatusInformer. ComponentStatuses() 设计模式ComponentStatusInformer // ConfigMaps returns a ConfigMapInformer. ConfigMaps() ConfigMapInformer // Endpoints returns a EndpointsInformer. Endpoints() EndpointsInformer // Events returns a EventInformer. Events() EventInformer // LimitRanges returns a 设计模式LimitRangeInformer. LimitRanges() LimitRangeInformer // Namespaces returns a NamespaceInformer. Namespaces() NamespaceInformer // Nodes returns a NodeInformer. Nodes() NodeInformer // PersistentVolumes returns a PersistentVolumeInformer. PersistentVolumes() PersistentVolumeInformer // PersistentVolumeClaims returns a PersistentVolumeClaimInformer. PersistentVolumeClaims() PersistentVolumeClaimInformer // Pods returns a PodInformer. Pods() PodInformer // PodTemplates returns a PodTemplateInformer. PodTemplates() PodTemplateInformer // ReplicationControllers returns a ReplicationControllerInformer. ReplicationControllers() ReplicationControllerInformer // ResourceQuotas returns a ResourceQuotaInformer. ResourceQuotas() ResourceQuotaInformer // Secrets returns a SecretInformer. Secrets() SecretInformer // Services returns a ServiceInformer. Services() ServiceInformer // ServiceAccounts returns a ServiceAccountInformer. ServiceAccounts() ServiceAccountInformer&#125;// 以Pods()为例，该方法返回PodInformer接口// 实现了Informer和Lister()方法type PodInformer interface &#123; Informer() cache.SharedIndexInformer Lister() v1.PodLister&#125; factory.Core().V1().Pods().Informer()1.首先调用Core()方法12345678910111213// Core()返回结构体group指针，group实现了V1()方法func (f *sharedInformerFactory) Core() core.Interface &#123; return core.New(f, f.namespace, f.tweakListOptions)&#125;type group struct &#123; factory internalinterfaces.SharedInformerFactory namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc&#125;func (g *group) V1() v1.Interface &#123; return v1.New(g.factory, g.namespace, g.tweakListOptions)&#125; 2.调用V1()方法12345678910// V1()返回结构体version的指针，version实现了v1.Interface中的所有方法func (g *group) V1() v1.Interface &#123; return v1.New(g.factory, g.namespace, g.tweakListOptions)&#125;type version struct &#123; factory internalinterfaces.SharedInformerFactory namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc&#125; ４.调用Pods()方法1234// 返回podInformer指针，podInformer实现了Informer(),Lister()方法func (v *version) Pods() PodInformer &#123; return &amp;podInformer&#123;factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions&#125;&#125; 5.最后调用Informer()方法,完成对象创建1234567891011121314151617181920212223242526272829// 调用InformerFor// staging/src/k8s.io/client-go/informers/core/v1/pod.gofunc (f *podInformer) Informer() cache.SharedIndexInformer &#123; return f.factory.InformerFor(&amp;corev1.Pod&#123;&#125;, f.defaultInformer)&#125;// staging/src/k8s.io/client-go/informers/factory.gofunc (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer &#123; f.lock.Lock() defer f.lock.Unlock() informerType := reflect.TypeOf(obj) // 此处会检查informer是否存在，如果存在则直接返回 // 复用informer,节省资源 informer, exists := f.info设计模式rmers[informerType] if exists &#123; return informer &#125; resyncPeriod, exists := f.customResync[informerType] if !exists &#123; resyncPeriod = f.defaultResync &#125; informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer&#125; REF:1.client-go/informers/factory.go2.client-go/informers/core/interface.go3.client-go/informers/core/v1/interface.go4.client-go/informers/core/v1/pod.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码设计模式之Builder]]></title>
    <url>%2F2023%2F04%2F06%2Fk8s%E6%BA%90%E7%A0%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8BBuilder%2F</url>
    <content type="text"><![CDATA[Builder模式也叫生成器模式，也叫建造者模式关于生成器模式详情可查看文章 记录下k8s源码中使用到的Builder模式。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// staging/src/k8s.io/client-go/rest/request.go// Request 构建一个请求发送给kube-apiserver.// 其中有很多元素需要设置，但只一些是必须的type Request struct &#123; c *RESTClient warningHandler WarningHandler rateLimiter flowcontrol.RateLimiter backoff BackoffManager timeout time.Duration maxRetries int // generic components accessible via method setters verb string pathPrefix string subpath string params url.Values headers http.Header // structural elements of the request that are part of the Kubernetes API conventions namespace string namespaceSet bool resource string resourceName string subresource string // output err error // only one of body / bodyBytes may be set. requests using body are not retriable. body io.Reader bodyBytes []byte retryFn requestRetryFunc&#125;// NewRequest 方法创建一个Request结构体并返回指针func NewRequest(c *RESTClient) *Request &#123; ... r := &amp;Request&#123; c: c, rateLimiter: c.rateLimiter, backoff: backoff, timeout: timeout, pathPrefix: pathPrefix, maxRetries: 10, retryFn: defaultRequestRetryFn, warningHandler: c.warningHandler, &#125; ... return r&#125;func (r *Request) Verb(verb string) *Request &#123; r.verb = verb return r&#125;func (r *Request) Prefix(segments ...string) *Request &#123; if r.err != nil &#123; return r &#125; r.pathPrefix = path.Join(r.pathPrefix, path.Join(segments...)) return r&#125;func (r *Request) Suffix(segments ...string) *Request &#123; if r.err != nil &#123; return r &#125; r.subpath = path.Join(r.subpath, path.Join(segments...)) return r&#125; 使用示例12r := NewRequest(clientSet)r.Verb("Post").Prefix("apps") REF:1.client-go/rest/request.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个deployment是怎么创建的]]></title>
    <url>%2F2023%2F04%2F01%2F%E4%B8%80%E4%B8%AAdeployment%E6%98%AF%E6%80%8E%E4%B9%88%E5%88%9B%E5%BB%BA%E7%9A%84%2F</url>
    <content type="text"><![CDATA[从一个deployment到创建对应的资源(如Replicat,pod)主要包括这几个主件kubee-apiserver,kube-controller-manager(deployment-controller,replicaset-controller),kubelet,kube-schedule。 k8s中kube-apiserver负责接收请求，kube-controller-manager负责监听资源变化并作出相应动作，kube-schedule负责调度，kubelet负责把容器拉起来。 当你使用kubectl create 命令创建deploy会发生如下请求。12345➜ ✗ k create -f tt.yaml --v=6I0401 14:30:30.064130 1625050 loader.go:374] Config loaded from file: /home/xxxx/.kube/configI0401 14:30:30.094255 1625050 round_trippers.go:553] GET https://192.168.3.112:6443/openapi/v2?timeout=32s 200 OK in 29 millisecondsI0401 14:30:30.203951 1625050 round_trippers.go:553] POST https://192.168.3.112:6443/apis/apps/v1/namespaces/default/deployments?fieldManager=kubectl-create&amp;fieldValidation=Strict 201 Created in 5 millisecondsdeployment.apps/use created 从日志中可以发现kubectl调用了接口/apis/aapps/v1/namespaces/default/deployments 123456789101112131415161718192021222324252627282930// 在这里会创建一个新的对象staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.gofunc (e *Store) Create(ctx context.Context, obj runtime.Object, createValidation rest.ValidateObjectFunc, options *metav1.CreateOptions) (runtime.Object, error) &#123; if objectMeta, err := meta.Accessor(obj); err != nil &#123; return nil, err &#125; else &#123; rest.FillObjectMetaSystemFields(objectMeta) // 根据GenerateName设置对象名称 if len(objectMeta.GetGenerateName()) &gt; 0 &amp;&amp; len(objectMeta.GetName()) == 0 &#123; objectMeta.SetName(e.CreateStrategy.GenerateName(objectMeta.GetGenerateName())) &#125; &#125; ... // 执行对应的deployment strategy,比如一些验证 if err := rest.BeforeCreate(e.CreateStrategy, ctx, obj); err != nil &#123; return nil, err &#125; ... // 获取对象的名称，在这里是deployment name name, err := e.ObjectNameFunc(obj) ... // 获取资源对象对应的路径,因为我是在default命名空间下创建的，所以key = "/deployments/default/use" key, err := e.KeyFunc(ctx, name) ... // 创建一个空的deployment对象 out := e.NewFunc() // 创建对象，最终是往etcd中写入了对应的数据,此时对应的replicaset,pod并没有被创建 if err := e.Storage.Create(ctx, key, obj, out, ttl, dryrun.IsDryRun(options.DryRun)); err != nil &#123; ... &#125; deployment创建成功后(可以通过kubectl get deploy进行查看)，deployment controller就要开始干活了，那deployment controller是怎么知道有活干了呢？ deployment controller通过informer机制可以监听到事件变化，当创建一个deployment后，kube-apiserver会通过http分块传输将输给客户,在这里deployment controller可以看作是apiserver的客户端 deployment创建完成后，deployment就要开始工作创建对应的replicaset12345678910111213141516171819202122232425262728293031323334353637383940414243// pkg/controller/deployment/deployment_controller.gofunc (dc *DeploymentController) Run(ctx context.Context, workers int) &#123; defer utilruntime.HandleCrash() // Start events processing pipeline. dc.eventBroadcaster.StartStructuredLogging(0) dc.eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: dc.client.CoreV1().Events("")&#125;) defer dc.eventBroadcaster.Shutdown() defer dc.queue.ShutDown() logger := klog.FromContext(ctx) logger.Info("Starting controller", "controller", "deployment") defer logger.Info("Shutting down controller", "controller", "deployment") if !cache.WaitForNamedCacheSync("deployment", ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.UntilWithContext(ctx, dc.worker, time.Second) &#125; &lt;-ctx.Done()&#125;func (dc *DeploymentController) worker(ctx context.Context) &#123; for dc.processNextWorkItem(ctx) &#123; &#125;&#125;func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool &#123; key, quit := dc.queue.Get() if quit &#123; return false &#125; defer dc.queue.Done(key) err := dc.syncHandler(ctx, key.(string)) dc.handleErr(ctx, err, key) return true&#125; 真正执行逻辑的函数是1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error &#123; logger := klog.FromContext(ctx) namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; klog.ErrorS(err, "Failed to split meta namespace cache key", "cacheKey", key) return err &#125; startTime := time.Now() logger.V(4).Info("Started syncing deployment", "deployment", klog.KRef(namespace, name), "startTime", startTime) defer func() &#123; logger.V(4).Info("Finished syncing deployment", "deployment", klog.KRef(namespace, name), "duration", time.Since(startTime)) &#125;() deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) &#123; logger.V(2).Info("Deployment has been deleted", "deployment", klog.KRef(namespace, name)) return nil &#125; if err != nil &#123; return err &#125; // Deep-copy otherwise we are mutating our cache. // TODO: Deep-copy only when needed. d := deployment.DeepCopy() everything := metav1.LabelSelector&#123;&#125; if reflect.DeepEqual(d.Spec.Selector, &amp;everything) &#123; dc.eventRecorder.Eventf(d, v1.EventTypeWarning, "SelectingAll", "This deployment is selecting all pods. A non-empty selector is required.") if d.Status.ObservedGeneration &lt; d.Generation &#123; d.Status.ObservedGeneration = d.Generation dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions&#123;&#125;) &#125; return nil &#125; // List ReplicaSets owned by this Deployment, while reconciling ControllerRef // through adoption/orphaning. rsList, err := dc.getReplicaSetsForDeployment(ctx, d) if err != nil &#123; return err &#125; // List all Pods owned by this Deployment, grouped by their ReplicaSet. // Current uses of the podMap are: // // * check if a Pod is labeled correctly with the pod-template-hash label. // * check that no old Pods are running in the middle of Recreate Deployments. podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil &#123; return err &#125; if d.DeletionTimestamp != nil &#123; return dc.syncStatusOnly(ctx, d, rsList) &#125; // Update deployment conditions with an Unknown condition when pausing/resuming // a deployment. In this way, we can be sure that we won't timeout when a user // resumes a Deployment with a set progressDeadlineSeconds. if err = dc.checkPausedConditions(ctx, d); err != nil &#123; return err &#125; if d.Spec.Paused &#123; return dc.sync(ctx, d, rsList) &#125; // rollback is not re-entrant in case the underlying replica sets are updated with a new // revision so we should ensure that we won't proceed to update replica sets until we // make sure that the deployment has cleaned up its rollback spec in subsequent enqueues. if getRollbackTo(d) != nil &#123; return dc.rollback(ctx, d, rsList) &#125; scalingEvent, err := dc.isScalingEvent(ctx, d, rsList) if err != nil &#123; return err &#125; if scalingEvent &#123; return dc.sync(ctx, d, rsList) &#125; switch d.Spec.Strategy.Type &#123; case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(ctx, d, rsList, podMap) // 新建一个deployment会执行到这个逻辑，创建对应的ReplicaSet case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(ctx, d, rsList) &#125; return fmt.Errorf("unexpected deployment strategy type: %s", d.Spec.Strategy.Type)&#125; 12345678910111213141516171819202122232425262728293031323334353637// rolloutRolling会创建对应的ReplicaSetfunc (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil &#123; return err &#125; allRSs := append(oldRSs, newRS) // Scale up, if we can. scaledUp, err := dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d) if err != nil &#123; return err &#125; if scaledUp &#123; // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) &#125; // Scale down, if we can. scaledDown, err := dc.reconcileOldReplicaSets(ctx, allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil &#123; return err &#125; if scaledDown &#123; // Update DeploymentStatus return dc.syncRolloutStatus(ctx, allRSs, newRS, d) &#125; if deploymentutil.DeploymentComplete(d, &amp;d.Status) &#123; if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil &#123; return err &#125; &#125; // Sync deployment status return dc.syncRolloutStatus(ctx, allRSs, newRS, d)&#125; 创建完对应的replicaset之后，replicaset controller也会监听到相应的对象变化信息。现在来到replicaset controller 主要看syncReplicaSet函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// pkg/controller/replicaset/replica_set.gofunc (rsc *ReplicaSetController) syncReplicaSet(ctx context.Context, key string) error &#123; startTime := time.Now() defer func() &#123; klog.FromContext(ctx).V(4).Info("Finished syncing", "kind", rsc.Kind, "key", key, "duration", time.Since(startTime)) &#125;() namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if apierrors.IsNotFound(err) &#123; klog.FromContext(ctx).V(4).Info("deleted", "kind", rsc.Kind, "key", key) rsc.expectations.DeleteExpectations(key) return nil &#125; if err != nil &#123; return err &#125; rsNeedsSync := rsc.expectations.SatisfiedExpectations(key) selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector) if err != nil &#123; utilruntime.HandleError(fmt.Errorf("error converting pod selector to selector for rs %v/%v: %v", namespace, name, err)) return nil &#125; // list all pods to include the pods that don't match the rs`s selector // anymore but has the stale controller ref. // TODO: Do the List and Filter in a single pass, or use an index. allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) if err != nil &#123; return err &#125; // Ignore inactive pods. filteredPods := controller.FilterActivePods(allPods) // NOTE: filteredPods are pointing to objects from cache - if you need to // modify them, you need to copy it first. filteredPods, err = rsc.claimPods(ctx, rs, selector, filteredPods) if err != nil &#123; return err &#125; var manageReplicasErr error if rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil &#123; manageReplicasErr = rsc.manageReplicas(ctx, filteredPods, rs) &#125; rs = rs.DeepCopy() newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) // Always updates status as pods come up or die. updatedRS, err := updateReplicaSetStatus(klog.FromContext(ctx), rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) if err != nil &#123; // Multiple things could lead to this update failing. Requeuing the replica set ensures // Returning an error causes a requeue without forcing a hotloop return err &#125; // Resync the ReplicaSet after MinReadySeconds as a last line of defense to guard against clock-skew. if manageReplicasErr == nil &amp;&amp; updatedRS.Spec.MinReadySeconds &gt; 0 &amp;&amp; updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) &amp;&amp; updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) &#123; rsc.queue.AddAfter(key, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second) &#125; return manageReplicasErr&#125; 在manageReplicas中会调用 rsc.podControl.CreatePods(…)向apiserver发送一个请求创建对应的pod,接下来就是经过kube-scheduler调度，然后由kubelet将容器拉起 此外省略kube-schedule调度过程，跳至kubelet如何拉取一个容器1234567891011121314151617181920212223// pkg/kubelet/kubelet.go// 从多个通道读取变化然后分发给对应的handler,因为是新建pod所以执行handler.HandlePodAdditions(u.Pods)func (kl *Kubelet) syncLoopIteration(ctx context.Context, configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123; select &#123; case u, open := &lt;-configCh: // Update from a config source; dispatch it to the right handler // callback. if !open &#123; klog.ErrorS(nil, "Update channel is closed, exiting the sync loop") return false &#125; switch u.Op &#123; case kubetypes.ADD: klog.V(2).InfoS("SyncLoop ADD", "source", u.Source, "pods", klog.KObjSlice(u.Pods)) // After restarting, kubelet will get all existing pods through // ADD as if they are new pods. These pods will then go through the // admission process and *may* be rejected. This can be resolved // once we have checkpointing. handler.HandlePodAdditions(u.Pods) ...&#125; 进入到HandlePodAdditions12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// pkg/kubelet/kubelet.go// 删减了一些不重要的逻辑func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123; // 对pod根据创建时间排序 sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods &#123; existingPods := kl.podManager.GetPods() // 将pod加入podManager kl.podManager.AddPod(pod) // 如果是mirrorpod则进入mirrorpod的处理逻辑 if kubetypes.IsMirrorPod(pod) &#123; kl.handleMirrorPod(pod, start) continue &#125; // Only go through the admission process if the pod is not requested // for termination by another part of the kubelet. If the pod is already // using resources (previously admitted), the pod worker is going to be // shutting it down. If the pod hasn't started yet, we know that when // the pod worker is invoked it will also avoid setting up the pod, so // we simply avoid doing any work. if !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123; // We failed pods that we rejected, so activePods include all admitted // pods that are alive. activePods := kl.filterOutInactivePods(existingPods) if utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) &#123; // To handle kubelet restarts, test pod admissibility using AllocatedResources values // (for cpu &amp; memory) from checkpoint store. If found, that is the source of truth. podCopy := pod.DeepCopy() for _, c := range podCopy.Spec.Containers &#123; allocatedResources, found := kl.statusManager.GetContainerResourceAllocation(string(pod.UID), c.Name) if c.Resources.Requests != nil &amp;&amp; found &#123; c.Resources.Requests[v1.ResourceCPU] = allocatedResources[v1.ResourceCPU] c.Resources.Requests[v1.ResourceMemory] = allocatedResources[v1.ResourceMemory] &#125; &#125; // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, podCopy); !ok &#123; kl.rejectPod(pod, reason, message) continue &#125; // For new pod, checkpoint the resource values at which the Pod has been admitted if err := kl.statusManager.SetPodAllocation(podCopy); err != nil &#123; //TODO(vinaykul,InPlacePodVerticalScaling): Can we recover from this in some way? Investigate klog.ErrorS(err, "SetPodAllocation failed", "pod", klog.KObj(pod)) &#125; &#125; else &#123; // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123; kl.rejectPod(pod, reason, message) continue &#125; &#125; &#125; mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) &#125;&#125; 进入到dispatchWork123456789101112131415// pkg/kubelet/kubelet.go// 开始在pod worker中进行异步更新func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123; // Run the sync in an async worker. kl.podWorkers.UpdatePod(UpdatePodOptions&#123; Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, StartTime: start, &#125;) // Note the number of containers for new pods. if syncType == kubetypes.SyncPodCreate &#123; metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) &#125;&#125; 进入到UpdatePod1234567891011// pkg/kubelet/pod_workers.go// 这里会更新pod的一些状态，并开启一个协程进入podWorkerLoop// 上面说的异步更新就是指这里会开启一个新的协程func (p *podWorkers) UpdatePod(options UpdatePodOptions) &#123; // spawn a pod worker go func() &#123; defer runtime.HandleCrash() defer klog.V(3).InfoS("Pod worker has stopped", "podUID", uid) p.podWorkerLoop(uid, outCh) &#125;()&#125; 进入podWorkerLoop1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// pkg/kubelet/pod_workers.go// 最终会进入到SyncPodfunc (p *podWorkers) podWorkerLoop(podUID types.UID, podUpdates &lt;-chan struct&#123;&#125;) &#123; var lastSyncTime time.Time for range podUpdates &#123; ... var isTerminal bool err := func() error &#123; // The worker is responsible for ensuring the sync method sees the appropriate // status updates on resyncs (the result of the last sync), transitions to // terminating (no wait), or on terminated (whatever the most recent state is). // Only syncing and terminating can generate pod status changes, while terminated // pods ensure the most recent status makes it to the api server. var status *kubecontainer.PodStatus var err error switch &#123; case update.Options.RunningPod != nil: // when we receive a running pod, we don't need status at all because we are // guaranteed to be terminating and we skip updates to the pod default: // wait until we see the next refresh from the PLEG via the cache (max 2s) // TODO: this adds ~1s of latency on all transitions from sync to terminating // to terminated, and on all termination retries (including evictions). We should // improve latency by making the pleg continuous and by allowing pod status // changes to be refreshed when key events happen (killPod, sync-&gt;terminating). // Improving this latency also reduces the possibility that a terminated // container's status is garbage collected before we have a chance to update the // API server (thus losing the exit code). status, err = p.podCache.GetNewerThan(update.Options.Pod.UID, lastSyncTime) if err != nil &#123; // This is the legacy event thrown by manage pod loop all other events are now dispatched // from syncPodFn p.recorder.Eventf(update.Options.Pod, v1.EventTypeWarning, events.FailedSync, "error determining status: %v", err) return err &#125; &#125; // Take the appropriate action (illegal phases are prevented by UpdatePod) switch &#123; case update.WorkType == TerminatedPod: err = p.podSyncer.SyncTerminatedPod(ctx, update.Options.Pod, status) case update.WorkType == TerminatingPod: var gracePeriod *int64 if opt := update.Options.KillPodOptions; opt != nil &#123; gracePeriod = opt.PodTerminationGracePeriodSecondsOverride &#125; podStatusFn := p.acknowledgeTerminating(podUID) // if we only have a running pod, terminate it directly if update.Options.RunningPod != nil &#123; err = p.podSyncer.SyncTerminatingRuntimePod(ctx, update.Options.RunningPod) &#125; else &#123; err = p.podSyncer.SyncTerminatingPod(ctx, update.Options.Pod, status, gracePeriod, podStatusFn) &#125; default: isTerminal, err = p.podSyncer.SyncPod(ctx, update.Options.UpdateType, update.Options.Pod, update.Options.MirrorPod, status) &#125; lastSyncTime = p.clock.Now() return err &#125;() &#125;&#125; syncPod12345// pkg/kubelet/kubelet.gofunc (kl *Kubelet) SyncPod(_ context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) &#123; ... result := kl.containerRuntime.SyncPod(ctx, pod, podStatus, pullSecrets, kl.backOff) ... kl.containerRuntime.SyncPod123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244// pkg/kubelet/kuberuntime/kuberuntime_manager.go// 在这里会调用CRI创建对应的容器,CNI由CRI负责调用func (m *kubeGenericRuntimeManager) SyncPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123; // Step 1: Compute sandbox and container changes. podContainerChanges := m.computePodActions(ctx, pod, podStatus) klog.V(3).InfoS("computePodActions got for pod", "podActions", podContainerChanges, "pod", klog.KObj(pod)) if podContainerChanges.CreateSandbox &#123; ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil &#123; klog.ErrorS(err, "Couldn't make a ref to pod", "pod", klog.KObj(pod)) &#125; if podContainerChanges.SandboxID != "" &#123; m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, "Pod sandbox changed, it will be killed and re-created.") &#125; else &#123; klog.V(4).InfoS("SyncPod received new pod, will create a sandbox for it", "pod", klog.KObj(pod)) &#125; &#125; // Step 2: Kill the pod if the sandbox has changed. if podContainerChanges.KillPod &#123; if podContainerChanges.CreateSandbox &#123; klog.V(4).InfoS("Stopping PodSandbox for pod, will start new one", "pod", klog.KObj(pod)) &#125; else &#123; klog.V(4).InfoS("Stopping PodSandbox for pod, because all other containers are dead", "pod", klog.KObj(pod)) &#125; killResult := m.killPodWithSyncResult(ctx, pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), nil) result.AddPodSyncResult(killResult) if killResult.Error() != nil &#123; klog.ErrorS(killResult.Error(), "killPodWithSyncResult failed") return &#125; if podContainerChanges.CreateSandbox &#123; m.purgeInitContainers(ctx, pod, podStatus) &#125; &#125; else &#123; // Step 3: kill any running containers in this pod which are not to keep. for containerID, containerInfo := range podContainerChanges.ContainersToKill &#123; klog.V(3).InfoS("Killing unwanted container for pod", "containerName", containerInfo.name, "containerID", containerID, "pod", klog.KObj(pod)) killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name) result.AddSyncResult(killContainerResult) if err := m.killContainer(ctx, pod, containerID, containerInfo.name, containerInfo.message, containerInfo.reason, nil); err != nil &#123; killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error()) klog.ErrorS(err, "killContainer for pod failed", "containerName", containerInfo.name, "containerID", containerID, "pod", klog.KObj(pod)) return &#125; &#125; &#125; // Keep terminated init containers fairly aggressively controlled // This is an optimization because container removals are typically handled // by container garbage collector. m.pruneInitContainersBeforeStart(ctx, pod, podStatus) // We pass the value of the PRIMARY podIP and list of podIPs down to // generatePodSandboxConfig and generateContainerConfig, which in turn // passes it to various other functions, in order to facilitate functionality // that requires this value (hosts file and downward API) and avoid races determining // the pod IP in cases where a container requires restart but the // podIP isn't in the status manager yet. The list of podIPs is used to // generate the hosts file. // // We default to the IPs in the passed-in pod status, and overwrite them if the // sandbox needs to be (re)started. var podIPs []string if podStatus != nil &#123; podIPs = podStatus.IPs &#125; // Step 4: Create a sandbox for the pod if necessary. podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox &#123; var msg string var err error klog.V(4).InfoS("Creating PodSandbox for pod", "pod", klog.KObj(pod)) metrics.StartedPodsTotal.Inc() createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod)) result.AddSyncResult(createSandboxResult) // ConvertPodSysctlsVariableToDotsSeparator converts sysctl variable // in the Pod.Spec.SecurityContext.Sysctls slice into a dot as a separator. // runc uses the dot as the separator to verify whether the sysctl variable // is correct in a separate namespace, so when using the slash as the sysctl // variable separator, runc returns an error: "sysctl is not in a separate kernel namespace" // and the podSandBox cannot be successfully created. Therefore, before calling runc, // we need to convert the sysctl variable, the dot is used as a separator to separate the kernel namespace. // When runc supports slash as sysctl separator, this function can no longer be used. sysctl.ConvertPodSysctlsVariableToDotsSeparator(pod.Spec.SecurityContext) // Prepare resources allocated by the Dynammic Resource Allocation feature for the pod if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) &#123; if m.runtimeHelper.PrepareDynamicResources(pod) != nil &#123; return &#125; &#125; podSandboxID, msg, err = m.createPodSandbox(ctx, pod, podContainerChanges.Attempt) if err != nil &#123; // createPodSandbox can return an error from CNI, CSI, // or CRI if the Pod has been deleted while the POD is // being created. If the pod has been deleted then it's // not a real error. // // SyncPod can still be running when we get here, which // means the PodWorker has not acked the deletion. if m.podStateProvider.IsPodTerminationRequested(pod.UID) &#123; klog.V(4).InfoS("Pod was deleted and sandbox failed to be created", "pod", klog.KObj(pod), "podUID", pod.UID) return &#125; metrics.StartedPodsErrorsTotal.Inc() createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg) klog.ErrorS(err, "CreatePodSandbox for pod failed", "pod", klog.KObj(pod)) ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil &#123; klog.ErrorS(referr, "Couldn't make a ref to pod", "pod", klog.KObj(pod)) &#125; m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, "Failed to create pod sandbox: %v", err) return &#125; klog.V(4).InfoS("Created PodSandbox for pod", "podSandboxID", podSandboxID, "pod", klog.KObj(pod)) resp, err := m.runtimeService.PodSandboxStatus(ctx, podSandboxID, false) if err != nil &#123; ref, referr := ref.GetReference(legacyscheme.Scheme, pod) if referr != nil &#123; klog.ErrorS(referr, "Couldn't make a ref to pod", "pod", klog.KObj(pod)) &#125; m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedStatusPodSandBox, "Unable to get pod sandbox status: %v", err) klog.ErrorS(err, "Failed to get pod sandbox status; Skipping pod", "pod", klog.KObj(pod)) result.Fail(err) return &#125; if resp.GetStatus() == nil &#123; result.Fail(errors.New("pod sandbox status is nil")) return &#125; // If we ever allow updating a pod from non-host-network to // host-network, we may use a stale IP. if !kubecontainer.IsHostNetworkPod(pod) &#123; // Overwrite the podIPs passed in the pod status, since we just started the pod sandbox. podIPs = m.determinePodSandboxIPs(pod.Namespace, pod.Name, resp.GetStatus()) klog.V(4).InfoS("Determined the ip for pod after sandbox changed", "IPs", podIPs, "pod", klog.KObj(pod)) &#125; &#125; // the start containers routines depend on pod ip(as in primary pod ip) // instead of trying to figure out if we have 0 &lt; len(podIPs) // everytime, we short circuit it here podIP := "" if len(podIPs) != 0 &#123; podIP = podIPs[0] &#125; // Get podSandboxConfig for containers to start. configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) if err != nil &#123; message := fmt.Sprintf("GeneratePodSandboxConfig for pod %q failed: %v", format.Pod(pod), err) klog.ErrorS(err, "GeneratePodSandboxConfig for pod failed", "pod", klog.KObj(pod)) configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message) return &#125; // Helper containing boilerplate common to starting all types of containers. // typeName is a description used to describe this type of container in log messages, // currently: "container", "init container" or "ephemeral container" // metricLabel is the label used to describe this type of container in monitoring metrics. // currently: "container", "init_container" or "ephemeral_container" start := func(ctx context.Context, typeName, metricLabel string, spec *startSpec) error &#123; startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name) result.AddSyncResult(startContainerResult) isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff) if isInBackOff &#123; startContainerResult.Fail(err, msg) klog.V(4).InfoS("Backing Off restarting container in pod", "containerType", typeName, "container", spec.container, "pod", klog.KObj(pod)) return err &#125; metrics.StartedContainersTotal.WithLabelValues(metricLabel).Inc() if sc.HasWindowsHostProcessRequest(pod, spec.container) &#123; metrics.StartedHostProcessContainersTotal.WithLabelValues(metricLabel).Inc() &#125; klog.V(4).InfoS("Creating container in pod", "containerType", typeName, "container", spec.container, "pod", klog.KObj(pod)) // NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs. if msg, err := m.startContainer(ctx, podSandboxID, podSandboxConfig, spec, pod, podStatus, pullSecrets, podIP, podIPs); err != nil &#123; // startContainer() returns well-defined error codes that have reasonable cardinality for metrics and are // useful to cluster administrators to distinguish "server errors" from "user errors". metrics.StartedContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc() if sc.HasWindowsHostProcessRequest(pod, spec.container) &#123; metrics.StartedHostProcessContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc() &#125; startContainerResult.Fail(err, msg) // known errors that are logged in other places are logged at higher levels here to avoid // repetitive log spam switch &#123; case err == images.ErrImagePullBackOff: klog.V(3).InfoS("Container start failed in pod", "containerType", typeName, "container", spec.container, "pod", klog.KObj(pod), "containerMessage", msg, "err", err) default: utilruntime.HandleError(fmt.Errorf("%v %+v start failed in pod %v: %v: %s", typeName, spec.container, format.Pod(pod), err, msg)) &#125; return err &#125; return nil &#125; // Step 5: start ephemeral containers // These are started "prior" to init containers to allow running ephemeral containers even when there // are errors starting an init container. In practice init containers will start first since ephemeral // containers cannot be specified on pod creation. for _, idx := range podContainerChanges.EphemeralContainersToStart &#123; start(ctx, "ephemeral container", metrics.EphemeralContainer, ephemeralContainerStartSpec(&amp;pod.Spec.EphemeralContainers[idx])) &#125; // Step 6: start the init container. if container := podContainerChanges.NextInitContainerToStart; container != nil &#123; // Start the next init container. if err := start(ctx, "init container", metrics.InitContainer, containerStartSpec(container)); err != nil &#123; return &#125; // Successfully started the container; clear the entry in the failure klog.V(4).InfoS("Completed init container for pod", "containerName", container.Name, "pod", klog.KObj(pod)) &#125; // Step 7: For containers in podContainerChanges.ContainersToUpdate[CPU,Memory] list, invoke UpdateContainerResources if isInPlacePodVerticalScalingAllowed(pod) &#123; if len(podContainerChanges.ContainersToUpdate) &gt; 0 || podContainerChanges.UpdatePodResources &#123; m.doPodResizeAction(pod, podStatus, podContainerChanges, result) &#125; &#125; // Step 8: start containers in podContainerChanges.ContainersToStart. for _, idx := range podContainerChanges.ContainersToStart &#123; start(ctx, "container", metrics.Container, containerStartSpec(&amp;pod.Spec.Containers[idx])) &#125; return&#125; 1234567deployment conroller 入口pkg/controller/deployment/deployment_controller.go:157func (dc *DeploymentController) Run(ctx context.Context, workers int)replicaset controllerpkg/controller/replicaset/replica_set.go:190func (rsc *ReplicaSetController) Run(ctx context.Context, workers int) REF:1.https://github.com/kubernetes/kubernetes]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[golang http.Get详解]]></title>
    <url>%2F2023%2F03%2F13%2Fgolang-http-Get%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[从一道题目开始, 如果你对这些输出不是很理解。接下我们将一步一步debug源码彻底搞懂这些问题。1234567891011121314151617181920212223242526func main() &#123; for i := 0; i &lt; 3; i++ &#123; resp,_:=http.Get("https://www.baidu.com") _, _ = ioutil.ReadAll(resp.Body) &#125; fmt.Printf("Num of Goroutines %d\n", runtime.NumGoroutine()) // 3，readLoop + writeLoop + main&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; resp,_:=http.Get("https://www.baidu.com") resp.Body.Close() &#125; fmt.Printf("Num of Goroutines %d\n", runtime.NumGoroutine()) // 3，readLoop + writeLoop + main&#125;func main() &#123; for i := 0; i &lt; 3; i++ &#123; http.Get("https://www.baidu.com") &#125; fmt.Printf("Num of Goroutines %d\n", runtime.NumGoroutine()) // 7, 3 readLoop + 3 writeLoop + main = 7&#125; 本次主要涉及到如下三个文件net/http/client.gonet/http/request.gonet/http/transport.go http.Client：该类型表示可以发起 HTTP 请求的 HTTP 客户端。它提供了 Get、Post 和 Do 等方法，用于使用不同的 HTTP 方法发起 HTTP 请求。它还可以配置自定义的 http.Transport 对象，以控制底层网络连接。 http.Request：该类型表示可以被 HTTP 客户端发送的 HTTP 请求。它提供了 Method、URL 和 Header 等字段，用于设置请求的 HTTP 方法、URL 和 HTTP 头部。它还提供了设置请求体的方法。 http.Transport：该类型表示 HTTP 客户端用于发起 HTTP 请求的传输层,它负责TCP连接的创建和维护。它提供了配置底层网络连接的选项，例如最大空闲连接数、最大空闲连接时长和连接超时。http.Client 类型使用 http.Transport 的实例来发起 HTTP 请求。 http.Get的调用链如下 1234567891011121314- http.Get - DefaultClient.Get - c.Do(req) - c.do(req) - c.send(req,...) - send(...) - rt.RoundTrip(req) - t.roundTrip(req) - t.getConn(req, ...) - t.queueForDial(w) - t.dialConnFor - t.dialConn(w.ctx, w.cm) - go pconn.readLoop() - go pconn.writeLoop() 1234567891011121314151617func (t *Transport) dialConn(ctx context.Context, cm connectMethod) (pconn *persistConn, err error) &#123; pconn = &amp;persistConn&#123; t: t, cacheKey: cm.key(), reqch: make(chan requestAndChan, 1), writech: make(chan writeRequest, 1), closech: make(chan struct&#123;&#125;), writeErrCh: make(chan error, 1), writeLoopDone: make(chan struct&#123;&#125;), &#125; ... ... go pconn.readLoop() go pconn.writeLoop() return pconn, nil&#125; 可以看到对于每一个链接都会启动一个goroutine从链接中读取数据和一个goroutine往链接中写数据。问题来了这两个goroutine什么时候退出呢？1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (pc *persistConn) readLoop() &#123; closeErr := errReadLoopExiting // default value, if not changed below defer func() &#123; pc.close(closeErr) pc.t.removeIdleConn(pc) &#125;() alive := true for alive &#123; waitForBodyRead := make(chan bool, 2) body := &amp;bodyEOFSignal&#123; body: resp.Body, earlyCloseFn: func() error &#123; waitForBodyRead &lt;- false &lt;-eofc // will be closed by deferred call at the end of the function return nil &#125;, fn: func(err error) error &#123; isEOF := err == io.EOF waitForBodyRead &lt;- isEOF if isEOF &#123; &lt;-eofc // see comment above eofc declaration &#125; else if err != nil &#123; if cerr := pc.canceled(); cerr != nil &#123; return cerr &#125; &#125; return err &#125;, &#125; ... ... // 当alive=false时，readLoop()会结束运行 select &#123; // 当waitForBodyRead值为false的时候程序退出 // 当执行earlyCloseFn时，才会给waitForBodyRead赋值为false case bodyEOF := &lt;-waitForBodyRead: pc.t.setReqCanceler(rc.req, nil) alive = alive &amp;&amp; bodyEOF &amp;&amp; !pc.sawEOF &amp;&amp; pc.wroteRequest() &amp;&amp; tryPutIdleConn(trace) if bodyEOF &#123; eofc &lt;- struct&#123;&#125;&#123;&#125; &#125; case &lt;-rc.req.Cancel: alive = false pc.t.CancelRequest(rc.req) case &lt;-rc.req.Context().Done(): alive = false pc.t.cancelRequest(rc.req, rc.req.Context().Err()) case &lt;-pc.closech: alive = false &#125; &#125;&#125; 我们来看下什么会执行earlyCloseFn函数和fn函数fn函数在调用ioutil.ReadAll(resp.Body)时会被执行，而earlyCloseFn会在调用resp.Body.Close()时执行earlyCoseFn()。123456789101112131415161718192021222324252627282930313233func (es *bodyEOFSignal) Read(p []byte) (n int, err error) &#123; es.mu.Lock() closed, rerr := es.closed, es.rerr es.mu.Unlock() if closed &#123; return 0, errReadOnClosedResBody &#125; if rerr != nil &#123; return 0, rerr &#125; n, err = es.body.Read(p) if err != nil &#123; es.mu.Lock() defer es.mu.Unlock() if es.rerr == nil &#123; es.rerr = err &#125; err = es.condfn(err) &#125; return&#125;// 执行fn函数，会往waitForBodyRead发送true值// 此时readLoop()并不会退出，但会把这个连接重新放回到连接池中(只有当数据被正常的读取完)func (es *bodyEOFSignal) condfn(err error) error &#123; if es.fn == nil &#123; return err &#125; err = es.fn(err) es.fn = nil return err&#125; 1234567891011121314// 当earlyCloseFn != nil 和错误不等于io.EOF时会执行earlyCloseFn函数func (es *bodyEOFSignal) Close() error &#123; es.mu.Lock() defer es.mu.Unlock() if es.closed &#123; return nil &#125; es.closed = true if es.earlyCloseFn != nil &amp;&amp; es.rerr != io.EOF &#123; return es.earlyCloseFn() &#125; err := es.body.Close() return es.condfn(err)&#125; 123456789101112131415161718192021222324252627282930313233343536373839// 当readLoop()退出时，会close channel,writeLoop()接收到后便会退出func (pc *persistConn) writeLoop() &#123; defer close(pc.writeLoopDone) for &#123; select &#123; case wr := &lt;-pc.writech: startBytesWritten := pc.nwrite err := wr.req.Request.write(pc.bw, pc.isProxy, wr.req.extra, pc.waitForContinue(wr.continueCh)) if bre, ok := err.(requestBodyReadError); ok &#123; err = bre.error // Errors reading from the user&apos;s // Request.Body are high priority. // Set it here before sending on the // channels below or calling // pc.close() which tears town // connections and causes other // errors. wr.req.setError(err) &#125; if err == nil &#123; err = pc.bw.Flush() &#125; if err != nil &#123; wr.req.Request.closeBody() if pc.nwrite == startBytesWritten &#123; err = nothingWrittenError&#123;err&#125; &#125; &#125; pc.writeErrCh &lt;- err // to the body reader, which might recycle us wr.ch &lt;- err // to the roundTrip function if err != nil &#123; pc.close(err) return &#125; case &lt;-pc.closech: return &#125; &#125;&#125; 总结： 如果调用ioutil.ReadAll读取了resp.Body,即使不调用resp.Body.Close()也不会出现泄漏,为什么建议每次都要调用resp.Body.Close()主要是因为处理异常的情况，当出现异常情况时，会往waitForBodyRead发送一个false使readLoop退出。 如果没有读取连接中的数据也没有调用resp.Body.Close()，会导致readLoop,writeLoop不会退出从而产生泄漏。每次http.Get都会产生两个goroutine 如果复用了连接将不会产生新的readLoop和writeLoop goroutine。]]></content>
  </entry>
  <entry>
    <title><![CDATA[不同的进程可以同时绑定同一个端口吗]]></title>
    <url>%2F2023%2F03%2F01%2F%E4%B8%8D%E5%90%8C%E7%9A%84%E8%BF%9B%E7%A8%8B%E5%8F%AF%E4%BB%A5%E5%90%8C%E6%97%B6%E7%BB%91%E5%AE%9A%E5%90%8C%E4%B8%80%E4%B8%AA%E7%AB%AF%E5%8F%A3%E5%90%97%2F</url>
    <content type="text"><![CDATA[不同的进程可以同时绑定同一个端口吗?当听到这个问题的时候，你的第一反应是不是肯定不行啊(如果你在工作中遇到过address already in use的情况)。大家都知道tcp通过四元组来确定一个唯一的连接，所以说理论上说不同的进程应该不能使用同一个端口。 其实在Linux3.9内核中引入了一个socket选项SO_REUSEPORT,如果在创建socket的时候添加了这个选项，就允许多个进程绑定到同一个端口。 多个进程监听同一个端口，那服务端是怎么处理客户端连接呢？内核将以round-robin的方式在进程之间分配传入的连接，这意味着每个进程将依次接收相等数量的连接。 可以使用如下的代码，分别在不同的终端运行下面的代码对上面提出的问题进行验证。如果在不同的终端都可以运行这段代码说明不同的进程可以占用同一个端口；然后可以在客户端curl服务端,观察日志输出就知道连接了哪个进程。 1234567891011121314151617181920212223242526272829import socket# create a socketsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# set the SO_REUSEPORT optionsock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)# bind the socket to the portsock.bind((&apos;localhost&apos;, 12345))# listen for incoming connectionssock.listen()while True: # accept incoming connections conn, addr = sock.accept() print(f&quot;Connection from &#123;addr&#125;&quot;) # receive data from the client data = conn.recv(1024) print(f&quot;Received data: &#123;data.decode()&#125;&quot;) # send a response back to the client response = &quot;Hello, client!&quot; conn.send(response.encode()) # close the connection conn.close()]]></content>
  </entry>
  <entry>
    <title><![CDATA[rsync的使用]]></title>
    <url>%2F2022%2F04%2F05%2Frsync%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在进行大文件／多文件拷贝的时候如果使用cp命令的话会看不到进度，可以使用rsync，可以显示过程进度。1rsync -av --times --stats --checksum --human-readable --itemize-changes --progress --out-format=&quot;[%t] [%i] %&apos;&apos;&apos;b %-100n&quot; /mnt/share/Datasets /mnt/cephfs &gt; /tmp/dataset.log &amp; REF:1.http://www.jb51.cc/linux/402267.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[k8s/kubeflow排障]]></title>
    <url>%2F2022%2F04%2F05%2Fk8s-kubeflow%E6%8E%92%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[记录一下部署kubeflow过程中遇到的问题。 部署ingress-nginx失败记录kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml执行命令后提示如下错误：1time=&quot;2021-12-30T01:47:24Z&quot; level=fatal msg=&quot;error creating kubernetes client config: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory&quot; 解决方法:修改kube-apiserver.yaml,去掉—disable-admission-plugins=ServiceAccount,等待kube-apiserver重启，然后重新部署 Pod启动失败, “cni0” already has an IP address1Warning FailedCreatePodSandBox 35s (x137911 over 4d21h) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;de753a2dbc417b7a05f99044d63aa285dac37beddc2bcbe806c4d3ff1772d027&quot; network for pod &quot;coredns-78fcd69978-h4lqn&quot;: networkPlugin cni failed to set up pod &quot;coredns-78fcd69978-h4lqn_kube-system&quot; network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 10.63.0.1/24 解决方法:12ifconfig cni0 down ip link delete cni0 安装MySQL报错如下,chown: changing ownership of ‘/var/lib/postgresql/data’: Operation not permitted解决方法:12345678910111213# PV使用的是nfsvim /etc/exports# 增加 no_root_squash，修改为如下/nfs 10.1.55.0/24(rw,sync,no_subtree_check,no_root_squash)# 重启nfs server/etc/init.d/nfs-kernel-server restart# 修改deploy,增加readOnly: falsevolumeMounts:- name: mysql-storage mountPath: /var/lib/mysql readOnly: false kubeflow/cache-server 启动失败, secret “webhook-server-tls” not found1MountVolume.SetUp failed for volume &quot;webhook-tls-certs&quot; : secret &quot;webhook-server-tls&quot; not found 解决方法：1把其它pod的状态弄到 Running 状态后，重启Pod 问题解决 登录kubeflow webui界面，创建Notebook Server, 报错如下1No default Storage Class is set. Can&apos;t create new Disks for the new Notebook. Please use an Existing Disk. 解决方法：12# 因为集群中己经配置过storageclass,因此直接将这个storageclass设置为默认的storageclass就行kubectl patch storageclass managed-nfs-storage -p &apos;&#123;&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&apos; 创建notebook报错, 创建volume报错1kubeflow 403 could not find csrf cookie xsrf-token in the request 解决方法:123456789101112k edit deploy jupyter-web-app-deployment -n kubeflow# 添加如下环境变量- name: APP_SECURE_COOKIES value: &quot;false&quot; k edit deploy volumes-web-app-deployment -n kubeflow # 添加如下环境变量- name: APP_SECURE_COOKIES value: &quot;false&quot; kubeflow第一次登录未出现创建Namespace的页面解决方法:12345k edit deploy centraldashboard# 修改为如下- name: REGISTRATION_FLOW value: &quot;true&quot; 如在部署前可修改文件apps/centraldashboard/upstream/base/params.env1234CD_CLUSTER_DOMAIN=cluster.localCD_USERID_HEADER=kubeflow-useridCD_USERID_PREFIX=CD_REGISTRATION_FLOW=true coredns启动报错123456root@ai-10-1-55-12-dev-sz:~# k logs coredns-6c76c8bb89-g98cq -n kube-system.:53[INFO] plugin/reload: Running configuration MD5 = db32ca3650231d74073ff4cf814959a7CoreDNS-1.7.0linux/amd64, go1.14.4, f59c03d[FATAL] plugin/loop: Loop (127.0.0.1:40293 -&gt; :53) detected for zone &quot;.&quot;, see https://coredns.io/plugins/loop#troubleshooting. Query: &quot;HINFO 4247289002016518455.6908858903734866660.&quot; 解决方法: 删除/etc/resolv.conf中的本地地址nameserver 127.0.0.53 安装kubeflow 之后,从istio-ingressgateway进入的所有流量都会重定向到/auth/dex，带来了很多问题。解决方法:1234567891011121314151617181920212223242526272829303132333435k get envoyfilter -n istio-systemk delete envoyfilter authn-filter -n istio-systemk edit cm istio -n istio-system # 新增如下内容,保存然后重启istioddata: mesh: |- extensionProviders: - name: &quot;dex-auth-provider&quot; envoyExtAuthzHttp: service: &quot;authservice.istio-system.svc.cluster.local&quot; port: &quot;8080&quot; # The default port used by oauth2-proxy. includeHeadersInCheck: [&quot;authorization&quot;, &quot;cookie&quot;, &quot;x-auth-token&quot;] # headers sent to the oauth2-proxy in the check request. headersToUpstreamOnAllow: [&quot;kubeflow-userid&quot;] # headers sent to backend application when request is allowed.# 新建AuthorizationPolicy，kf-ap.yaml, k apply -f kf-ap.yamlapiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: dex-auth namespace: istio-system spec: selector: matchLabels: istio: ingressgateway action: CUSTOM provider: # The provider name must match the extension provider defined in the mesh config. name: dex-auth-provider rules: # The rules specify when to trigger the external authorizer. - to: - operation: hosts: [&quot;kubeflow.xxx.com&quot;] 通过ingress-nginx访问harbor dashboard正常，后改为通过istio-ingressgateway访问dashboard登录报40312022-01-21T06:43:14Z [DEBUG] [/lib/http/error.go:59]: &#123;&quot;errors&quot;:[&#123;&quot;code&quot;:&quot;FORBIDDEN&quot;,&quot;message&quot;:&quot;CSRF token invalid&quot;&#125;]&#125; 一开始以为是istio-ingressgateway的限制，所以查看了所有的AuthorizationPolicy,发现默认对istio-ingressgateway的流量是放行的。后查看harbor-core发现流量己经进入到集群内，直觉可能跟https有关重新安装harbor解决问题。解决方法:123helm uninstall harbor -n harbor#https -&gt; httphelm upgrade --cleanup-on-fail --install harbor . --namespace=harbor --set externalURL=http://tharbor.fiture.com Istio 容器一直处于创建中。MountVolume.SetUp failed for volume “istio-token”:failed to fetch token: the API server does not have TokenRequest endpoints enabled解决方法：123456vim /etc/manifests/kube-apiserver.yaml# 添加如下内容- --service-account-key-file=/etc/kubernetes/pki/sa.pub- --service-account-signing-key-file=/etc/kubernetes/pki/sa.key- --service-account-issuer=api- --service-account-api-audiences=api,vault,factors docker login,Error response from daemon: Get “https://tharbor.fiture.com/v2/“: dial tcp 10.1.55.16:443: connect: connection refused解决方法：1234添加:insecure-registriesvim /etc/docker/daemon.json insecure-registries: [&quot;http://harbor.com&quot;] 访问kubeflow dashboard显示空白解决方法：12k edit vs dex -n auth# host修改为对应的域名 登录kubeflow, 点击左侧菜单，显示Sorry, /jupyter/ is not a valid page解决方法:原因是vs dex和vs centeraldashboard配置了host: kubeflow.xx.com, 都删除掉host就行 创建notebook提示Insufficient nvidia.com/gpu。解决方法:123# 安装nvidia-device-plugink apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/master/nvidia-device-plugin.yml 当挂载目录文件过多导致pod启动失败, kubeflow notebook启动失败解决方法:1234567891011# 1.istio层面的原因k edit deploy istiod -n istio-system# 添加如下环境变量- name: ENABLE_LEGACY_FSGROUP_INJECTION value: &quot;false&quot; # 2.kubeflow的原因k edit deploy notebook-controller-deployment# 添加如下环境变量- name: ADD_FSGROUP value: &quot;false&quot; Cephfs mount error解决方法:12345在每个节点上安装cefs-commonapt install ceph-common # 对ceph key 进行base64加密https://github.com/kubernetes-retired/kube-deploy/issues/264#issuecomment-292815926 点击tensorboard发生404解决方法：删除tensorboard server.重新新建一个server,名称不能与notebook名称相同。 Metric-server not ready:k8s metrics server x509: cannot validate certificate for because it doesn’t contain any IP SANs”解决方法:12# Deployment 启动参数增加--kubelet-insecure-tlscontainers: - args: - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - ---kubelet-insecure-tls [Docker] 错误之Error response from daemon: could not select device driver ““ with capabilities: [[gpu]]解决方法:12apt-get install nvidia-container-runtimesystemctl restart docker ping gitlab.xxx.com偶尔出现Name or service not known。解决方法:12345678# 添加policy sequential k edit cm coredns -n kube-system forward . /etc/resolv.conf &#123; max_concurrent 1000 policy sequential &#125; # 然后删除corndns的pod]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>kubeflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyterhub启动容器失败]]></title>
    <url>%2F2021%2F12%2F26%2Fjupyterhub%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[在使用jupyterhub时候往启动的容器里挂载了一个文件很多的目录，导致容器启动报错如下。配置如下:1234567891011121314151617181920singleuser: defaultUrl: &quot;/lab&quot; extraEnv: JUPYTERHUB_SINGLEUSER_APP: &quot;jupyter_server.serverapp.ServerApp&quot; storage: capacity: 20Gi dynamic: storageClass: managed-nfs-storage-nfs-10 extraVolumes: - name: dataset persistentVolumeClaim: claimName: dataset-pvc - name: share persistentVolumeClaim: claimName: share-pvc extraVolumeMounts: - name: dataset mountPath: /Datasets - name: share mountPath: /home/jovyan/share Warning FailedMount 77s kubelet Unable to attach or mount volumes: unmounted volumes=[dataset], unattached volumes=[dataset volume-admin jupyterhub-share]: timed out waiting for the condition因为刚开始挂载了一个文件较小的目录是能正常启动的，当又挂载了一个文件很多的目录后，容器启动失败并提示如下错误。因为小目录是可以挂载成功的，所以可以排除，网络等原因，猜测可能是k8s的问题。 于是，自己写了个yaml文件并把pvc dataset 进行挂载，发现此时容器正常启动并无异常。123456789101112131415161718192021222324252627282930313233343536373839apiVersion: v1kind: Podmetadata: name: mount-testspec: containers: - name: mount-test image: gpu-jupyter:cu111-ubuntu20.04 command: - sleep - &quot;36000&quot; imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /DataSets name: dataset initContainers: - command: - iptables - -A - OUTPUT - -d - 169.254.169.254 - -j - DROP image: jupyterhub/k8s-network-tools:1.1.4 imagePullPolicy: IfNotPresent name: bbb securityContext: capabilities: add: - NET_ADMIN privileged: true runAsUser: 0 securityContext: fsGroup: 100 volumes: - name: dataset persistentVolumeClaim: claimName: dataset-pvc 于是和jupyterhub启动的容器yaml进行对比，然后不断添加配置，直到添加securityContext:fsGroup:100后导致容器启动失败。查看k8s文档，发现当指定了fsGroup后，当挂载文件时，k8s会对文件的所有权和权限进行检查和修改，导致容器启动缓慢，并会打印出mount 失败的日志。 解决方法：1.不挂载大目录2.删除fsGroup配置 修改jupyterhub的config配置，改为如下,然后helm upgrade123singleuser: fsGid: null... 1.k8s-security-context]]></content>
  </entry>
  <entry>
    <title><![CDATA[golang-struct-option-value]]></title>
    <url>%2F2021%2F07%2F30%2Fgolang-struct-option-val%2F</url>
    <content type="text"><![CDATA[在Golang中初始化struct之后如何优雅的设置一些字段的值.现在来学习下k8s源码中是如何做的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package mainimport "fmt"type Config struct &#123; a int b int res resource&#125;type resource struct &#123; kind string&#125;// Option接口type Option interface &#123; Apply(*Config)&#125;type aOption int// 为需要修改的字段实现一个Apply方法,具体修改struct中的字段内容func (o aOption) Apply(config *Config) &#123; config.a = int(o)&#125;// 返回aOption类型的值func WithA(a int) Option &#123; return aOption(a)&#125;type resourceOption struct &#123; res resource&#125;func (o resourceOption) Apply(config *Config) &#123; config.res = o.res&#125;func WithRes(res resource) Option&#123; return resourceOption&#123;res: res&#125;&#125;// newA opts为可选参数func newA(opts ...Option) *Config &#123; config := &amp;Config&#123; a: 0, b: 0, res: resource&#123; kind: "Cat", &#125;, &#125; // 执行额外的赋值操作,通过opts中的内容修改config中的值 for _, opt := range opts&#123; opt.Apply(config) &#125; return config&#125;func main() &#123; opts := []Option&#123;WithA(1), WithRes(resource&#123;kind: "Dog"&#125;)&#125; r := newA(opts...) fmt.Println(r) // &amp;&#123;1 0 &#123;Dog&#125;&#125;&#125; 另外一种类似的方法,相当于省略了Apply函数，将Apply函数的内容写在了WithClientSet函数中。123456789101112131415161718192021222324252627282930313233package mainimport "fmt"type frameworkOptions struct &#123; clientSet string&#125;type fOption func(*frameworkOptions)func WithClientSet(clientSet string) fOption &#123; return func(o *frameworkOptions) &#123; o.clientSet = clientSet &#125;&#125;func newFrameWork(opts ...fOption) *frameworkOptions &#123; fr := &amp;frameworkOptions&#123; clientSet: "Cat", &#125; // 执行额外的赋值操作,通过opts中的内容修改frameworkOptions中的值 for _,opt := range opts &#123; opt(fr) &#125; return fr&#125;func main() &#123; opts := []fOption&#123;WithClientSet("Dog")&#125; r := newFrameWork(options...) fmt.Println(r) // &amp;&#123;Dog&#125;&#125; REF:1.https://github.com/kubernetes/kubernetes/blob/master/vendor/go.opentelemetry.io/otel/sdk/metric/controller/basic/config.go]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s informer]]></title>
    <url>%2F2021%2F07%2F25%2Fk8s-informer%2F</url>
    <content type="text"><![CDATA[今天通过一张图和一段代码来完全掌握k8s中的informer机制。 图片来源 Reflector: 用于监听指定k8s资源的变化，并将事件推送到DeltaFIFO中.监听的对象需要实现ListAndWatch方法.监听的对象可以是k8s中的内置资源也可以自定义资源DeltaFIFO: 一个存储资源对象的先进先出队列Indexer: 自带索引功能的本地存储.可参考k8s-indexerk8s代码版本为V1.21 Reflector结构定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687// staging/src/k8s.io/client-go/tools/cache/reflector.go// Reflector 监听指定资源变化并将变化事件推送到指定存储type Reflector struct &#123; // 只列出部分字段 name string // 指定存储 // 需实现Add,Update,Delete,List,ListKeys,Get,GetByKey,Replace,Resync方法 store Store listerWatcher ListerWatcher ...&#125;// staging/src/k8s.io/client-go/tools/cache/listwatch.gotype Lister interface &#123; List(options metav1.ListOptions) (runtime.Object, error)&#125;type Watcher interface &#123; Watch(options metav1.ListOptions) (watch.Interface, error)&#125;type ListerWatcher interface &#123; Lister Watcher&#125;// ListFunc knows how to list resourcestype ListFunc func(options metav1.ListOptions) (runtime.Object, error)// WatchFunc knows how to watch resourcestype WatchFunc func(options metav1.ListOptions) (watch.Interface, error)// ListWatch knows how to list and watch a set of apiserver resources. It satisfies the ListerWatcher interface.// It is a convenience function for users of NewReflector, etc.// ListFunc and WatchFunc must not be niltype ListWatch struct &#123; ListFunc ListFunc WatchFunc WatchFunc // DisableChunking requests no chunking for this list watcher. DisableChunking bool&#125;// NewListWatchFromClient creates a new ListWatch from the specified client, resource, namespace and field selector.func NewListWatchFromClient(c Getter, resource string, namespace string, fieldSelector fields.Selector) *ListWatch &#123; optionsModifier := func(options *metav1.ListOptions) &#123; options.FieldSelector = fieldSelector.String() &#125; return NewFilteredListWatchFromClient(c, resource, namespace, optionsModifier)&#125;// NewFilteredListWatchFromClient creates a new ListWatch from the specified client, resource, namespace, and option modifier.// Option modifier is a function takes a ListOptions and modifies the consumed ListOptions. Provide customized modifier function// to apply modification to ListOptions with a field selector, a label selector, or any other desired options.func NewFilteredListWatchFromClient(c Getter, resource string, namespace string, optionsModifier func(options *metav1.ListOptions)) *ListWatch &#123; listFunc := func(options metav1.ListOptions) (runtime.Object, error) &#123; optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Do(context.TODO()). Get() &#125; watchFunc := func(options metav1.ListOptions) (watch.Interface, error) &#123; options.Watch = true optionsModifier(&amp;options) return c.Get(). Namespace(namespace). Resource(resource). VersionedParams(&amp;options, metav1.ParameterCodec). Watch(context.TODO()) &#125; return &amp;ListWatch&#123;ListFunc: listFunc, WatchFunc: watchFunc&#125;&#125;// List a set of apiserver resourcesfunc (lw *ListWatch) List(options metav1.ListOptions) (runtime.Object, error) &#123; // ListWatch is used in Reflector, which already supports pagination. // Don't paginate here to avoid duplication. return lw.ListFunc(options)&#125;// Watch a set of apiserver resourcesfunc (lw *ListWatch) Watch(options metav1.ListOptions) (watch.Interface, error) &#123; return lw.WatchFunc(options)&#125; DeltaFIFO结构定义123456789101112131415// staging/src/k8s.io/client-go/tools/cache/delta_fifo.gotype DeltaFIFO struct &#123; lock sync.RWMutex cond sync.Cond items map[string]Deltas queue []string ...&#125;type Delta struct &#123; Type DeltaType // 事件类型：string Object interface&#123;&#125; // 资源对象: k8s对象&#125;type Deltas []Delta 通过一段代码来搞清楚informer是怎么工作的12345678910111213141516171819202122232425262728293031func main() &#123; // 构建config config, err := clientcmd.BuildConfigFromFlags("","/root/.kube/config") if err != nil&#123; panic(err) &#125; clientset, err := kubernetes.NewForConfig(config) if err != nil&#123; panic(err) &#125; stopCh := make(chan struct&#123;&#125;) defer close(stopCh) // informer的工厂函数,返回的是sharedInformerFactory对象 factory := informers.NewSharedInformerFactory(clientset, time.Minute) // 创建Pod资源对象的Informer informer := factory.Core().V1().Pods().Informer() // 注册事件回调函数 informer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; p := obj.(*corev1.Pod) fmt.Println("add a pod:", p.Name) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; fmt.Println("update a pod") &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; fmt.Println("delete a pod") &#125;, &#125;) informer.Run(stopCh)&#125; 1.获取informer工厂函数 12345678910111213141516171819202122232425262728293031323334// client-go/informers/factory.go// type sharedInformerFactory struct &#123; client kubernetes.Interface namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc lock sync.Mutex defaultResync time.Duration customResync map[reflect.Type]time.Duration informers map[reflect.Type]cache.SharedIndexInformer // startedInformers is used for tracking which informers have been started. // This allows Start() to be called multiple times safely. startedInformers map[reflect.Type]bool&#125;// 调用NewSharedInformerFactoryWithOptions创建一个sharedInformerFactory并最先实例化func NewSharedInformerFactoryWithOptions(client kubernetes.Interface, defaultResync time.Duration, options ...SharedInformerOption) SharedInformerFactory &#123; factory := &amp;sharedInformerFactory&#123; client: client, namespace: v1.NamespaceAll, defaultResync: defaultResync, informers: make(map[reflect.Type]cache.SharedIndexInformer), startedInformers: make(map[reflect.Type]bool), customResync: make(map[reflect.Type]time.Duration), &#125; // Apply all options for _, opt := range options &#123; factory = opt(factory) &#125; return factory&#125; 12345678// SharedIndexInformer provides add and get Indexers ability based on SharedInformer.// 在SharedInformer的基础上提供了Indexers能力type SharedIndexInformer interface &#123; SharedInformer // AddIndexers add indexers to the informer before it starts. AddIndexers(indexers Indexers) error GetIndexer() Indexer&#125; 2.informer的创建流程:12345```golang// client-go/informers/factory.gofunc (f *sharedInformerFactory) Core() core.Interface &#123; return core.New(f, f.namespace, f.tweakListOptions)&#125; 12345678910111213141516// client-go/informers/core/interface.gotype group struct &#123; factory internalinterfaces.SharedInformerFactory namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc&#125;// New(core.New) returns a new Interface.func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface &#123; return &amp;group&#123;factory: f, namespace: namespace, tweakListOptions: tweakListOptions&#125;&#125;// V1func (g *group) V1() v1.Interface &#123; return v1.New(g.factory, g.namespace, g.tweakListOptions)&#125; 12345678910111213141516// client-go/informers/core/v1/interface.gotype version struct &#123; factory internalinterfaces.SharedInformerFactory namespace string tweakListOptions internalinterfaces.TweakListOptionsFunc&#125;// New(V1.New) returns a new Interface.func New(f internalinterfaces.SharedInformerFactory, namespace string, tweakListOptions internalinterfaces.TweakListOptionsFunc) Interface &#123; return &amp;version&#123;factory: f, namespace: namespace, tweakListOptions: tweakListOptions&#125;&#125;// Pods返回PodInformer实例func (v *version) Pods() PodInformer &#123; return &amp;podInformer&#123;factory: v.factory, namespace: v.namespace, tweakListOptions: v.tweakListOptions&#125;&#125; 12345// client-go/informers/core/v1/pod.gofunc (f *podInformer) Informer() cache.SharedIndexInformer &#123; return f.factory.InformerFor(&amp;corev1.Pod&#123;&#125;, f.defaultInformer)&#125; 123456789101112131415161718192021// client-go/informers/factory.gofunc (f *sharedInformerFactory) InformerFor(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer &#123; f.lock.Lock() defer f.lock.Unlock() informerType := reflect.TypeOf(obj) informer, exists := f.informers[informerType] if exists &#123; return informer &#125; resyncPeriod, exists := f.customResync[informerType] if !exists &#123; resyncPeriod = f.defaultResync &#125; informer = newFunc(f.client, resyncPeriod) f.informers[informerType] = informer return informer&#125; 3.informer是如何启动的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// client-go/tools/cache/shared_informer.gofunc (s *sharedIndexInformer) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() // 创建一个FIFO队列 fifo := NewDeltaFIFOWithOptions(DeltaFIFOOptions&#123; KnownObjects: s.indexer, EmitDeltaTypeReplaced: true, &#125;) cfg := &amp;Config&#123; Queue: fifo, // 设置ListerWatcher函数 ListerWatcher: s.listerWatcher, // 对象类型v1.Pod ObjectType: s.objectType, FullResyncPeriod: s.resyncCheckPeriod, RetryOnError: false, ShouldResync: s.processor.shouldResync, // 这里应该是设置事件回调函数 Process: s.HandleDeltas, WatchErrorHandler: s.watchErrorHandler, &#125; func() &#123; s.startedLock.Lock() defer s.startedLock.Unlock() // 根据上面的配置文件,创建一个Controller对象 s.controller = New(cfg) s.controller.(*controller).clock = s.clock s.started = true &#125;() // Separate stop channel because Processor should be stopped strictly after controller processorStopCh := make(chan struct&#123;&#125;) var wg wait.Group defer wg.Wait() // Wait for Processor to stop defer close(processorStopCh) // Tell Processor to stop // 启动两个goroutine wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run) wg.StartWithChannel(processorStopCh, s.processor.run) defer func() &#123; s.startedLock.Lock() defer s.startedLock.Unlock() s.stopped = true // Don't want any new listeners &#125;() // 运行控制器 s.controller.Run(stopCh)&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 进入到s.controller.Run(stopCh),在这里会创建Reflector对象// client-go/tools/cache/controller.go// s.controller.Run(stopCh)func (c *controller) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() go func() &#123; &lt;-stopCh c.config.Queue.Close() &#125;() // 创建Reflector r := NewReflector( c.config.ListerWatcher, c.config.ObjectType, c.config.Queue, c.config.FullResyncPeriod, ) r.ShouldResync = c.config.ShouldResync r.WatchListPageSize = c.config.WatchListPageSize r.clock = c.clock if c.config.WatchErrorHandler != nil &#123; r.watchErrorHandler = c.config.WatchErrorHandler &#125; c.reflectorMutex.Lock() c.reflector = r c.reflectorMutex.Unlock() var wg wait.Group // 以goroutine的形式启动Reflector wg.StartWithChannel(stopCh, r.Run) // 以goroutine的形式启动c.processLoop, wait.Until(c.processLoop, time.Second, stopCh) wg.Wait()&#125;// 从工作队列中获取元素,并进行处理func (c *controller) processLoop() &#123; for &#123; obj, err := c.config.Queue.Pop(PopProcessFunc(c.config.Process)) if err != nil &#123; if err == ErrFIFOClosed &#123; return &#125; if c.config.RetryOnError &#123; // This is the safe way to re-enqueue. c.config.Queue.AddIfNotPresent(obj) &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839// client-go/tools/cache/delta_fifo.go// 从队列中Pop元素，并执行用户自定义的事件回调函数func (f *DeltaFIFO) Pop(process PopProcessFunc) (interface&#123;&#125;, error) &#123; f.lock.Lock() defer f.lock.Unlock() for &#123; for len(f.queue) == 0 &#123; // When the queue is empty, invocation of Pop() is blocked until new item is enqueued. // When Close() is called, the f.closed is set and the condition is broadcasted. // Which causes this loop to continue and return from the Pop(). if f.closed &#123; return nil, ErrFIFOClosed &#125; f.cond.Wait() &#125; id := f.queue[0] f.queue = f.queue[1:] if f.initialPopulationCount &gt; 0 &#123; f.initialPopulationCount-- &#125; item, ok := f.items[id] if !ok &#123; // This should never happen klog.Errorf("Inconceivable! %q was in f.queue but not f.items; ignoring.", id) continue &#125; delete(f.items, id) // 用户自定义的事件 处理函数 err := process(item) if e, ok := err.(ErrRequeue); ok &#123; f.addIfNotPresent(id, item) err = e.Err &#125; // Don't need to copyDeltas here, because we're transferring // ownership to the caller. return item, err &#125;&#125; 让我们进入process(item)函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// client-go/tools/cache/shared_informer.go// 真正处理DeltaFIFO数据的函数// 这里会将获取到的数据存入indexer中(如果数据存在的话进行更新操作，否则直接插入)，完成后还会进行distribute操作// 将事件分发给监听者func (s *sharedIndexInformer) HandleDeltas(obj interface&#123;&#125;) error &#123; s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) &#123; switch d.Type &#123; case Sync, Replaced, Added, Updated: s.cacheMutationDetector.AddObject(d.Object) if old, exists, err := s.indexer.Get(d.Object); err == nil &amp;&amp; exists &#123; if err := s.indexer.Update(d.Object); err != nil &#123; return err &#125; isSync := false switch &#123; case d.Type == Sync: // Sync events are only propagated to listeners that requested resync isSync = true case d.Type == Replaced: if accessor, err := meta.Accessor(d.Object); err == nil &#123; if oldAccessor, err := meta.Accessor(old); err == nil &#123; // Replaced events that didn't change resourceVersion are treated as resync events // and only propagated to listeners that requested resync isSync = accessor.GetResourceVersion() == oldAccessor.GetResourceVersion() &#125; &#125; &#125; s.processor.distribute(updateNotification&#123;oldObj: old, newObj: d.Object&#125;, isSync) &#125; else &#123; if err := s.indexer.Add(d.Object); err != nil &#123; return err &#125; s.processor.distribute(addNotification&#123;newObj: d.Object&#125;, false) &#125; case Deleted: if err := s.indexer.Delete(d.Object); err != nil &#123; return err &#125; s.processor.distribute(deleteNotification&#123;oldObj: d.Object&#125;, false) &#125; &#125; return nil&#125;// 将事件分发给监听者func (p *sharedProcessor) distribute(obj interface&#123;&#125;, sync bool) &#123; p.listenersLock.RLock() defer p.listenersLock.RUnlock() if sync &#123; for _, listener := range p.syncingListeners &#123; listener.add(obj) &#125; &#125; else &#123; for _, listener := range p.listeners &#123; listener.add(obj) &#125; &#125;&#125; 那存量的pod列表信息是何时放入到DeltaFIFO中的呢,也就是说何时调用ListFunc？1234567891011121314151617181920212223// client-go/tools/cache/reflector.gofunc (r *Reflector) ListAndWatch(stopCh &lt;-chan struct&#123;&#125;) error &#123; ... // 此处会调用client-go/informers/core/v1/pod.go中NewFilteredPodInformer中的ListFunc,获取pod资源对象列表 pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) &#123; return r.listerWatcher.List(opts) &#125;)) // 此处将pod资源对象列表插入到DeltaFIFO中 if err := r.syncWith(items, resourceVersion); err != nil &#123; return fmt.Errorf("unable to sync list result: %v", err) &#125; ...&#125;// 将list接口中的数据存放到cache.DeltaFIFO中func (r *Reflector) syncWith(items []runtime.Object, resourceVersion string) error &#123; found := make([]interface&#123;&#125;, 0, len(items)) for _, item := range items &#123; found = append(found, item) &#125; // 对store中已存在的元素进行替换操作 return r.store.Replace(found, resourceVersion)&#125; 1234567891011121314151617181920212223242526272829303132// 真正执行入队的操作func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface&#123;&#125;) error &#123; id, err := f.KeyOf(obj) if err != nil &#123; return KeyError&#123;obj, err&#125; &#125; oldDeltas := f.items[id] // 入队 newDeltas := append(oldDeltas, Delta&#123;actionType, obj&#125;) // 去重 newDeltas = dedupDeltas(newDeltas) if len(newDeltas) &gt; 0 &#123; if _, exists := f.items[id]; !exists &#123; f.queue = append(f.queue, id) &#125; f.items[id] = newDeltas f.cond.Broadcast() &#125; else &#123; // This never happens, because dedupDeltas never returns an empty list // when given a non-empty list (as it is here). // If somehow it happens anyway, deal with it but complain. if oldDeltas == nil &#123; klog.Errorf("Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; ignoring", id, oldDeltas, obj) return nil &#125; klog.Errorf("Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; breaking invariant by storing empty Deltas", id, oldDeltas, obj) f.items[id] = newDeltas return fmt.Errorf("Impossible dedupDeltas for id=%q: oldDeltas=%#+v, obj=%#+v; broke DeltaFIFO invariant by storing empty Deltas", id, oldDeltas, obj) &#125; return nil&#125; 总结：使用IDE对文中的示例代码进行调试，因为这段代码会启动多个不会退出的协程，所以为调试增加了难度。执行informer.Run(stopCh)会运行多个goroutine,可以在对应的协程代码里面打断点进行调试，更好的理解informer机制。123456wg.StartWithChannel(processorStopCh, s.cacheMutationDetector.Run)wg.StartWithChannel(processorStopCh, s.processor.run)// 运行Reflector实例wg.StartWithChannel(stopCh, r.Run)wait.Until(c.processLoop, time.Second, stopCh) REF:1.https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md2.https://cloudnative.to/blog/client-go-informer-source-code]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-workqueue]]></title>
    <url>%2F2021%2F07%2F25%2Fk8s-workqueue%2F</url>
    <content type="text"><![CDATA[workqueue提供了如下特性1234- 公平: 队列中的元素以先进先出的方式进行处理- Stingy(翻译为:吝啬的): 在高并发情况下和一元素在处理之前添加了多次的情况下,队列中的元素只会被处理一次- 并发性:多消费者和生产者.支持元素被正在处理的情况下重新入队- 通知机制：ShutDown方法通过信号量通知队列不再接收新的元素 Kubernetes 中使用 WorkQueue 的好处主要有以下几点: 降低并发冲突：WorkQueue 可以确保每个任务都被串行处理，这意味着每个任务将独立地执行，而不会受到其他任务的干扰，从而降低并发冲突的可能性。 控制任务执行速率：使用 WorkQueue 可以限制并控制任务的执行速率，这对于资源敏感的应用程序和需要限制负载的场景非常有用。 实现重试逻辑：WorkQueue 支持任务重试，当一个任务失败时，它可以将任务重新排队，以便在稍后的时间再次尝试执行该任务，从而实现重试逻辑。 消除重复工作：使用 WorkQueue 可以有效地避免重复处理相同的任务，以提高应用程序的性能和效率。 Queue1234567891011// vendor/k8s.io/client-go/util/workqueue/queue.go// 定义了队列常用的方法 type Interface interface &#123; Add(item interface&#123;&#125;) // 添加一个元素到队列 Len() int // 元素个数 Get() (item interface&#123;&#125;, shutdown bool) // 获取一个元素,shutdown标记队列是否关闭 Done(item interface&#123;&#125;) // 标记一个元素已经处理完 ShutDown() // 关闭队列 ShutDownWithDrain() // 关闭队列,但是等待队列中的元素处理完 ShuttingDown() bool // 队列是否正在关闭&#125; 12345678910111213141516171819202122232425type empty struct&#123;&#125;type t interface&#123;&#125;type set map[t]empty// 队列的实现type Type struct &#123; // 实际存储元素的地方,此处的每个元素应该存在于dirty中而不存在于processing set queue []t // 所有需要被处理的item,类型是集合 dirty set // processing用于标记一个元素是否正在处理.在并发场景下这些元素也可能存在dirty set. // 当我们处理完这个事件之后会将其移除,然后检查其是否存在于dirty set,如果存在将其入队. processing set cond *sync.Cond shuttingDown bool metrics queueMetrics unfinishedWorkUpdatePeriod time.Duration clock clock.Clock&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// 入队操作func (q *Type) Add(item interface&#123;&#125;) &#123; q.cond.L.Lock() defer q.cond.L.Unlock() // 如果队列是处理shuttingDown状态下,直接返回，不再进行入队操作 if q.shuttingDown &#123; return &#125; // 如果dirty set存在这个元素，则不会再次入队,保证元素在处理前只会进行一次入队操作 if q.dirty.has(item) &#123; return &#125; q.metrics.add(item) // 插入dirty set q.dirty.insert(item) // 如果元素存在于processing set中,则返回 if q.processing.has(item) &#123; return &#125; // 将元素添加到队列中 q.queue = append(q.queue, item) q.cond.Signal() // 通知getter有新元素到来&#125;// 返回队列长度func (q *Type) Len() int &#123; q.cond.L.Lock() defer q.cond.L.Unlock() return len(q.queue)&#125;// Get取出队列头部元素func (q *Type) Get() (item interface&#123;&#125;, shutdown bool) &#123; q.cond.L.Lock() defer q.cond.L.Unlock() // 如果队列长度为0且处于shuttingDown状态下,阻塞 for len(q.queue) == 0 &amp;&amp; !q.shuttingDown &#123; // 接收到q.cond.Signal()后结束阻塞 q.cond.Wait() &#125; if len(q.queue) == 0 &#123; // We must be shutting down. return nil, true &#125; // 对队列头部元素进行出队操作 item, q.queue = q.queue[0], q.queue[1:] q.metrics.get(item) // 将元素插入processing set 并且将其从dirty set删除 q.processing.insert(item) q.dirty.delete(item) return item, false&#125;// 标记一个元素已被处理完func (q *Type) Done(item interface&#123;&#125;) &#123; q.cond.L.Lock() defer q.cond.L.Unlock() q.metrics.done(item) // 从processing set中移除item q.processing.delete(item) // 如果元素在处理过程中又插入了相同的元素，重新将元素入队 if q.dirty.has(item) &#123; q.queue = append(q.queue, item) q.cond.Signal() &#125;&#125;// ShutDown 如果队列处于shuttingDown状态下，将不会往队列中添加新的元素// 当工作的协程将队列中的元素处理完之后，它们将会退出func (q *Type) ShutDown() &#123; q.cond.L.Lock() defer q.cond.L.Unlock() q.shuttingDown = true q.cond.Broadcast()&#125;func (q *Type) ShuttingDown() bool &#123; q.cond.L.Lock() defer q.cond.L.Unlock() return q.shuttingDown&#125; 通过一个例子学习workqueue的工作原理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import ( "fmt" "k8s.io/client-go/util/workqueue" "sync")func main() &#123; var wg sync.WaitGroup down := make(chan struct&#123;&#125;) q := workqueue.New() q.Add(1) q.Add(2) q.Add(3) // 支持Stingy的特性的微妙之处就在于这,如果元素已经存在于队列中且不存在于processing set中，将不会入队. q.Add(1) // 执行完4次Add操作后，队列中的元素内容如下 // queue --&gt; [1, 2, 3] // dirty --&gt; &#123;1, 2, 3&#125; // processing --&gt; &#123;&#125; // queue --&gt; [2, 3] // dirty --&gt; &#123;1, 2, 3&#125; // processing --&gt; &#123;1&#125; item,_ := q.Get() wg.Add(1) go func() &#123; defer wg.Done() fmt.Println("processing",item) &lt;-down // 执行q.Done(item)之后的状态 // queue --&gt; [2, 3, 1] // dirty --&gt; &#123;1, 2, 3&#125; // processing --&gt; &#123;&#125; // 此时元素1重新入队 q.Done(item) fmt.Println("processed",item) &#125;() // 在q.Add(1)执行之前此时状态为 // queue --&gt; [2, 3] // dirty --&gt; &#123;2, 3&#125; // processing --&gt; &#123;1&#125; q.Add(1) // 在执行q.Add(1)之后此时状态为 // queue --&gt; [2, 3] // dirty --&gt; &#123;1, 2, 3&#125; // processing --&gt; &#123;1&#125; // queue --&gt; [2, 3] // dirty --&gt; &#123;1, 2, 3&#125; // processing --&gt; &#123;1&#125; down&lt;- struct&#123;&#125;&#123;&#125; wg.Wait() fmt.Println()&#125; 小结:Stingy这一特性在高并发情况下是通过互斥锁来保证元素只会被处理一次;而对于同一元素多次入队的情况，如果该元素还未被处理，则会直接丢弃。 DelayingQueue顺便看k8s中的延迟队列,延迟队列基于FIFO队列实现，在原有的基础上添加了AddAfter方法.在延迟一段时间后将元素插入到队列中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187// vendor/k8s.io/client-go/util/workqueue/delaying_queue.gotype DelayingInterface interface &#123; Interface // AddAfter adds an item to the workqueue after the indicated duration has passed AddAfter(item interface&#123;&#125;, duration time.Duration)&#125;// delayingType 提供延迟入队操作type delayingType struct &#123; // 嵌套普通队列queue Interface // clock tracks time for delayed firing clock clock.Clock // stopCh lets us signal a shutdown to the waiting loop stopCh chan struct&#123;&#125; // stopOnce guarantees we only signal shutdown a single time stopOnce sync.Once // heartbeat ensures we wait no more than maxWait before firing heartbeat clock.Ticker // waitingForAddCh is a buffered channel that feeds waitingForAdd waitingForAddCh chan *waitFor // metrics counts the number of retries metrics retryMetrics&#125;// newDelayingQueue 真正实例化延迟队列的代码func newDelayingQueue(clock clock.Clock, q Interface, name string) *delayingType &#123; ret := &amp;delayingType&#123; Interface: q, clock: clock, heartbeat: clock.NewTicker(maxWait), stopCh: make(chan struct&#123;&#125;), waitingForAddCh: make(chan *waitFor, 1000), metrics: newRetryMetrics(name), &#125; // 运行一个循环 go ret.waitingLoop() return ret&#125;// waitingLoop 不断的检查waitForPriorityQueue的元素func (q *delayingType) waitingLoop() &#123; defer utilruntime.HandleCrash() // 队里没有元素时等待 never := make(&lt;-chan time.Time) // Make a timer that expires when the item at the head of the waiting queue is ready var nextReadyAtTimer clock.Timer // 构造一个优先级队列 waitingForQueue := &amp;waitForPriorityQueue&#123;&#125; heap.Init(waitingForQueue) // 用于处理重复添加逻辑 waitingEntryByData := map[t]*waitFor&#123;&#125; for &#123; if q.Interface.ShuttingDown() &#123; return &#125; now := q.clock.Now() // Add ready entries for waitingForQueue.Len() &gt; 0 &#123; entry := waitingForQueue.Peek().(*waitFor) // 如果元素中的readyAt 在now之后，则退出 if entry.readyAt.After(now) &#123; break &#125; // 从优先级队列中取出顶部元素，添加到workqueue中 entry = heap.Pop(waitingForQueue).(*waitFor) q.Add(entry.data) delete(waitingEntryByData, entry.data) &#125; // Set up a wait for the first item's readyAt (if one exists) nextReadyAt := never if waitingForQueue.Len() &gt; 0 &#123; if nextReadyAtTimer != nil &#123; nextReadyAtTimer.Stop() &#125; entry := waitingForQueue.Peek().(*waitFor) // 返回一个定时器对象,时间间隔是：entry.readyAt-now nextReadyAtTimer = q.clock.NewTimer(entry.readyAt.Sub(now)) // 猜测这样做是不用一直占用cpu资源，在优先级最高的元素入队的时间未到之前可以挂起 nextReadyAt = nextReadyAtTimer.C() &#125; select &#123; case &lt;-q.stopCh: return case &lt;-q.heartbeat.C(): // continue the loop, which will add ready items case &lt;-nextReadyAt: // continue the loop, which will add ready items case waitEntry := &lt;-q.waitingForAddCh: // if waitEntry.readyAt.After(q.clock.Now()) &#123; insert(waitingForQueue, waitingEntryByData, waitEntry) &#125; else &#123; q.Add(waitEntry.data) &#125; drained := false for !drained &#123; select &#123; case waitEntry := &lt;-q.waitingForAddCh: if waitEntry.readyAt.After(q.clock.Now()) &#123; insert(waitingForQueue, waitingEntryByData, waitEntry) &#125; else &#123; q.Add(waitEntry.data) &#125; default: drained = true &#125; &#125; &#125; &#125;&#125;// 插入元素到优先级队列，如果元素已存在则更新其readyAtfunc insert(q *waitForPriorityQueue, knownEntries map[t]*waitFor, entry *waitFor) &#123; // if the entry already exists, update the time only if it would cause the item to be queued sooner existing, exists := knownEntries[entry.data] if exists &#123; if existing.readyAt.After(entry.readyAt) &#123; existing.readyAt = entry.readyAt heap.Fix(q, existing.index) &#125;做 return &#125; // 插入优先级队列 heap.Push(q, entry) knownEntries[entry.data] = entry&#125;// 用于记录入队的元素以及需要被入队的时间 type waitFor struct &#123; data t readyAt time.Time // 优先级队列(堆)中的索引 index int&#125;// waitForPriorityQueue 为一个优先级队列,实现了heap.Interface中的方法 // 值readyAt最小的元素是顶点(index 0).// Peek返回索引0中的元素// Pop 将元素从队列中移除// Push将元素插入到队列中type waitForPriorityQueue []*waitFor// AddAfter func (q *delayingType) AddAfter(item interface&#123;&#125;, duration time.Duration) &#123; // don't add if we're already shutting down if q.ShuttingDown() &#123; return &#125; q.metrics.retry() // immediately add things with no delay if duration &lt;= 0 &#123; q.Add(item) return &#125; select &#123; case &lt;-q.stopCh: // unblock if ShutDown() is called // 将元素添加到waitingForAddCh 中，此channel中的数据将在waitingLoop方法中被处理 case q.waitingForAddCh &lt;- &amp;waitFor&#123;data: item, readyAt: q.clock.Now().Add(duration)&#125;: &#125;&#125; 12345678910111213141516171819202122232425// waitForPriorityQueue实现了heap.Interface接口,最小堆// Push adds an item to the queue. Push should not be called directly; instead,// use `heap.Push`.func (pq *waitForPriorityQueue) Push(x interface&#123;&#125;) &#123; n := len(*pq) item := x.(*waitFor) item.index = n *pq = append(*pq, item)&#125;// Pop removes an item from the queue. Pop should not be called directly;// instead, use `heap.Pop`.func (pq *waitForPriorityQueue) Pop() interface&#123;&#125; &#123; n := len(*pq) item := (*pq)[n-1] item.index = -1 *pq = (*pq)[0:(n - 1)] return item&#125;// Peek returns the item at the beginning of the queue, without removing the// item or otherwise mutating the queue. It is safe to call directly.func (pq waitForPriorityQueue) Peek() interface&#123;&#125; &#123; return pq[0]&#125; RateLimitingQueue123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154// vendor/k8s.io/client-go/util/workqueue/rate_limiting_queue.go// RateLimitingInterface is an interface that rate limits items being added to the queue.type RateLimitingInterface interface &#123; DelayingInterface // AddRateLimited adds an item to the workqueue after the rate limiter says it's ok AddRateLimited(item interface&#123;&#125;) // Forget indicates that an item is finished being retried. Doesn't matter whether it's for perm failing // or for success, we'll stop the rate limiter from tracking it. This only clears the `rateLimiter`, you // still have to call `Done` on the queue. // 结束重试 Forget(item interface&#123;&#125;) // NumRequeues returns back how many times the item was requeued // 重新入队次数 NumRequeues(item interface&#123;&#125;) int&#125;// 实现RateLimitingInterface的结构体type rateLimitingType struct &#123; DelayingInterface rateLimiter RateLimiter&#125;// 入队操作func (q *rateLimitingType) AddRateLimited(item interface&#123;&#125;) &#123; q.DelayingInterface.AddAfter(item, q.rateLimiter.When(item))&#125;// 核心对象RateLimiter// vendor/k8s.io/client-go/util/workqueue/default_rate_limiters.gotype RateLimiter interface &#123; // When gets an item and gets to decide how long that item should wait When(item interface&#123;&#125;) time.Duration // Forget indicates that an item is finished being retried. Doesn't matter whether it's for failing // or for success, we'll stop tracking it Forget(item interface&#123;&#125;) // NumRequeues returns back how many failures the item has had NumRequeues(item interface&#123;&#125;) int&#125;// 这个接口有５个实现// BucketRateLimiter// 使用golang.org/x/time/rate.Limiter实现type BucketRateLimiter struct &#123; *rate.Limiter&#125;var _ RateLimiter = &amp;BucketRateLimiter&#123;&#125;func (r *BucketRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; return r.Limiter.Reserve().Delay()&#125;func (r *BucketRateLimiter) NumRequeues(item interface&#123;&#125;) int &#123; return 0&#125;func (r *BucketRateLimiter) Forget(item interface&#123;&#125;) &#123;&#125;// ItemExponentialFailureRateLimiter// 失败次数越多，间隔时间越长type ItemExponentialFailureRateLimiter struct &#123; failuresLock sync.Mutex failures map[interface&#123;&#125;]int baseDelay time.Duration maxDelay time.Duration&#125;func (r *ItemExponentialFailureRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; r.failuresLock.Lock() defer r.failuresLock.Unlock() exp := r.failures[item] r.failures[item] = r.failures[item] + 1 // The backoff is capped such that 'calculated' value never overflows. backoff := float64(r.baseDelay.Nanoseconds()) * math.Pow(2, float64(exp)) if backoff &gt; math.MaxInt64 &#123; return r.maxDelay &#125; calculated := time.Duration(backoff) if calculated &gt; r.maxDelay &#123; return r.maxDelay &#125; return calculated&#125;// ItemFastSlowRateLimiter// 快慢指的是定义一个阈值，达到阈值之前快速重试，超过了就慢慢重试type ItemFastSlowRateLimiter struct &#123; failuresLock sync.Mutex failures map[interface&#123;&#125;]int maxFastAttempts int fastDelay time.Duration slowDelay time.Duration&#125;func (r *ItemFastSlowRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; r.failuresLock.Lock() defer r.failuresLock.Unlock() r.failures[item] = r.failures[item] + 1 if r.failures[item] &lt;= r.maxFastAttempts &#123; return r.fastDelay &#125; return r.slowDelay&#125;// MaxOfRateLimiter// 过维护多个限速器列表，然后返回其中限速最严格的一个延时// MaxOfRateLimiter calls every RateLimiter and returns the worst case response// When used with a token bucket limiter, the burst could be apparently exceeded in cases where particular items// were separately delayed a longer time.type MaxOfRateLimiter struct &#123; limiters []RateLimiter&#125;func (r *MaxOfRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; ret := time.Duration(0) for _, limiter := range r.limiters &#123; curr := limiter.When(item) if curr &gt; ret &#123; ret = curr &#125; &#125; return ret&#125;// WithMaxWaitRateLimiter// 加入最大延迟属性，如果到了最大延时则返回type WithMaxWaitRateLimiter struct &#123; limiter RateLimiter maxDelay time.Duration&#125;func (w WithMaxWaitRateLimiter) When(item interface&#123;&#125;) time.Duration &#123; delay := w.limiter.When(item) if delay &gt; w.maxDelay &#123; return w.maxDelay &#125; return delay&#125; REF:1.https://pkg.go.dev/k8s.io/client-go/util/workqueue2.Kubernetes源码剖析]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s set类型实现]]></title>
    <url>%2F2021%2F07%2F14%2Fk8s-set%E7%B1%BB%E5%9E%8B%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Golang是没有像python一样本身实现set类型，如果在开发过程中需要使用set类型的话则需要自己实现。今天我们来学习一下k8s源码中的int类型的set实现。代码位于https://github.com/kubernetes/kubernetes/tree/master/staging/src/k8s.io/apimachinery/pkg/util/sets目录下。 1.set可以看作值为空的map，为了节省内存，这里使用struct{}来充当map中的值。因为空的struct{}大小可以看作为零，因为空的struct{}是指向同一个地址。12345// 定义值为空结构体type Empty struct &#123;&#125;// Int类型set,值为Emptytype Int map[int]Empty 2.sets中提供一个将字典转换为set类型的方法1234567891011// 将键为int的map转换成set,set的元素为map的key// 如果theMap的键不为int则会产生panicfunc IntKeySet(theMap interface&#123;&#125;) Int &#123; v := reflect.ValueOf(theMap) ret := Int&#123;&#125; for _, keyValue := range v.MapKeys() &#123; ret.Insert(keyValue.Interface().(int)) &#125; return ret&#125; 3.初始化元素为int类型的set12345func NewInt(items ...int) Int &#123; ss := Int&#123;&#125; ss.Insert(items...) return ss&#125; 4.下面是set的一些常见的方法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156// 将int类型的元素添加到set中func (s Int) Insert(items ...int) Int &#123; for _, item := range items &#123; s[item] = Empty&#123;&#125; &#125; return s&#125;// 删除set中的所有元素func (s Int) Delete(items ...int) Int &#123; for _, item := range items &#123; delete(s, item) &#125; return s&#125;// 判断一个元素是否存在set,存在返回true,否则返回falsefunc (s Int) Has(item int) bool &#123; _, contained := s[item] return contained&#125;// 判断是否所有元素都存在于set集合中，如果是返回true,否则返回falsefunc (s Int) HasAll(items ...int) bool &#123; for _, item := range items &#123; if !s.Has(item) &#123; return false &#125; &#125; return true&#125;// 判断任意一个元素是否存在于set中，如果存在返回true,否则返回falsefunc (s Int) HasAny(items ...int) bool &#123; for _, item := range items &#123; if s.Has(item) &#123; return true &#125; &#125; return false&#125;// 计算在s中存在，在s2中不存在的元素集合// s1 = &#123;a1, a2, a3&#125;// s2 = &#123;a1, a2, a4, a5&#125;// s1.Difference(s2) = &#123;a3&#125;// s2.Difference(s1) = &#123;a4, a5&#125;func (s Int) Difference(s2 Int) Int &#123; result := NewInt() for key := range s &#123; if !s2.Has(key) &#123; result.Insert(key) &#125; &#125; return result&#125;// 计算集合并集// s1 = &#123;a1, a2&#125;// s2 = &#123;a3, a4&#125;// s1.Union(s2) = &#123;a1, a2, a3, a4&#125;// s2.Union(s1) = &#123;a1, a2, a3, a4&#125;func (s1 Int) Union(s2 Int) Int &#123; result := NewInt() for key := range s1 &#123; result.Insert(key) &#125; for key := range s2 &#123; result.Insert(key) &#125; return result&#125;// 计算集合的交集// s1 = &#123;a1, a2&#125;// s2 = &#123;a2, a3&#125;// s1.Intersection(s2) = &#123;a2&#125;func (s1 Int) Intersection(s2 Int) Int &#123; var walk, other Int result := NewInt() if s1.Len() &lt; s2.Len() &#123; walk = s1 other = s2 &#125; else &#123; walk = s2 other = s1 &#125; for key := range walk &#123; if other.Has(key) &#123; result.Insert(key) &#125; &#125; return result&#125;// 判断s1是否是s2的超集，如果是返回true,否则返回falsefunc (s1 Int) IsSuperset(s2 Int) bool &#123; for item := range s2 &#123; if !s1.Has(item) &#123; return false &#125; &#125; return true&#125;// 判断两个集合是否相等func (s1 Int) Equal(s2 Int) bool &#123; return len(s1) == len(s2) &amp;&amp; s1.IsSuperset(s2)&#125;type sortableSliceOfInt []int// 为sortableSliceOfInt类型实现排序方法 func (s sortableSliceOfInt) Len() int &#123; return len(s) &#125;func (s sortableSliceOfInt) Less(i, j int) bool &#123; return lessInt(s[i], s[j]) &#125;func (s sortableSliceOfInt) Swap(i, j int) &#123; s[i], s[j] = s[j], s[i] &#125;// 根据集合内容返回排序好的切片func (s Int) List() []int &#123; res := make(sortableSliceOfInt, 0, len(s)) for key := range s &#123; res = append(res, key) &#125; sort.Sort(res) return []int(res)&#125;// 返回未排序的切片func (s Int) UnsortedList() []int &#123; res := make([]int, 0, len(s)) for key := range s &#123; res = append(res, key) &#125; return res&#125;// 返回一个元素(key, true)并将其删除，如果集合为空则返回(0,false)func (s Int) PopAny() (int, bool) &#123; for key := range s &#123; s.Delete(key) return key, true &#125; var zeroValue int return zeroValue, false&#125;// 返回set长度func (s Int) Len() int &#123; return len(s)&#125;// 对两个元素进行比较func lessInt(lhs, rhs int) bool &#123; return lhs &lt; rhs&#125;]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s Indexer索引器实现]]></title>
    <url>%2F2021%2F07%2F11%2Fk8s-Indexer%E7%B4%A2%E5%BC%95%E5%99%A8%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1.Indexer是什么？Indexer是client-go用来存储资源对象并自带索引功能的本地存储(内存)，Reflector从DeltaFIFO中将消费出来的资源对象存储到Indexer。Indexer中的数据与Etcd集群中的数据保持完全一致。client-go可以很方便的从本地存储(内存)读取相应的资源对象数据，而不用每次都远程从Etcd集群中读取数据，减轻kube-apiserver和Etcd的压力。 2.Indexer涉及到的数据结构123456789101112131415161718192021222324252627// staging/src/k8s.io/client-go/tools/cache/index.go// 存储缓存数据type Index map[string]sets.String// 索引器,key为索引器名称;value为索引器的实现函数type Indexers map[string]IndexFunc// 索引器实现函数,接受一个资源对象返回一个字符串列表type IndexFunc func(obj interface&#123;&#125;) ([]string, error)// key这索引器名称,value为Indextype Indices map[string]Index// staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go// cache.NewIndexer返回一个Indexer对象，为cache&#123;&#125;type cache struct &#123; // 可看作一个并发安全的Map,并实现了ByIndex方法(下文会用到), cacheStorage ThreadSafeStore keyFunc KeyFunc&#125;// ThreadSafeStore的实现type threadSafeMap struct &#123; lock sync.RWMutex items map[string]interface&#123;&#125; indexers Indexers indices Indices&#125; 3.通过一个例子了解Indexer的使用和实现 1234567891011121314151617181920212223242526272829303132333435// 代码来自于&lt;&lt;kubernetes源码剖析&gt;&gt;package mainimport ( "fmt" "k8s.io/api/core/v1" metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/tools/cache" "strings")func UsersIndexFunc(obj interface&#123;&#125;) ([]string, error) &#123; pod := obj.(*v1.Pod) userString := pod.Annotations["users"] return strings.Split(userString, ","), nil&#125;func main() &#123; // 实例化一个Indexer对象,第一个参数为计算对象key值的函数,第二个参数用于定义索引器,其中key为索引器名称(byUser),值为索引器函数 index := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123; "byUser": UsersIndexFunc, &#125;) pod1 := &amp;v1.Pod&#123;ObjectMeta: metav1.ObjectMeta&#123;Name: "one", Annotations: map[string]string&#123;"users": "ernie,bert"&#125;&#125;&#125; // 添加pod对象到index index.Add(pod1) // 执行索引器得到索引结果 erniePods, err := index.ByIndex("byUser", "ernie") if err != nil &#123; panic(err) &#125; for _, erniePod := range erniePods &#123; fmt.Println(erniePod.(*v1.Pod).Name) &#125;&#125; 4.程序执行步骤 初始化后的index对象,items和indices都为空123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// index.Add(pod1)// 将资源对象添加到缓存func (c *cache) Add(obj interface&#123;&#125;) error &#123; // 计算对象的key key, err := c.keyFunc(obj) if err != nil &#123; return KeyError&#123;obj, err&#125; &#125; c.cacheStorage.Add(key, obj) return nil&#125;// 真正将资源对象添加到缓存中的操作func (c *threadSafeMap) Add(key string, obj interface&#123;&#125;) &#123; c.lock.Lock() defer c.lock.Unlock() oldObject := c.items[key] // 添加数据到缓存中 c.items[key] = obj // 更新indices c.updateIndices(oldObject, obj, key)&#125;func (c *threadSafeMap) updateIndices(oldObj interface&#123;&#125;, newObj interface&#123;&#125;, key string) &#123; // if we got an old object, we need to remove it before we add it again if oldObj != nil &#123; c.deleteFromIndices(oldObj, key) &#125; // 遍历索引器 for name, indexFunc := range c.indexers &#123; // indexValues = ['ernie','bert'] indexValues, err := indexFunc(newObj) if err != nil &#123; panic(fmt.Errorf("unable to calculate an index entry for key %q on index %q: %v", key, name, err)) &#125; index := c.indices[name] // 判断Index是否存在,不存在则新建 if index == nil &#123; index = Index&#123;&#125; c.indices[name] = index &#125; // 遍历indexValues(['ernie','bert']),构建一个反向映射，从indexValue指向key // 类似于构建成&#123;"ernie":"one","bert":"one"&#125;字典结构,用于快速通过索引找到对应的资源对象 for _, indexValue := range indexValues &#123; set := index[indexValue] if set == nil &#123; set = sets.String&#123;&#125; index[indexValue] = set &#125; set.Insert(key) &#125; &#125;&#125; 执行完Add操作后的对象 123456789101112131415161718192021222324252627282930// index.ByIndex 通过索引查询资源对象func (c *cache) ByIndex(indexName, indexKey string) ([]interface&#123;&#125;, error) &#123; return c.cacheStorage.ByIndex(indexName, indexKey)&#125;// func (c *threadSafeMap) ByIndex(indexName, indexedValue string) ([]interface&#123;&#125;, error) &#123; c.lock.RLock() defer c.lock.RUnlock() // 通过索引器名称(此例为:byUser)，获取索引器函数 indexFunc := c.indexers[indexName] if indexFunc == nil &#123; return nil, fmt.Errorf("Index with name %s does not exist", indexName) &#125; // 通过索引器名称,获取对应的缓存数据 index := c.indices[indexName] // 获取index对应的值，此例indexedValue=ernie,set=&#123;"one"&#125; set := index[indexedValue] list := make([]interface&#123;&#125;, 0, set.Len()) // 遍历set,从缓存中取出对象并返回 for key := range set &#123; list = append(list, c.items[key]) &#125; return list, nil&#125; 5.小结Indexer通过索引器函数来实现索引的可定制化，用户可以定制自己的索引器实现函数，实现自定义条件查询。Index构建了一个类似于map的结构(只不过值为k8s实现的set类型)，键为索引，值为资源对象通过keyFunc计算出来的key，可提供快速查询的功能。要想更好理解的话，最好自己调试一下。 1.Kubernetes源码剖析]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker配置代理]]></title>
    <url>%2F2021%2F06%2F09%2Fdocker%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[在k8s编译中需要到很多k8s.gcr.io中的镜像,查了很多资料都没有很好的解决办法。原因是国内同步的镜像都是很久之前的，没有我需要的镜像，所以只能通过代理来解决 1.首先你需要一个科学上网的工具，这一步不多说，自行查找资料。2.尝试执行如下命令，发现并没有用，原因在于docker并没有使用系统的代理。 12345export http_proxy=http://127.0.0.1:8123export https_proxy=http://127.0.0.1:8123systemctl daemon-reloadsystemctl restart docker 3.为docker设置镜像拉取代理vim /lib/systemd/system/docker.service系统版本不一样此路径也可能不同1234567891011[Service]Type=notifyExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecReload=/bin/kill -s HUP $MAINPIDTimeoutSec=0RestartSec=2Restart=always# 添加如下两行Environment="HTTP_PROXY=http://127.0.0.1:8123"Environment="HTTPS_PROXY=http://127.0.0.1:8123" 12systemctl daemon-reloadsystemctl restart docker 4.然后你就能顺利的拉取k8s.gcr.io上的镜像了。]]></content>
      <tags>
        <tag>k8s</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s-crd]]></title>
    <url>%2F2021%2F05%2F15%2Fk8s-crd%2F</url>
    <content type="text"><![CDATA[CRD是k8s提供对现有资源进行扩展的一种方式，当现有k8s资源都无法满足你的需求时就可以考虑使用CRD对现有资源进行扩展。 环境信息：golang 1.16.2,kubernetes v1.19.0 如何自定义一个k8s CRD(Custom Resource Definition) 1.新建一个项目，项目名称为k8s-crd1234➜ gopro mkdir k8s-crd➜ gopro cd k8s-crd ➜ k8s-crd mkdir -p pkg/apis/crd/v1➜ k8s-crd go mod init hysyeah.top/k8s-crd 2.定义资源注册所需的字段信息，pkg/apis/crd/register.go123456package crdconst ( GroupName = "crd.alpha.io" Version = "v1") 3.定义全局标签，pkg/apis/crd/v1/doc.go.k8s中许多代码都是通过代码生成器生成的，代码生成器通过Tags(标签)来判断一个包如何进行代码生成。k8s中存在如下两种Tag: 全局Tags: 定义在每个包的doc.文件中，对整个包中的类型自动生成代码 局部Tags: 定义在Go语言的类型声明上方，只对指定的类型自动生成代码 可参考deepcopy-gen 全局Tags告诉deepcopy-gen代码生成器为该包中的每个类型自动生成DeepCopy函数。其中//+groupNmae定义了资源组名称，一般使用域名形式命名1234// +k8s:deepcopy-gen=package// +groupName=crd.hysyeah.toppackage v1 4.定义资源类型，pkg/apis/crd/v1/types.go1234567891011121314151617181920212223242526272829303132package v1import ( metav1 "k8s.io/apimachinery/pkg/apis/meta/v1")// +genclient// +genclient:noStatus// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// Machine describes a Machine resourcetype Dog struct &#123; metav1.TypeMeta `json:",inline"` metav1.ObjectMeta `json:"metadata,omitempty"` Spec DogSepc `json:"spec"`&#125;// DogSpec定义资源拥有的属性type DogSpec struct &#123; //在这里可以自定义Dog所拥有的属性,我们这种定义了Kg和Age两个属性 Kg int `json:"kg"` Age int `json:"age"`&#125;// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// DogList定义列表资源type DogList struct &#123; metav1.TypeMeta `json:",inline"` metav1.ListMeta `json:"metadata"` Items []Dog `json:"items"`&#125; Dog上方的Tags为局部Tags。它定义了两个代码生成器genclient和deepcopy-gen。其中genclient代码生 成器为这个资源类型自动生成对应的客户端代码,deepcopy-gen代码生成器为这个资源类型自动生成DeepCopy函数，并为该类型生成返回值为runtime.Object类型的DeepCopyObject函数。 5.定义资源注册方法,pkg/apis/crd/v1/register.go1234567891011121314151617181920212223242526272829303132333435363738package v1import ( metav1 "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/apimachinery/pkg/runtime" "k8s.io/apimachinery/pkg/runtime/schema" "hysyeah.top/k8s-crd/pkg/apis/crd")// 定义注册资源的资源组和版本var SchemeGroupVersion = schema.GroupVersion&#123; Group: crd.GroupName, Version: crd.Version,&#125;var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme)func Resource(resource string) schema.GroupResource &#123; return SchemeGroupVersion.WithResource(resource).GroupResource()&#125;func Kind(kind string) schema.GroupKind &#123; return SchemeGroupVersion.WithKind(kind).GroupKind()&#125;// addKnownTypes 定义注册方法func addKnownTypes(scheme *runtime.Schema) error &#123; scheme.AddKnownType( SchemeGroupVersion, &amp;Dog&#123;&#125;, &amp;DogList&#123;&#125;, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125; 6.配置code-generator,在项目根目录下新建目录hack并新建文件tools.go。12345678910111213141516171819202122// +build tools/* Copyright 2019 The Kubernetes Authors. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.*/// This package imports things required by build scripts, to force `go mod` to see them as dependenciespackage toolsimport _ "k8s.io/code-generator" 新建文件hack/boilerplate.go.txt,生成代码需要添加这个license123456789101112131415/*Copyright The Kubernetes Authors.Licensed under the Apache License, Version 2.0 (the "License");you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an "AS IS" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.*/ 新建hack/update-codegen.sh12345678910111213141516171819202122232425262728293031#!/usr/bin/env bash# Copyright 2017 The Kubernetes Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at## http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.set -o errexitset -o nounsetset -o pipefail# generate the code with:# --output-base because this script should also be able to run inside the vendor dir of# k8s.io/kubernetes. The output-base is needed for the generators to output into the vendor dir# instead of the $GOPATH directly. For normal projects this can be dropped.ROOT_PACKAGE="hysyeah.top/k8s-crd"CUSTOM_RESOURCE_NAME="crd"CUSTOM_RESOURCE_VERSION="v1"chmod +x ../vendor/k8s.io/code-generator/generate-groups.sh../vendor/k8s.io/code-generator/generate-groups.sh all "$ROOT_PACKAGE/pkg/client" "$ROOT_PACKAGE/pkg/apis" "$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION" \ --go-header-file $(pwd)/boilerplate.go.txt 7.生成代码 123456789//在项目根目录下➜ k8s-crd go mod vendor➜ k8s-crd chmod +x hack/update-codegen.sh➜ k8s-crd cd hack ➜ hack./update-codegen.shGenerating deepcopy funcsGenerating clientset for crd:v1 at hysyeah.top/k8s-crd/pkg/client/clientsetGenerating listers for crd:v1 at hysyeah.top/k8s-crd/pkg/client/listersGenerating informers for crd:v1 at hysyeah.top/k8s-crd/pkg/client/informers 执行完代码生成命令后，根据日志可能看出代码已经生成出来了，但是你却发现生成的文件并不在项目目录中，这是什么原因呢？其实代码生成的路径是$GOPATH/src/hysyeah.top/k8s-crd，将生成代码拷贝至项目中。此时项目结构如下,其中红框中的代码是自动生成的： 8.有了自定义资源，我们还需要定义如何使用它，不然这个资源也无用之地。而对于资源的如何使用k8s是通过controller来进行资源控制的。 新建目录pkg/signals,接收系统信号，直接使用社区代码 controller.go,业务逻辑代码，可以针对资源不同的事件注册对应的函数 main.go, 入口函数 9.编译，生成一个二进制文件12go mod vendorgo build -o dogcontroller 10.创建CRD12345➜ k8s-crd kubectl apply -f Dog.yamlcustomresourcedefinition.apiextensions.k8s.io/dogs.crd.hysyeah.top created➜ k8s-crd kubectl get crdNAME CREATED ATdogs.crd.hysyeah.top 2021-05-15T12:25:01Z 11.创建Dog实例12➜ k8s-crd kubectl apply -f example-dog.yamldog.crd.hysyeah.top/alpha-dog created 12.运行dogcontroller1./dogcontroller -kubeconfig=$HOME/.kube/config 13.分别执行相应的操作，controller会解发相应的动作。以删除dog实例为例。 小结：以上便是手动构建CRD的过程，可以使我们更好的了解CRD和Controller的工作原理。其实工作中如果要自定义一个CRD和Controller大可不必那么麻烦，可以借助一些框架使整个过程更方便，如kubebuilder,operator-sdk。 参考：1.https://github.com/kangxiaoning/learn-kubernetes-crd/blob/main/README.md2.https://github.com/kubernetes/code-generator]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go defer陷阱[译]]]></title>
    <url>%2F2021%2F01%2F02%2FGo-defer%E9%99%B7%E9%98%B1%2F</url>
    <content type="text"><![CDATA[对等于nil的函数defer操作如果通过defer调用一个值等于nil的函数，会引发panic错误。12345func main() &#123; var run func() = nil defer run() fmt.Println("runs")&#125; 输出：123runspanic: runtime error: invalid memory address or nil pointer dereference[signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x452dc8] 原因：在函数进行到最后之后，才执行run()函数,并引发panic错误，因为这是一个值为nil的函数 在for循环中调用defer 上面的defer row.Close()在for循环中并不会立即执行直到函数结束(第一次循环结束并不会执行)。在调用defer会产生一个函数调用栈,如果循环次数过多将会产生意料之外的问题。 解决方法： 1.直接调用row.Close()不使用defer123456789func () &#123; for &#123; row, err := db.Query("SELECT ...") if err != nil&#123; ... &#125; row.Close() &#125;&#125; 2.将操作放入一个匿名函数中，当函数结束时将会执行defer调用的函数1234567891011121314func () &#123; for &#123; func() &#123; row, err := db.Query("SELECT ...") if err != nil &#123; .. &#125; defer row.Close() .. // deferred funcs run here &#125;() &#125;&#125; Defer as wrapper有些时个你需要对闭包使用defer以追求实用或者有其它的一些原因。比如：要打开数据库连接，然后运行一些查询，最后运行以确保断开连接。12345678910111213type database struct&#123;&#125;func (db *database) connect() (disconnect func()) &#123; fmt.Println("connect") return func() &#123; fmt.Println("disconnect") &#125;&#125;func main() &#123; db := &amp;database&#123;&#125; defer db.connect() fmt.Println("query db...")&#125; 输出：12query db...connect 为什么最后没有执行disconnect?这里出现这个bug的原因是connect函数返回的值并没有执行而是被保存了起来。 解决方法:1234567func main() &#123; db := &amp;database&#123;&#125; close := db.connect() defer close() fmt.Println("query db...")&#125; 不好的实践：虽然下面的代码可以正常的执行,但是不推荐这样使用。123db := &amp;database&#123;&#125;defer db.connect()().. Defer in a block你可能会想deferred func会在结束一个代码块的时候执行，实际上deferred func只会在包含它的函数结束的时候执行。123456789func main() &#123; &#123; defer func() &#123; fmt.Println("block: defer runs") &#125;() fmt.Println("block: ends") &#125; fmt.Println("main: ends")&#125; 输出：123block: endsmain: endsblock: defer runs 原因：deferred func只会在函数代码块结束的时候执行。 解决方法：使用匿名函数12345678910111213func main() &#123; func() &#123; defer func() &#123; fmt.Println("func: defer runs") &#125;() fmt.Println("func: ends") &#125;() fmt.Println("main: ends")&#125;// output:func: endsfunc: defer runsmain: ends Deferred method陷阱不使用指针：1234567891011type Car struct &#123; model string&#125;func (c Car) PrintModel() &#123; fmt.Println(c.model)&#125;func main() &#123; c := Car&#123;model: "DeLorean DMC-12"&#125; defer c.PrintModel() c.model = "Chevrolet Impala"&#125; 输出：1DeLorean DMC-12 使用指针：123func (c *Car) PrintModel() &#123; fmt.Println(c.model)&#125; 输出:1Chevrolet Impala 当使用defer时，传递给函数的参数会被立即保存下来而不用等到函数执行。 当一个方法的接收者是值接收者时，这个接收者会被拷贝(当调用defer函数的时候)所以当修改Car结构的数据被修改后,defer调用的函数并不会知道，因为它使用的是拷贝过来的数据。 如果一个方法的接收者是指针，当调用defer时，虽然也会产生一个新的指针，但这和原因的指针指向的是同一个对象，所以任何的对于结构体Car中的任何改变都能被探测到。 打印Z —&gt; A1234567func main() &#123; for i := 0; i &lt; 4; i++ &#123; defer fmt.Print(i) &#125;&#125;//output: 3210 执行for循环的时候进行压栈操作，当函数结束的时候弹出然后执行压栈的函数 参数作用域问题，参数被覆盖1234567891011121314151617181920type reader struct &#123;&#125;func (r reader) Close() error &#123; return errors.New("close Error")&#125;func release(r io.Closer) (err error) &#123; defer func() &#123; if err := r.Close();err != nil&#123; ... &#125; &#125;() return&#125;func main() &#123; r := reader&#123;&#125; err := release(r) fmt.Print(err)&#125;//output: nil 也许的你期待的返回是“close Error“，但实际返回的err却是nil。 原因：在if代码块中使用新的err覆盖了name result中的err值,所以release()返回了原来的result-value。这里还有疑惑，既然被被覆盖了难道不应该返回不为nil的值吗 解决方案：使用=而不:=12345678func release(r io.Closer) (err error) &#123; defer func() &#123; if err = r.Close();err != nil&#123; fmt.Println("err in r.close") &#125; &#125;() return&#125; 即时计算参数的值传递给deferred func的参数是在函数注册(调用defer)的时候进行计算的而不是当它执行的时候计算。123456789101112131415161718type message struct &#123; content string&#125;func (p *message) set(c string) &#123; p.content = c&#125;func (p *message) print() string &#123; return p.content&#125;func main() &#123; m := &amp;message&#123;content: "Hello"&#125; defer fmt.Print(m.print()) m.set("World") // deferred func runs&#125;//output: Hello 为什么输出不是”World”在调用defer时，fmt.Print是在函数结束之前执行，但是传递给它的参数m.print()会立即执行，所以传递给fmt.Print的参数是”Hello”,而且这个值会被保存直到defer中的函数执行。 for循环中的捕获在循环中deferred func将会看到最新的值当函数执行的时候，有一种情况除外，就是把值当作参数传递给了deferred func。 1234567891011func main() &#123; for i := 0; i &lt; 3; i++ &#123; defer func() &#123; fmt.Println(i) &#125;() &#125;&#125;//output333 Why?当defer中的函数运行时，deferred func看到的是i的最新值。因为当调用defer进行函数注册时，Go运行时捕获的是变量i的地址。当循环结束后i的值变为3,所以当运行defer中的函数时因为指针指向的是同一个值所以输出都为3。 解决方案1：将值当作参数传递给defer中的函数1234567891011func main() &#123; for i := 0; i &lt; 3; i++ &#123; defer func(i int) &#123; fmt.Println(i) &#125;(i) &#125;&#125;//output:210 解决方案2：使用一个新的变量i覆盖掉外一层的i12345678func main() &#123; for i := 0; i &lt; 3; i++ &#123; i := i defer func() &#123; fmt.Println(i) &#125;() &#125;&#125; 解决方案3：如果只有一个函数调用12345func main() &#123; for i := 0; i &lt; 3; i++ &#123; defer fmt.Println(i) &#125;&#125; defer中函数的返回值defer函数中的返回值对于调用者是不可见，但你仍可以使用命名返回值的方法改变结果值。1234567891011func release() error &#123; defer func() error &#123; return errors.New("error") &#125;() return nil&#125;func main() &#123; r := release() fmt.Println(r)&#125;//outpu:&lt;nil&gt; 解决方案：改变命名结果值123456func release() (err error) &#123; defer func() &#123; err = errors.New("error") &#125;() return nil&#125; 在deferred func之后调用recover函数123456func main() &#123; recover() panic("error")&#125;//output:panic: error 在defer之外使用recover()不能捕获panic。解决方案：12345678910111213func do() &#123; defer func() &#123; r := recover() fmt.Println("recovered:", r) &#125;() panic("error")&#125;func main() &#123; do()&#125;//outputrecovered: error 错误的顺序调用defer func123456789101112func do() error &#123; res, err := http.Get("http://notexists") defer res.Body.Close() if err != nil &#123; return err &#125; // ..code... return nil&#125;//outputpanic: runtime error: invalid memory address or nil pointer dereference 原因是我们没有检查http请求是否成功，如果失败则会出现如上错误。因为出错的情况下res为nil，当执行res.Body的时候则会引发panic。 解决方案：加个判断1234567891011func do() error &#123; res, err := http.Get("http://notexists") if res != nil &#123; defer res.Body.Close() &#125; if err != nil &#123; return err &#125; // ..code... return nil&#125; 不检查错误不要以后把善后的工作委派给了defer就可以安全的释放资源。你有可能会丢失一些有用的报错信息。 不推荐：f.Close()可能会报错，但是我们意识不到。123456789func do() error &#123; f, err := os.Open("book.txt") if err != nil &#123; return err &#125; defer f.Close() // ..code... return nil&#125; 更好的做法是检查错误并处理错误12345678910111213func do() error &#123; f, err := os.Open("book.txt") if err != nil &#123; return err &#125; defer func() &#123; if err := f.Close(); err != nil &#123; // log etc &#125; &#125;() // ..code... return nil&#125; 你还可以使用命名结果值返回defer中的错误020年10月19日12345678910111213func do() (err error) &#123; f, err := os.Open("book.txt") if err != nil &#123; return err &#125; defer func() &#123; if ferr := f.Close(); ferr != nil &#123; err = ferr &#125; &#125;() // ..code... return nil&#125; Note:你可以使用这个包包裹多种错误。这是有必要的，因为f.Close可能会覆盖在其之前的错误。在一个错误中包裹另一个错误并打印到日志中可以更好的排查错误。你也可以使用这个包来捕获你不想检查的错误 释放相同的资源你可能会对一个资源进行多次释放操作，这会发生预料之后的问题。 12345678910111213141516171819202122232425func do() error &#123; f, err := os.Open("book.txt") if err != nil &#123; return err &#125; defer func() &#123; if err := f.Close(); err != nil &#123; // log etc &#125; &#125;() // ..code... f, err = os.Open("another-book.txt") if err != nil &#123; return err &#125; defer func() &#123; if err := f.Close(); err != nil &#123; // log etc &#125; &#125;() return nil&#125;//outputclosing resource #another-book.txtclosing resource #another-book.txt WHY？因为当deferred func运行时，看到的是最新的值，所以看到的变量f是最新的那个(another-book.txt)。因为同一资源被释放了两次。 解决方案：1234567891011121314151617181920212223242526func do() error &#123; f, err := os.Open("book.txt") if err != nil &#123; return err &#125; defer func(f io.Closer) &#123; if err := f.Close(); err != nil &#123; // log etc &#125; &#125;(f) // ..code... f, err = os.Open("another-book.txt") if err != nil &#123; return err &#125; defer func(f io.Closer) &#123; if err := f.Close(); err != nil &#123; // log etc &#125; &#125;(f) return nil&#125;//output:closing resource #another-book.txtclosing resource #book.txt panic/recover可以获取可返回任何类型字符串：12345678910func errorly() &#123; defer func() &#123; fmt.Println(recover()) &#125;() if badHappened &#123; panic("error run run") &#125;&#125;//output:"error run run" Error:12345678910func errorly() &#123; defer func() &#123; fmt.Println(recover()) &#125;() if badHappened &#123; panic(errors.New("error run run") &#125;&#125;//output:"error run run" 可接收任何类型的参数panic不仅可以接收字符串也可以接收error类型。这意味着你可以把任何类型的参数传递给panic然后从recover中获取。 123456789101112type myerror struct &#123;&#125;func (myerror) String() string &#123; return "myerror there!"&#125;func errorly() &#123; defer func() &#123; fmt.Println(recover()) &#125;() if badHappened &#123; panic(myerror&#123;&#125;) &#125;&#125; WHY？在GO中interface{}类型意味着任何类型.panic和recover的定义12func panic(v interface&#123;&#125;)func recover() interface&#123;&#125; 它们的工作流程如下panic(value) —&gt; recover() —&gt; value recover只是返回传递给panic的值 REF: Go defer 1Go defer 2Go defer 3]]></content>
      <tags>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的知道子网的作用吗]]></title>
    <url>%2F2020%2F12%2F26%2F%E4%BD%A0%E7%9C%9F%E7%9A%84%E7%9F%A5%E9%81%93%E5%AD%90%E7%BD%91%E7%9A%84%E4%BD%9C%E7%94%A8%E5%90%97%2F</url>
    <content type="text"><![CDATA[知识点回顾我们肯定使用过ifconfig命令来查询计算机中的网络设备。不知道你有没有注意过其中的netmask(子网掩码),但你真的知道它的来龙去脉吗?今天我们就来重新认识一下netmask。 在进入主题内容前我们先来回顾一些关于IP的知识点。每一个需要使用互联网的设备都必须具有一个唯一标识,而这个唯一标识就是IP地址。就像在生活中你要打电话别个时,你必须知道这个人的手机号码不然你就无法联系到他，可以说IP地址就类似于你的手机号码。 IP地址采用分层设计,并可以将其分为如下五类。不同种类的IP地址网络号和主机号都是不一样的。 各类IP的地址范围如下,方便记忆: 子网掩码的由来,为什么需要子网掩码IP地址结构设计之初是两层结构,将IP地址分为网络号和主机号,不同的网络号下包含一系列的主机(网络设备)。在这种结构下大量的主机连接在同一个网络下，当主机数量少的网络还能正常的工作，然而这种方式缺乏弹性,当主机数量过多的时候会带来一系列的问题。举个例子,当一个公司规模较小的时候，分为两个层级部门—&gt;员工。当有消息要传达到一个部门时只需要把消息一个个传达给指定部门的人员即可，当一个部门人员较少时这样不会有太大的问题。随着公司的快速发展，各个部门的人员越来越多，此时如果有消息传达给一个部门，只能把消息一个一个的发给部门里的员工;部门里并不是所有的员工都需要知道这个消息，但我们没有其它的信息来对员工进行分类，所以只能把消息发给部门里的所有员工。这样做的缺点显而易见，不仅管理不太方便，而且有些员工会收到自己不需要的消息。为了解决这个问题，公司往往会在目前的基础上新增一个层级：组。部门—&gt;分组—&gt;员工在部门的下面设置组，将对应的员工分配在对应的组，不仅可以方便管理还可以避免消息(资源)的浪费。 软件设计也是来源于生活。为了更好的适应于更大的网络和更好更方便的管理网络设备，于是提出了子网这个设计。 增加了子网相对以前的二层IP地址有哪些优点呢？ 弹性，可以自定义子网和子网中的主机数 对公网的不可见性，对于子网的划分只存在于组织(可以认为是局域网)内。还是上面的例子，不管公司的组织架构怎么调整在公外面看来你还是那个公司 通过子网可以对网络进行分组，不同分组的主机进行广播只有本组的主机才可以收到消息 子网掩码将大的网络域，分为更小的子网。较小的网络会创建较小的广播，从而产生较少的广播流量。 此外，子网还通过将网络问题隔离到特定的存在来简化故障排除 IP地址是固定的32位，那如何在原来二层结构的IP地址进行的子网的划分呢？ 此时就是轮到子网掩码上场了。 为了在不改变IP地址结构的前提下,于是人们提出了子网掩码。但是用哪几位数据来表示子网呢，答案是从主机号中借位来充当子网。 FOR EXAMPLE：B类地址:154.71.0.0, 如果不使用子网的话，154.71这个网络号可以容纳65534(pow(2,10)-2)个主机。然而你可以通过子网掩码对这个网络进行子网划分。你可以使用1位来表示子网，则剩余的15位来表示主机号；如果使用2位来表示子网，则剩余的14位来表示主机号，以此类推。你使用越多的位数来表示子网，相应的用来表示主机号的位数也就越好。最少要使用2位来表示主机号，因为减去主机后全为0和全为1的两个IP地址。 假如你要使用5位来作子网，则subnet id这5位在子网掩码中必须为1，表示这5位用于子网。12345678910# RFC950Special Addresses: From the Assigned Numbers memo [9]: &quot;In certain contexts, it is useful to have fixed addresses with functional significance rather than as identifiers of specific hosts. When such usage is called for, the address zero is to be interpreted as meaning &quot;this&quot;, as in &quot;this network&quot;. The address of all ones are to be interpreted as meaning &quot;all&quot;, as in &quot;all hosts&quot;. For example, the address 128.9.255.255 could be interpreted as meaning all hosts on the network 128.9. Or, the address 0.0.0.37 could be interpreted as meaning host 37 on this network.&quot; It is useful to preserve and extend the interpretation of these special addresses in subnetted networks. This means the values of all zeros and all ones in the subnet field should not be assigned to actual (physical) subnets. 上面这段话的大致意思是在IP地址中全为0或全为1的有特殊的含义或特殊的用途。比如IP地址128.9.255.255是网络号为128.9的文播地址，发往128.9.255.255的网络包会广播至该网络的所有主机。地址128.9.0.0会被解释为此网络，也不建议使用。因此上面5-bit的子网建议使用除0和31以外的子网。子网计算器 有无子网通信有何差异在无子网这个概念之前，假如主机A访问主机B，主机A会判断主机B和自己是不是在同一个网段，即拥有同样的网络号，如果网络号相同则A会通过ARP获得B的MAC地址然后进行通信；如果网络不相同，则将请求发送给默认网关然后再由网关进行路由。 12345IF ip_net_number(dg.ip_dest) = ip_net_number(my_ip_addr) THEN send_dg_locally(dg, dg.ip_dest) ELSE send_dg_locally(dg, gateway_to(ip_net_number(dg.ip_dest))) 实现子网划分后，主机A判断和主机B是否在同一个子网，如果在同一个子网A会通过ARP获得B的MAC地址然后进行通信；如果不在同一个子网，则将请求发送给默认网关然后再由网关进行路由。12345IF bitwise_and(dg.ip_dest, my_ip_mask) = bitwise_and(my_ip_addr,my_ip_mask) THEN send_dg_locally(dg, dg.ip_dest) ELSE send_dg_locally(dg, gateway_to(bitwise_and(dg.ip_dest, my_ip_mask))) 子网掩码是如何发现的当网络启动的时候主机如何确定自己的子网掩码呢？ 首先主机会从磁盘文件中获取子网掩码信息，如果存在则直接使用；如果磁盘上没有记录子网掩码信息，主机将会发出广播信息以此请求子网掩码。 例子:来自rfc950 假如在A类网络36.0.0.0有主机IP为36.40.0.123,网关为32.40.0.62,使用8-bit作为subnet id则其子掩码为255.255.0.0。 如果主机不知道自己的IP地址则会通过RARP协议获取自己的IP地址。然后发送ICMP请求到广播地址255.255.255.255: key val Source address: 36.40.0.123 Destination address: 255.255.255.255 Protocol: ICMP = 1 Type: Address Mask Request = AM1 Code: 0 Mask: 0 然后网关可以直接回复网络包给主机36.40.0.123,主机便获取到了自己的子网掩码 key val Source address: 36.40.0.62 Destination address: 36.40.0.123 Protocol: ICMP = 1 Type: Address Mask Request = AM2 Code: 0 Mask: 255.255.0.0 假如主机根本不知道自己的主机号则会发出如下内容的包。 key val Source address: 0.0.0.0 Destination address: 255.255.255.255 Protocol: ICMP = 1 Type: Address Mask Request = AM1 Code: 0 Mask: 0 网关将会收到数据包，然后回复如下内容的数据包：|key|val||—-|—-||Source address:|36.40.0.62||Destination address:|255.255.255.255||Protocol:|ICMP = 1||Type:|Address Mask Request = AM2||Code:|0||Mask:|255.255.0.0| 如果主机不允许进行广播消息并且假设主机知道网关的地址，主机刚会直接发数据包给网关请求子网掩码。 无法获取子网掩码的情况在如下三种情况下，经过多次尝试后仍可能无法获取子网掩码：1.网络与其它所有网络隔离2.未使用子网而且没有其它主机可以提供子网掩码3.此网络上的所有网关都未能启动。 在第一种和第二种情况，子网掩码等于网络号。在第三种情况下，无法确定主机的子网掩码因此最安全的做法就是将子网掩码设置为网络号，虽然可能是错误的。当网关正常工作后，网关会广播Address Mask Reply类型的数据包，主机收到数据包后，会更正自己的子网掩码。 REF：1.RFC9502.tcpipguide.com]]></content>
      <tags>
        <tag>－网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-perf]]></title>
    <url>%2F2020%2F11%2F21%2Flinux-perf%2F</url>
    <content type="text"><![CDATA[参考：1.[https://perf.wiki.kernel.org/index.php/Main_Page]]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-xxd]]></title>
    <url>%2F2020%2F11%2F21%2Flinux-xxd%2F</url>
    <content type="text"><![CDATA[xxd是Linux平台下的一款工具，可以用于原文件和二进制文件之间的互相转换。man page描述：12345xxd creates a hex dump of a given file or standard input. It can alsoconvert a hex dump back to its original binary form. Like uuencode(1)and uudecode(1) it allows the transmission of binary data in a `mail-safe&apos; ASCII representation, but has the advantage of decoding to stan‐dard output. Moreover, it can be used to perform binary file patching xxd常用方法 12345678910➜ ~ echo 'hello,world' &gt; origin.txt# 将文件内容转换为十六进制内容➜ ~ xxd origin.txt00000000: 6865 6c6c 6f2c 776f 726c 640a hello,world.# 将十六进制内容恢复到一个新文件中➜ ~ xxd origin.txt |xxd -r &gt; new.text➜ ~ cat new.text hello,world xxd还支持将文件转换成二进制，格式化输出等。具体可查看官方文档 参考:1.linux-man-page-xxd]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过nginx-controller将redis服务暴露到集群外]]></title>
    <url>%2F2020%2F10%2F13%2F%E9%80%9A%E8%BF%87nginx-controller%E5%B0%86redis%E6%9C%8D%E5%8A%A1%E6%9A%B4%E9%9C%B2%E5%88%B0%E9%9B%86%E7%BE%A4%E5%A4%96%2F</url>
    <content type="text"><![CDATA[在微服务迁移过程中，在进行流量切换的过程中，旧的服务和微服务会共存一段时间。当用户从前端访问时，流量分别会被转发到旧服务和新服务上。为了使用户可以同时访问旧服务和新服务，需要共用一个redis。因为以前使用的都是各自集群内部的redis(新旧服务在不同的k8s集群上)，因此为了让集群外的服务也能访问另一集群的redis服务，所以需要将redis暴露到集群外。 我们ingress使用的是nginx controller. 1.找到nginx controller对应的serviceingress-nginx,一般在kube-system命名空间下,并编辑。123456789kubectl get svc -n kube-system# 编辑ingress-nginx并新增6379端口kubectl edit svc ingress-nginx -n kube-system－ name: tcp-redis port: 6379 protocol: TCP targetPort: 6379 2.修改ingress-nginx对应TCP协议的configmap。123456kubectl edit cm tcp-services -n kube-system# 在data字段下添加如下内容data: &lt;namespace/service name&gt;:&lt;service port&gt;:[PROXY]:[PROXY] 6379: &lt;namespace&gt;/redis:6379 3.然后就可以通过redis-cli在集群外访问redis服务了。 参考：1.https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[osm-install]]></title>
    <url>%2F2020%2F09%2F28%2Fosm-install%2F</url>
    <content type="text"><![CDATA[1.使用源码安装osm-cli12345git clone git@github.com:hysyeah/osm.gitcd osmgit checkout v0.4.0 # tag需要和下面的--osm-image-tag对应，不然可能出现不兼容的错误make build-osm 2.osm install —osm-image-tag v0.4.0如果不指定tag可能会出现如下错误 3.部署完后会创建一个命名空间osm-system 参考： 1.install guide2.issue]]></content>
      <tags>
        <tag>osm</tag>
        <tag>service-mesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-cobra]]></title>
    <url>%2F2020%2F07%2F18%2FGo-cobra%2F</url>
    <content type="text"><![CDATA[Cobra是一个构建命令行应用的强有力工具。在许多项目中都有用到Cobra，比如Kubernetes,Hugo等等。 https://github.com/hysyeah/gotour/tree/master/tour REF:1.https://github.com/spf13/cobra]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-strings]]></title>
    <url>%2F2020%2F07%2F18%2FGo-strings%2F</url>
    <content type="text"><![CDATA[Packagestrings提供了对UTF-8编码字符串的一些简单操作。 安装gomacro,Go版本的REPL,对一些函数进行验证。go get github.com/cosmos72/gomacro 12345678910111213141516171819202122232425262728293031323334353637➜ hysyeah gomacro// Welcome to gomacro. Type :help for help, :copy for copyright and license.// This is free software with ABSOLUTELY NO WARRANTY.gomacro&gt; import "fmt"gomacro&gt; import "strings"gomacro&gt; strings.Compare("hello","hello")0 // intgomacro&gt; strings.Compare("hello","hell")1 // intgomacro&gt; strings.Compare("hell","hello")-1 // intgomacro&gt; strings.Contains("hello","h")true // boolgomacro&gt; strings.ContainsAny("hello","e")true // boolgomacro&gt; strings.ContainsAny("hello编程","编程")true // boolgomacro&gt; strings.Count("hello", "h")1 // intgomacro&gt; strings.Count("hello", "l")2 // intgomacro&gt; strings.HasPrefix("hello","he")true // boolgomacro&gt; strings.Index("hello", "h")0 // intgomacro&gt; strings.IndexAny("hello", "x") //匹配任意一个字符，并返回第一个匹配字符的index,如果不匹配则返回-1-1 // intgomacro&gt; strings.IndexAny("hello","hl")0 // intgomacro&gt; strings.Split("hello,a,b",",")[hello a b] // []stringgomacro&gt; s := []string&#123;"foo", "bar", "baz"&#125;gomacro&gt; fmt.Println(strings.Join(s, ", "))foo, bar, baz14 // int&lt;nil&gt; // error REF:1.https://golang.org/pkg/strings/]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-error]]></title>
    <url>%2F2020%2F07%2F18%2FGo-error%2F</url>
    <content type="text"><![CDATA[Packageerrors为Go语言提供了错误处理的函数。 error的定义：123type error interface &#123; Error() string&#125; errors中提供的方法1234567891011// 根据给定的字符串返回一个error类型func New(text string) error// 在错误链中查找第一个等于target的错误，并将target设置为这个错误返回true,否则返回falsefunc As(err error, target interface&#123;&#125;) bool// 判断错误链中是否包含target类型的错误func Is(err, target error) bool//调用参数err本身的Unwrap方法，如果err存在Unwrap方法返回该方法返回的值，否则返回nilfunc Unwrap(err error) error Go从1.13开始提供了Unwarp方法来获取嵌套的错误。1.创建一个嵌套的错误方法一：通过fmt.Error(&quot;%w&quot;, err)来创建123456789func main() &#123; err1 := errors.New("new err") err2 := fmt.Errorf("err2: [%w]", err1) // err2: [new err] fmt.Println(err2) e := errors.Unwrap(err2) // new err fmt.Println(e)&#125; 方法二：自定义结构体，并实现Error()和Unwrap()方法123456789101112131415161718192021type QueryError struct &#123; Query string Err error&#125;func (e *QueryError) Error() string &#123; return e.Query&#125;func (e *QueryError) Unwrap() error &#123; return e.Err&#125;func main() &#123; err1 := QueryError&#123;"select", errors.New("permission denied")&#125; e := errors.Unwrap(&amp;err1) // output: permission denied fmt.Println(e)&#125; 2.errors.Is判断嵌套的error是否包含特定的错误123456func main() &#123; err1 := errors.New("new err") err2 := fmt.Errorf("err2: [%w]", err1) // output: true fmt.Println(errors.Is(err2, err1))&#125; 3.errors.As判断错误链中是否包含target类型的错误12345678910111213141516type QueryError struct &#123; Query string&#125;func (e *QueryError) Error() string &#123; return e.Query&#125;func main() &#123; var target *QueryError err2 := fmt.Errorf("err2: [%w]", &amp;QueryError&#123;Query:"select error"&#125;) // output: true fmt.Println(errors.As(err2, &amp;target))&#125; REF:1.https://blog.golang.org/go1.13-errors2.https://golang.org/pkg/errors/]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-flag]]></title>
    <url>%2F2020%2F07%2F16%2FGo-flag%2F</url>
    <content type="text"><![CDATA[标准库flag实现了命令行参数的解析。 123456789101112131415func main() &#123; //ip类型是指针,存放的是指向flagname的指针 var ip = flag.Int("flagname", 1234, "help message") var flagvar string //将命令行参数值写入到flagvar中,与StringVar对应的还有IntVar,UintVar等 //func StringVar(p *string, name string, value string, usage string) flag.StringVar(&amp;flagvar, "flagvar", "helloflag", "help message") //将命令行解析为定义的标志 flag.Parse() fmt.Printf("the point address of ip is %x:\n", ip) fmt.Printf("the value of flagname is %d\n", *ip) fmt.Printf("the value of flagvar is %s\n",flagvar)&#125; 命令行传参形式：-flag-flag=x-flag x //只支持非布尔类型 其中-和--功能是一样的。 子命令的使用123456789101112func main() &#123; flag.Parse() // 创建一个带有指定名称和错误处理属性的空命令集 goCmd := flag.NewFlagSet("create", flag.ExitOnError) goCmd.StringVar(&amp;name, "name", "go", "help") args := flag.Args() // 对命令进行解析，否则获取不到name中的值 goCmd.Parse(args[1:]) fmt.Println(name)&#125; type Value1234567// 值是存储在标志中的动态值的接口。// 对于每个存在的标志，按命令行顺序调用一次Set。// 可以通过这个接口的方法实现个性化操作type Value interface &#123; String() string Set(string) error&#125; 如下代码会对传入的参数值末尾加上_value12345678910111213141516171819202122232425package mainimport ( "flag" "fmt")type Name stringfunc (n *Name) String() string &#123; return fmt.Sprint(*n)&#125;func (n *Name) Set(value string) error &#123; *n = Name(value + "_value") return nil&#125;func main() &#123; var name Name flag.Var(&amp;name, "name", "help message") flag.Parse() fmt.Printf("name is %s",name)&#125;// go run flag2.go -name hys// output: name is hys_value REF:1.https://golang.org/pkg/flag/]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-syscall]]></title>
    <url>%2F2020%2F07%2F11%2FGo-syscall%2F</url>
    <content type="text"><![CDATA[包syscall提供了操作系统系统调用的接口。不同的操作提供原语会有所区别，所以有些系统是不一样的。godoc会根据当前的操作系统显示对应的文档。如果你想显示对应的文档，可以设置GOOS和GOARCH两个环境变量。大部分的系统调用在其它的Package中提供了兼容性更好的接口，如果可能的话推荐使用其它的Package比如os, time, net。 这个包官方不建议使用，具体可参考,推荐使用go get golang.org/x/sys。 REF:1.https://docs.google.com/document/d/1QXzI9I1pOfZPujQzxhyRy6EeHYTQitKKjHfpq0zpxZs/edit#heading=h.8gsfvmj9td4q]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Namespace]]></title>
    <url>%2F2020%2F07%2F05%2FLinux-Namespace%2F</url>
    <content type="text"><![CDATA[Linux Namespace是容器的基石，可以说没有Linux Namespace就不存在现在的容器技术。容器最大的功能就是对资源进行隔离，容器正是通过Linux Namespace技术实现了各种资源的隔离。 Linux Namespace总共有6种类别的Namespace，分别为 Namespace system call flag kernal version UTS Namespace CLONE_NEWUTS 2.6.19 Mount Namespace CLONE_NEWNS 2.4.19 IPC Namespace CLONE_NEWIPC 2.6.19 PID Namespace CLONE_NEWPID 2.6.24 Network Namespace CLONE_NEWNET 2.6.29 User Namespace CLONE_NEWUSER 3.8 1.UTS NamespaceUTS是UNIX Time Sharing的缩写。主要用来隔离hostname，在每个UTS Namespace下，每个Namespace都可以拥有自己的Namespace。12345678910111213141516171819202122232425262728package main// 2020-06-11// UTS Namespace主要用来隔离hostname(主机名用于标识主机)和domainname两个系统标识(已淘汰)// pstree 命令// https://www.jianshu.com/p/049f13e55840import ( "os/exec" "syscall" "os" "log")func main() &#123; //指定被fork出来的新进程内的初始命令 cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil&#123; log.Fatal(err) &#125;&#125; 执行这段代码会进入一个sh运行运行环境。echo $$是输出当前的PID，readlink查看对应的进程是否是在同一个Namespace。hostname -b bird修改hostname为bird,重新打开一个终端，输入hostname发现宿主机的hostname并没有被改变。 2.IPC Namespace用于隔离System V IPC和POSIX message queues。 12345678910111213func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; log.Fatal(err) &#125;&#125; 左边为宿主机，右边为新建的sh环境。新建的message queue在右边的隔离Namespace并不能看到。12ipcs -q //查看ipc message queueipcmk -Q //新建一个message queue 3.PID NamespacePID Namespace用于隔离进程ID12345678910111213141516171819202122232425262728293031323334353637func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; log.Fatal(err) &#125;&#125;``` 执行上述代码`sudo go /opt/go/bin/go run pidNamespace`。`echo $$`发现当前的PID为1，而通过`pstree`命令发现运行执行上述代码的PID为58329,而隔离命名空间的PID却为1，说明这个58329在新的命令空间PID映射为1。![](http://img.hysyeah.top/2020/6/11/20200611221910-pid-namespace.png)#### 4.Mount Namespace`Mount Namespace`用于隔离各个进程看到的挂载点视图，是`Linux`实现的第一个`Namespace`类型，在不同的`Namespace`中看到的文件系统是不一样的。```gofunc main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC |syscall.CLONE_NEWPID | syscall.CLONE_NEWNS, &#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run();err != nil&#123; log.Fatal(err) &#125;&#125; 执行上述代码，然后执行ls /proc发现/proc中的内容还是宿主机上的。通过命令mount -t proc proc /proc将/proc mount到新建的命名空间,此时再执行ls /proc发现少了很多文件。在当前Namespace中，sh进程是PID为1的进程。这就说明当前的Mount Namespace中的mount和外部空间是隔离的，mount操作并没有影响到外部。Docker volume正是利用了这个特性。 5.User NamespaceUser Namespace主要是用于隔离用户的用户组ID。一个进程的User ID和Group ID在User Namespace内外是不同的。常用的场景是在宿主机以一个非root用户运行创建一个User Namespace`,然后在命名空间里面却映射成root用户。 12345678910111213141516func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER, &#125; // 加上下面这行代码会报没有权限的错误 //cmd.SysProcAttr.Credential = &amp;syscall.Credential&#123;Uid: uint32(1000), Gid: uint32(1000)&#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; log.Fatal(err) &#125; os.Exit(-1)&#125; 可以看到在宿主机和User Namespace中它们的UID是不同的。 6.Network NamespaceNetwork Namespace用于隔离网络设备，IP地址，端口，路由表，防火墙规则等网络栈的Namespace。每个容器独占一个Network Namespace，每个容器都能随意使用自己的端口而不会产生冲突。1234567891011121314151617func main() &#123; cmd := exec.Command("sh") cmd.SysProcAttr = &amp;syscall.SysProcAttr&#123; Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWIPC | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS | syscall.CLONE_NEWNS | syscall.CLONE_NEWUSER |syscall.CLONE_NEWNET, &#125; // 加上下面这行代码会报没有权限的错误 //cmd.SysProcAttr.Credential = &amp;syscall.Credential&#123;Uid: uint32(1000), Gid: uint32(1000)&#125; cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr if err := cmd.Run(); err != nil &#123; log.Fatal(err) &#125; os.Exit(-1)&#125; 分别在宿主机和新建sh环境上分别查看网络设备，发现网络设备是不一样的，说明网络已经隔离。 Network Namespace中的进程如何与宿主机进行通信12345➜ ~ ip netns // 查看Network Namespace列表➜ ~ sudo ip netns add netns1 //创建一个名称为netns1的Network Namespace➜ ~ ip netnsnetns1 查询名为netns1的网络命名空间下的网络设备，此时网卡状态是DOWN进行netns1执行本志回环地址，发现网络不通，因为此时设备的状态是DOWN,通过命令ip netns exec netns1 ip link set dev lo lup启动设备后可以PING通。但此时只能PING通Network Namespace中的本地地址，还不能与宿主机进行通信。 我们可以新建一对虚拟的网卡veth pair与宿主机进行通信。veth pair总是成对出现且相互连接，报文从一端进去就会从另一端出来。创建一对虚拟网卡，并查看网络设备；此时veth0和veth1都在宿主机的网络命名空间内。 将veth1加入到网络命名空间netns1中ip link set veth1 netns netns1，此时再查看网络设备。发现veth1已到了netns下。 设置网卡状态并绑定IP。从宿主机PING netns1下的veth1.从netns1下PING 宿主机中的veth0 netns1中的路由表与防火墙规则也是隔离的 Ref:1.https://book.douban.com/subject/27082348/2.https://book.douban.com/subject/34855927/]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-reflect]]></title>
    <url>%2F2020%2F07%2F05%2FGo-reflect%2F</url>
    <content type="text"><![CDATA[Package reflect实现了运行时反射，允许程序获取任意对象的运行时信息。最常见的用法就是通过调用TypeOf方法获取对象类型；调用ValueOf返回对象运行时数据。反射是程序检查自身结构的一种能力，是一种形式的元编程。类型和接口(Types and interfaces)反射构建在类型系统之上。Go是静态类型语言，每个变量都有一个静态类型，在编译阶段确认。 123456789101112type MyInt intfunc main() &#123; var i int var j MyInt fmt.Println(reflect.TypeOf(i)) fmt.Println(reflect.TypeOf(j))&#125;output:intmain.MyInt 变量i的类型为int,j的类型为MyInt,虽然i和j的底层类型都是int类型，但它们之间如果不进行类型转换则不能进行赋值。interface{}可以表示任何类型的数据。 Go反射三大法则1.从interface{}开始进行对象的反射简单来说反射就是一种从`interface`类型的变量取出里面的`Type`和`value`的一种机制。通过`reflect.TypeOf`和`reflect.ValueOf` 可以分别获取反射的类型和值。 12345678func main() &#123; var x float64 = 3.4 r := reflect.TypeOf(x) fmt.Println("type:", r) // output: type:float64 fmt.Println(reflect.TypeOf(r)) // output: ＊reflect.rtype&#125; 上述代码表明变量x的类型为float64，而且传递给函数TypeOf的参数x是float64类型，而不是interface类型，所以这跟interface又有什么关系呢？查看函数TypeOf源码123456// TypeOf returns the reflection Type that represents the dynamic type of i.// If i is a nil interface value, TypeOf returns nil.func TypeOf(i interface&#123;&#125;) Type &#123; eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ)&#125; 发现TypeOf的参数类型为interface{},所以参数x会存储在一个空的interface中，然后再从这个interface中恢复类型信息。func ValueOf(i interface{}) Value也是如此。 2.从反射对象可以获得interface的值 对于一个reflect.Value类型的对象(反射对象)，我们可以通过Interfaace方法从中获取interface值。123456func main() &#123; var x float64 = 3.4 r := reflect.ValueOf(x) fmt.Println(r.Interface()) // output: 3.4&#125; 123func (v Value) Interface() (i interface&#123;&#125;) &#123; return valueInterface(v, true)&#125; reflect.ValueOf的作用是把float64类型的变量转换成reflect.Value类型，而Interface的作用与reflect.ValueOf正好相反。 3.要想修改一个反射对象，反射对象必须是可设置的执行如下代码12345func main() &#123; var x float64 = 3.4 v := reflect.ValueOf(x) v.SetFloat(7.1) &#125; 所以报错的原因是什么呢？请看如下代码:12var x float64 = 3.4v := reflect.ValueOf(x) 因为Go语言的函数调用的参数都是值传递的，所以传递给reflect.ValueOf函数的参数x并不是x本身而是x的一份拷贝。如果v.SetFloat(7.1)可以设置成功，这并不能更新x的值，更新的只是x的拷贝；但是真正的x并没有更新，这样做不但没有用而且会造成很多的疑惑。 要想对原有变量进行修改可以采用如下方式：1234567func main() &#123; i := 3.4 v := reflect.ValueOf(&amp;i) //获取变量指针 v.Elem().SetFloat(10) //通过Elem()获取指针指向的变量，SetFloat对变量进行更新 fmt.Println(i) //output :10&#125; 通过反射对结构体进行修改：12345678func main() &#123; t := T&#123;23, "skidoo"&#125; s := reflect.ValueOf(&amp;t).Elem() s.Field(0).SetInt(10) fmt.Println(s.Field(0)) // output: 10&#125; Ref: 1.https://golang.org/pkg/reflect/2.https://blog.golang.org/laws-of-reflection]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go Json]]></title>
    <url>%2F2020%2F07%2F05%2FGo-Json%2F</url>
    <content type="text"><![CDATA[JSON(JavaScript Object Notations)是一种简单的数据交互格式，经常用于不同系统之间的信息交换。 Go语言中的encoding/json根据RFC 7159规范实现了JSON的编码解码。 编码 将Go语言中的结构编码为Json 1func Marshal(v interface&#123;&#125;) ([]byte, error) 12345678910111213141516171819202122package mainimport ( "encoding/json" "fmt")type Message struct &#123; Name string Body string Time int64&#125;func main() &#123; m := Message&#123;"Alice", "Hello", 1294706395881547000&#125; b, _ := json.Marshal(m) fmt.Printf(string(b))&#125;//output&#123;"Name":"Alice","Body":"Hello","Time":1294706395881547000&#125; 只有能被表达为有效的JSON格式的Go数据类型才能被正确的编码: JSON对象只支持字符串作为键；被编码的结构体必须符合如下格式map[string]T(T为json package支持的任意Go类型)－ Channel,Complex,func不能被编码－ 不支持循环的数据结构；会导致Marshal掉入无限循环－ 如果数据类型是指针将会编码会指针指向的值(如果指针为nil刚编码为null) json package只会处理结构体中以大写字母开头的字段 解码 将JSON解码为Go语言中的结构体 1func Unmarshal(data []byte, v interface&#123;&#125;) error 1234567891011121314151617181920 package mainimport ( "encoding/json" "fmt")type Message struct &#123; Name string Body string Time int64&#125;func main() &#123; var m Message b := []byte(`&#123;"Name":"Alice","Body":"Hello","Time":1294706395881547000&#125;`) json.Unmarshal(b, &amp;m) fmt.Println(m) fmt.Println(m.Body)&#125; 那Unmarshal是如何解码JSON中的数据，对于JSON的字段是如何对应结构体的字段？假如JSON中有一个键为FooUnmarshal首先会寻找导出字段中有标签json:”Foo”的字段；如果不存在标签则会寻找导出字段``Foo;如果还未找到，则会寻找导出字段FOO，FoO或者其它忽略大小写的匹配。如果未找到匹配字段则会忽略。 解码任意结构的JSON123456789101112131415161718192021package mainimport ( "encoding/json" "fmt")func main() &#123; var f interface&#123;&#125; b := []byte(`&#123;"Name":"Wednesday","Age":6,"Parents":["Gomez","Morticia"]&#125;`) json.Unmarshal(b, &amp;f) //此时并不能通过f.Name或f["Name]进行数据字段访问，需要通过如下语句进行类型转换，可以通过f.(type)判断f可以转换为什么类型 m := f.(map[string]interface&#123;&#125;) for k,v := range f &#123; fmt.Println(k, v) &#125;&#125; Streaming Encoders and Decodersjson包也提供了Decoder和Encoder类型来处理流式的JSON数据。12func NewDecoder(r io.Reader) *Decoderfunc NewEncoder(w io.Writer) *Encoder 123456789101112131415161718192021222324252627package mainimport ( "encoding/json" "log" "os")func main() &#123; dec := json.NewDecoder(os.Stdin) enc := json.NewEncoder(os.Stdout) for &#123; var v map[string]interface&#123;&#125; if err := dec.Decode(&amp;v); err != nil &#123; log.Println(err) return &#125; for k := range v &#123; if k != "Name" &#123; delete(v, k) &#125; &#125; if err := enc.Encode(&amp;v); err != nil &#123; log.Println(err) &#125; &#125;&#125; 上面代码实现的功能是从标准输入读取数据，先进行解码，然后判断键如果不等于Name则删除，最后输出到标准输出。 struct field tags12345678910111213141516171819202122232425// 编码为json时，字段Name名称会被改写为myName// &#123;"myName":"Alice","Body":"Hello","Time":1294706395881547000&#125;Name string `json:"myName"`// 编码为json时，字段Name名称会被改写为myName// 如果Name字段为空，则输出为&#123;"Body":"Hello","Time":1294706395881547000&#125;,不包含myName字段Name string `json: "myName,omitempty"`// 编码为json时，字段Name名称不会被改写// 如果Name字段为空，则输出为&#123;"Body":"Hello","Time":1294706395881547000&#125;,不包含Name字段Name string `json: ",omitempty"`// 编码为json时，忽略字段NameName string `json: "-"`// 编码为json时，将键Name替换为－Name string `json:"-,"`//string标签只用于string, float,integer,bool类型的字段// 将int类型转换为字符串，输出为&#123;"Time":"1294706395881547000"&#125;Time int64 `json:",string"`// 将bool类型转换为字符串，输出为&#123;"Bool":"true"&#125;Bool bool `json:",string"` 结构体嵌套12345678910type Embedded struct &#123; F1 int F2 string&#125;// 在结构体中嵌套结构体Extra,需指定为inlinetype Foo struct &#123; Name `json:"name"` Embedded `json:",inline"`&#125; Ref:1.https://blog.golang.org/json2.https://golang.org/pkg/encoding/json/3.https://play.golang.org/p/NnwTh9KI5r]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>GoPackage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Protocol Buffers and GRPC]]></title>
    <url>%2F2020%2F06%2F30%2FProtocol-Buffers-and-GRPC%2F</url>
    <content type="text"><![CDATA[Protocol BuffersProtocol buffers是由Google推出来的用于描述结构化数据的一种数据格式,类似于XML,但更小更快更简单，语言无关，平台无关而且支持可扩展。通过描述性语言定义你所需要的数据结构,可以实现一次定义即可以生成多种语言的相关的代码。目前Protocal buffers基本上已支持大多数的编程语言。 本文将介绍在Go语言中怎么使用Protocol buffers 1.protoc安装下载所需的版本，解压设置环境变量2.protoc-gen-go的安装 go install google.golang.org/protobuf/cmd/protoc-gen-go 3.新建一个Go Module12345➜ gopro mkdir grpc➜ gopro cd grpc➜ grpc mkdir src➜ cd src➜ src go mod init example.com/grpc 整个目录的结构如下 4.在protofiles中编辑person.pb.go文件。如下代码用protobuf声明式语言定义了一个数据结构123456789101112131415161718192021syntax = "proto3";message Person &#123; string name = 1; int32 id = 2; string email = 3; enum PhoneType &#123; MOBILE = 0; HOME = 1; WORK = 2; &#125; message PhoneNumber &#123; string number = 1; PhoneType type = 2; &#125; repeated PhoneNumber phones = 4;&#125;message AddressBook &#123; repeated Person people = 1;&#125; 5.通过proto生成Go语言对应的文件 在grpc/src/protofiles执行protoc --go_out=. *.proto会在当前目录下生成文件person.pb.go6.编写main.go和main_json.go文件123456789101112131415161718192021222324252627// main.gopackage mainimport ( pb "example.com/grpc/protofiles" //我们可以使用person.pb.go中的结构傅Person "fmt" "github.com/golang/protobuf/proto")func main() &#123; // 对Person进行初始化，并将指针赋值给变量p p := &amp;pb.Person&#123; Id: 1234, Name: "Roger F", Email: "rf@gmail.com", Phones: []*pb.Person_PhoneNumber&#123; &#123;Number: "555-4321", Type: pb.Person_HOME&#125;, &#125;, &#125; p1 := &amp;pb.Person&#123;&#125; body, _ := proto.Marshal(p) _ = proto.Unmarshal(body, p1) fmt.Println("Original struct loadef from proto file:",p,"\n") fmt.Println("Marshaled proto data: ", body, "\n") fmt.Println("Unmarshaled struct: ", p1)&#125; 1234567891011121314151617181920212223242526// main_json.gopackage mainimport ( "encoding/json" pb "example.com/grpc/protofiles" "fmt")func main() &#123; p := &amp;pb.Person&#123; Id: 1234, Name: "Roger F", Email: "rf@gmail.com", Phones: []*pb.Person_PhoneNumber&#123; &#123;Number: "555-4321", Type: pb.Person_HOME&#125;, &#125;, &#125; body, _ := json.Marshal(p) fmt.Println(string(body))&#125; 7.执行go mod tidy安装依赖。8.分别执行main.go和main_json.go Go类型与对应的Protobuf类型 Go Protobuf type float32 float float64 double uint32 fixed32 uint64 fixed64 []byte bytes Protobut type Default value string “” bytes empty bytes[] bool false int,int32,int64,float,double 0 enum 0 GRPC1.安装grpcGo语言相关库go get google.golang.org/grpcgo get google.golang.org/grpc/cmd/protoc-gen-go-grpc2.在src目录下新建grpc文件夹，最终的目录结构如下。3.定义transaction.proto12345678910111213141516syntax = "proto3";package grpc;message TransactionRequest &#123; string from = 1; string to = 2; float amount = 3;&#125;message TransactionResponse &#123; bool confirmation = 1;&#125;service MoneyTransaction &#123; rpc MakeTransaction(TransactionRequest) returns (TransactionResponse) &#123;&#125;&#125; 4.执行命令protoc --go_out=plugins=grpc:. transaction.proto,会在grpc目录下生成文件transaction.pb.go,文件中的部分内容。1234567891011121314151617181920212223242526272829303132333435363738type TransactionRequest struct &#123; state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields From string `protobuf:"bytes,1,opt,name=from,proto3" json:"from,omitempty"` To string `protobuf:"bytes,2,opt,name=to,proto3" json:"to,omitempty"` Amount float32 `protobuf:"fixed32,3,opt,name=amount,proto3" json:"amount,omitempty"`&#125;type TransactionResponse struct &#123; state protoimpl.MessageState sizeCache protoimpl.SizeCache unknownFields protoimpl.UnknownFields Confirmation bool `protobuf:"varint,1,opt,name=confirmation,proto3" json:"confirmation,omitempty"`&#125;type moneyTransactionClient struct &#123; cc grpc.ClientConnInterface&#125;func NewMoneyTransactionClient(cc grpc.ClientConnInterface) MoneyTransactionClient &#123; return &amp;moneyTransactionClient&#123;cc&#125;&#125;func (c *moneyTransactionClient) MakeTransaction(ctx context.Context, in *TransactionRequest, opts ...grpc.CallOption) (*TransactionResponse, error) &#123; out := new(TransactionResponse) err := c.cc.Invoke(ctx, "/grpc.MoneyTransaction/MakeTransaction", in, out, opts...) if err != nil &#123; return nil, err &#125; return out, nil&#125;func RegisterMoneyTransactionServer(s *grpc.Server, srv MoneyTransactionServer) &#123; s.RegisterService(&amp;_MoneyTransaction_serviceDesc, srv)&#125; 5.编写client.go和server.go 1234567891011121314151617181920212223242526272829303132333435363738394041// server.gopackage mainimport ( "log" "net" pb "example.com/grpc/grpc" "golang.org/x/net/context" "google.golang.org/grpc" "google.golang.org/grpc/reflection")const ( port = ":50051")// server is used to create MoneyTransactionServer.type server struct&#123;&#125;// MakeTransaction implements MoneyTransactionServer.MakeTransactionfunc (s *server) MakeTransaction(ctx context.Context, in *pb.TransactionRequest) (*pb.TransactionResponse, error) &#123; log.Printf("Got request for money Transfer....") log.Printf("Amount: %f, From A/c:%s, To A/c:%s", in.Amount, in.From, in.To) // Do database logic here.... return &amp;pb.TransactionResponse&#123;Confirmation: true&#125;, nil&#125;func main() &#123; lis, err := net.Listen("tcp", port) if err != nil &#123; log.Fatalf("Failed to listen: %v", err) &#125; s := grpc.NewServer() pb.RegisterMoneyTransactionServer(s, &amp;server&#123;&#125;) // Register reflection service on gRPC server. reflection.Register(s) if err := s.Serve(lis); err != nil &#123; log.Fatalf("Failed to serve: %v", err) &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738// client.gopackage mainimport ( "log" pb "example.com/grpc/grpc" "golang.org/x/net/context" "google.golang.org/grpc")const ( address = "localhost:50051")func main() &#123; // Set up a connection to the server. conn, err := grpc.Dial(address, grpc.WithInsecure()) if err != nil &#123; log.Fatalf("Did not connect: %v", err) &#125; defer conn.Close() //创建一个rpc客户端 c := pb.NewMoneyTransactionClient(conn) // Prepare data. Get this from clients like Frontend or App from := "1234" to := "5678" amount := float32(1250.75) // Contact the server and print out its response. r, err := c.MakeTransaction(context.Background(), &amp;pb.TransactionRequest&#123;From: from, To: to, Amount: amount&#125;) if err != nil &#123; log.Fatalf("Could not transact: %v", err) &#125; log.Printf("Transaction confirmed: %t", r.Confirmation)&#125; 6.分别执行server.go和client.go Ref：1.https://developers.google.com/protocol-buffers/docs/gotutorial2.Building RESTful Web services with Go3.GRPC]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-module的使用]]></title>
    <url>%2F2020%2F06%2F27%2Fgo-module%2F</url>
    <content type="text"><![CDATA[Go 1.11和1.12已经基本上支持Go Modules,Go的新依赖管理系统，更好的管理依赖的版本信息。 本文使用版本Go1.13.在Go Modules之前，每个项目都必须设置一个GOPATH,并将第三方包安装在各自的目录下，多个项目之间不能共用第三方包。 123456789101112GO111MODULEGO111MODULE 有三个值：off, on和auto（默认值）。GO111MODULE=off，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找GO111MODULE=on，go命令行会使用module功能GO111MODULE=auto，默认值，go命令行将会根据当前目录来决定是否启用module功能如下两种情况会启用module功能：－ 当前目录在GOPATH/src之外且该目录包含go.mod文件－ 当前文件在包含go.mod文件的目录下面。当module功能启用时，依赖包的存放位置变更为$GOPATH/pkg，允许同一个package多个版本并存，且多个项目可以共享缓存的 module。 新建一个module在$GOPATH/src目录之外新建一个目录hello,并新建一个文件server.go整个目录结构如下：123456789101112131415161718192021➜ gopro mkdir hello➜ gopro cd hello➜ hello vim hello.go➜ hello cat hello.gopackage hellofunc Hello() string &#123; return "Hello, world."&#125;➜ hello cat hello_test.go package helloimport "testing"func TestHello(t *testing.T) &#123; want := "Hello, world." if got := Hello(); got != want &#123; t.Errorf("Hello() = %q, want %q", got, want) &#125;&#125; 到目前为止这个目录包含package,但不是一个module，因为不包含go.mod文件。执行go test,将会提示如下错误(使用的go版本为1.13)12➜ hello go testgo: cannot find main module; see 'go help modules' 使用go mod init把目录转换成为一个module123456789➜ hello go mod init example.com/hellogo: creating new go.mod: module example.com/hello➜ hello go testPASSok example.com/hello 0.002s➜ hello cat go.mod module example.com/hello //定义了 module path,是导入路径的根目录go 1.13 go.mod文件一般只出现于module的根目录。如果你在module下面新建了一个子目录world,world会自动被认为是example.com/hello的一部分，导入路径为example.com/hello/world。(go.mod所在的目录被认为是一个module的根目录,导入路径为example.com/hello加入对应的package名称) 添加依赖12345678➜ hello cat hello.gopackage helloimport "rsc.io/quote"func Hello() string &#123; return quote.Hello()&#125; 执行go test命令,该命令会自动下载所需要的包。并将所需的依赖写入到go.mod123456789101112➜ hello go testgo: finding rsc.io/quote v1.5.2go: downloading rsc.io/quote v1.5.2go: extracting rsc.io/quote v1.5.2go: downloading rsc.io/sampler v1.3.0go: extracting rsc.io/sampler v1.3.0go: downloading golang.org/x/text v0.0.0-20170915032832-14c0d48ead0cgo: extracting golang.org/x/text v0.0.0-20170915032832-14c0d48ead0cgo: finding rsc.io/sampler v1.3.0go: finding golang.org/x/text v0.0.0-20170915032832-14c0d48ead0cPASSok example.com/hello 0.003s 123456➜ hello cat go.mod module example.com/hellogo 1.13require rsc.io/quote v1.5.2 go命令还维护了一个文件go.sum,里面定义了go.mod包中所需的依赖项。1234567➜ hello cat go.sum golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c h1:qgOY6WgZOaTkIIMiVjBQcw93ERBE4m30iBm00nkL0i8=golang.org/x/text v0.0.0-20170915032832-14c0d48ead0c/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=rsc.io/quote v1.5.2 h1:w5fcysjrx7yqtD/aO+QwRjYZOKnaM9Uh2b40tElTs3Y=rsc.io/quote v1.5.2/go.mod h1:LzX7hefJvL54yjefDEDHNONDjII0t9xZLPXsUe+TKr0=rsc.io/sampler v1.3.0 h1:7uVkIFmeBqHfdjD+gZwtXXI+RODJ2Wc4O7MPEh/QiW4=rsc.io/sampler v1.3.0/go.mod h1:T1hPZKmBbMNahiBKFy5HrXp6adAjACjK9JXDnKaTXpA= 更新依赖1234567➜ hello go get golang.org/x/textgo: finding golang.org/x/text v0.3.3go: downloading golang.org/x/text v0.3.3go: extracting golang.org/x/text v0.3.3➜ hello go testPASSok example.com/hello 0.003s 此时查看依赖项,golang.org/x/text已经被升级到v0.3.312345678910111213141516➜ hello go list -m allgo: finding golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6eexample.com/hellogolang.org/x/text v0.3.3golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6ersc.io/quote v1.5.2rsc.io/sampler v1.3.0➜ hello cat go.modmodule example.com/hellogo 1.13require ( golang.org/x/text v0.3.3 // indirect rsc.io/quote v1.5.2) 尝试升级rsc.io/sampler12345678910➜ hello go get rsc.io/samplergo: finding rsc.io/sampler v1.99.99go: downloading rsc.io/sampler v1.99.99go: extracting rsc.io/sampler v1.99.99➜ hello go test--- FAIL: TestHello (0.00s) hello_test.go:8: Hello() = "99 bottles of beer on the wall, 99 bottles of beer, ...", want "Hello, world."FAILexit status 1FAIL example.com/hello 0.002s rsc.io/sampler包是更新成功了，但go test命令执行失败。查看rsc.io/sampler版本信息12➜ hello go list -m -versions rsc.io/samplerrsc.io/sampler v1.0.0 v1.2.0 v1.2.1 v1.3.0 v1.3.1 v1.99.99 导致出错的原因可能是，rsc.io/sampler@v1.99.99与原来版本的包不兼容所致。我们重新安装rsc.io/sampler@v1.3.1然后再进行测试。1234567➜ hello go get rsc.io/sampler@v1.3.1go: finding rsc.io/sampler v1.3.1go: downloading rsc.io/sampler v1.3.1go: extracting rsc.io/sampler v1.3.1➜ hello go testPASSok example.com/hello 0.003s 添加对新主版本的依赖修改hello.go的内容如下:123456789101112131415➜ hello cat hello.gopackage helloimport ( "rsc.io/quote" quoteV3 "rsc.io/quote/v3")func Hello() string &#123; return quote.Hello()&#125;func Proverb() string &#123; return quoteV3.Concurrency()&#125; 修改hello_test.go的内容如下：1234567891011121314151617package helloimport "testing"func TestHello(t *testing.T) &#123; want := "Hello, world." if got := Hello(); got != want &#123; t.Errorf("Hello() = %q, want %q", got, want) &#125;&#125;func TestProverb(t *testing.T) &#123; want := "Concurrency is not parallelism." if got := Proverb(); got != want &#123; t.Errorf("Proverb() = %q, want %q", got, want) &#125;&#125; 执行go test命令123456➜ hello go test go: finding rsc.io/quote/v3 v3.1.0go: downloading rsc.io/quote/v3 v3.1.0go: extracting rsc.io/quote/v3 v3.1.0PASSok example.com/hello 0.003s 查看rsc.io/quote,发现存在不同版本的两个包。123➜ hello go list -m rsc.io/q...rsc.io/quote v1.5.2rsc.io/quote/v3 v3.1.0 升级新主版本的依赖因为一些原因我们需要将quote升级到V3版本。修改hello.go的内容如下：1234567891011121314➜ hello cat hello.gopackage helloimport ( quoteV3 "rsc.io/quote/v3")func Hello() string &#123; return quoteV3.HelloV3()&#125;func Proverb() string &#123; return quoteV3.Concurrency()&#125; 执行go test命令123➜ hello go testPASSok example.com/hello 0.003s 移除没有使用的依赖在上一步中，我们的代码已经没有使用rsc.io/quote/v1版本，但rsc.io/quote v1.5.2还是存在于依赖中。1234567➜ hello go list -m allexample.com/hellogolang.org/x/text v0.3.3golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6ersc.io/quote v1.5.2rsc.io/quote/v3 v3.1.0rsc.io/sampler v1.3.1 因为构建单个软件包（例如使用go build或go test）可以轻松判断出什么时候缺少什么东西和需要添加什么东西，但是不能确定什么时候可以安全地删除东西。 仅在检查模块中的所有软件包以及这些软件包的所有可能的构建标记组合之后，才能删除依赖项。 普通的build命令不会加载此信息，因此它不能安全地删除依赖项。 使用go mod tidy命令清除未使用的依赖。1234567➜ hello go mod tidy➜ hello go list -m allexample.com/hellogolang.org/x/text v0.3.3golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6ersc.io/quote/v3 v3.1.0rsc.io/sampler v1.3.1 go mod相关命令: COMMAND DESC download download modules to local cache edit edit go.mod from tools or scripts graph print module requirement graph init initialize new module in current directory tidy add missing and remove unused modules vendor make vendored copy of dependencies verify verify dependencies have expected content why explain why packages or modules are needed Ref：1.https://blog.golang.org/using-go-modules2.https://juejin.im/post/5c8e503a6fb9a070d878184a]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuberntes Custom Resources]]></title>
    <url>%2F2020%2F06%2F27%2Fkuberntes-Custom-Resources%2F</url>
    <content type="text"><![CDATA[Custom resources自定义资源(Custom Resources):是对现有Kubernetes API的一种扩展。当你创建一个CR后，你可以使用kubectl命令，像操作Pod一样操作自定义资源。 Custom controllerscustom resources只是定义了如何存储和查询结构化数据。如果要想CR真正的被使用起来，还需要定义Custom controllers,定义Custom controllers之后才可以真正的提供描述性接口。描述性接口允许你只定义资源所需要达到的理想状态，控制器会为了达到理想状态进行一系列的操作，并维护这个理想状态。｀Operatorr pattern可以使Custom resourcs和Custom controllers`完美结合。 如何添加custom resourcesk8s提供了两种方法添加自定义资源到kubernetes集群。 － CRD:CustomResourceDefinitions:简单不需要编程 API Aggregation:功能强大需要编程 两者使用难度对比： CRDs Aggregated API Do not require programming. Users can choose any language for a CRD controller. Requires programming in Go and building binary and image. No additional service to run; CRDs are handled by API server. An additional service to create and that could fail. No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades. May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server. No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API. You need to handle multiple versions of your API; for example, when developing an extension to share with the world. Ref：1.https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s源码结构]]></title>
    <url>%2F2020%2F06%2F26%2Fk8s%E6%BA%90%E7%A0%81%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[源码目录 说明 cmd/ 存放可执行文件的入口代码，每个可执行文件都会对应一个main函数 pkg/ 存放核心库代码，可被项目内部或外部直接引用 vendor/ 存放项目依赖的库代码，一般为每三方库代码 api/ 存放OpenAPI/Swagger文件 build/ 存放构建相关的脚本 test/ 存放测试工具及测试数据 docs/ 存放设计或用户使用文档 hack/ 存放与构建，测试等相关的脚本 third_party/ 存放第三方工具，代码或其他组件 plugin/ 存放Kubernetes插件代码目录，例如认证，授权等相关插件 staging/ 存放部分核心库的暂存目录 translations/ 存放il8n(国际化)语言包的相关文件，可以在不修改内部代码的情况下支持不同语言及地区 k8s.io/kubernetes/cmd/kubectl/kubectl.go123456789101112131415161718func main() &#123; rand.Seed(time.Now().UnixNano()) //随机数种子 command := cmd.NewDefaultKubectlCommand() // TODO: once we switch everything over to Cobra commands, we can go back to calling // cliflag.InitFlags() (by removing its pflag.Parse() call). For now, we have to set the // normalize func and add the go flag set by hand. pflag.CommandLine.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) pflag.CommandLine.AddGoFlagSet(goflag.CommandLine) // cliflag.InitFlags() logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil &#123; os.Exit(1) &#125;&#125; Makefile: 顶层Makefile文件，描述了整个项目所有代码文件的编译顺序，编译规则及编译后的二进制输出等Makefile.generated_files:描述了代码生成的逻辑 1apt install cloc //代码统计工具 结构体，json,protobufk8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/types.go12345678910111213141516171819type APIResource struct &#123; Name string `json:"name" protobuf:"bytes,1,opt,name=name"` SingularName string `json:"singularName" protobuf:"bytes,6,opt,name=singularName"` Namespaced bool `json:"namespaced" protobuf:"varint,2,opt,name=namespaced"` Group string `json:"group,omitempty" protobuf:"bytes,8,opt,name=group"` Version string `json:"version,omitempty" protobuf:"bytes,9,opt,name=version"` Kind string `json:"kind" protobuf:"bytes,3,opt,name=kind"` ShortNames []string `json:"shortNames,omitempty" protobuf:"bytes,5,rep,name=shortNames"` Categories []string `json:"categories,omitempty" protobuf:"bytes,7,rep,name=categories"` StorageVersionHash string `json:"storageVersionHash,omitempty" protobuf:"bytes,10,opt,name=storageVersionHash"`&#125; 12345678910111213141516171819202122232425262728293031323334// staging/src/k8s.io/apimachinery/pkg/runtime/schema/group_version.go// 标识一个资源type GroupVersionResource struct &#123; Group string Version string Resource string&#125;type GroupKind struct &#123; Group string Kind string&#125;type GroupVersion struct &#123; Group string Version string&#125;type GroupResource struct &#123; Group string Resource string&#125;type GroupVersionKind struct &#123; Group string Version string Kind string&#125;Kind与Resource有何区别？Kind the name of a particular object schema (e.g. the "Cat" and "Dog" kinds would have different attributes and properties)Resource a representation of a system entity, sent or retrieved as JSON via HTTP to the server. Resources are exposed via: - Collections - a list of resources of the same type, which may be queryable - Elements - an individual resource, addressable via a URL 123456type TypeMeta struct &#123; // Kind is a string value representing the REST resource this object represents. Kind string `json:"kind,omitempty" protobuf:"bytes,1,opt,name=kind"` APIVersion string `json:"apiVersion,omitempty" protobuf:"bytes,2,opt,name=apiVersion"`&#125; 外部版本的资源定义在vendor/k8s.io/api目录下，完整路径为vendor/k8s.io/api/&lt;group&gt;/&lt;version&gt;/&lt;resource file&gt; kubectl api-versions列出当前支持的资源组和资源版本kubectl api-resources列出当前 支持的资源列表 k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/runtime/interfaces.go12345678// Object interface must be supported by all API types registered with Scheme. Since objects in a scheme are// expected to be serialized to the wire, the interface an Object must provide to the Scheme allows// serializers to set the kind, version, and group the object is represented as. An Object may choose// to return a no-op ObjectKindAccessor in cases where it is not expected to be serialized.type Object interface &#123; GetObjectKind() schema.ObjectKind DeepCopyObject() Object&#125; 目前Kubernetes系统中的所有资源类型都已注册到Scheme资源注册表中，其是一个内存型的注册表UnversionedType:无版本资源类型KnownType:有版本资源类型 yamlSerializer使用第三方库gopkg.in/yaml.v2来实现序列化和反序列化操作。 github.com/json-iteractor/go 可通过kubectl convert命令进行资源版本转换 // pkg/util/tail/tail.goFindTailLineStartIndex]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s Golang代码编码规范]]></title>
    <url>%2F2020%2F06%2F24%2Fk8s%E4%BB%A3%E7%A0%81%E7%BC%96%E7%A0%81%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[总结下k8s项目编码规范，加深记忆。 编码规范可参阅Effective Go. 1.Gofmt 所有的代码都必须使用gofmt进行格式化，可选的工具还包括goimports(可看作gofmt的超集) 2.注释语句 注释必须完整，清楚，允许一定的冗余。注释应该必须以描述的资源开头。 比如你注释的是一个函数就应该以函数名开头，注释的是一个结构体就应该以结构体的名称开头并以.结尾 12345 // Request represents a request to run a command.type Request struct &#123; ...// Encode writes the JSON encoding of req to w.func Encode(w io.Writer, req *Request) &#123; ... 3.Context如果一个函数中有参数类型为Context,一般把它作为第一个Ref1func F(ctx context.Context, /* other arguments */) &#123;&#125; 4.复制To avoid unexpected aliasing, be careful when copying a struct from another package. For example, the bytes.Buffer type contains a []byte slice. If you copy a Buffer, the slice in the copy may alias the array in the original, causing subsequent method calls to have surprising effects. In general, do not copy a value of type T if its methods are associated with the pointer type, *T. 5.Crypto Rand不要使用math/rand来生成私钥，即使只生成一次。如果没有设置种子，生成器生成的随机数完成是可预测的。使用time.Nanosecond()作为种子，种子差异比较小，使用crypto/rand来生成。1234567891011121314151617import ( "crypto/rand" // "encoding/base64" // "encoding/hex" "fmt")func Key() string &#123; buf := make([]byte, 16) _, err := rand.Read(buf) if err != nil &#123; panic(err) // out of randomness, should never happen &#125; return fmt.Sprintf("%x", buf) // or hex.EncodeToString(buf) // or base64.StdEncoding.EncodeToString(buf)&#125; 6.空切片定义一个空切片，使用var t []string而不是t := []string{}。前者描述的是一个空的切片,后者是non-nil但长度为0的切片。两者在功能上是一样的(len和cap都为0)，但是我们倾向于使用前者。 当编码为一个JSON对象时，nil—&gt; null,[]string{} —&gt; JSON array []。 7.文档注释－Doc CommentsAll top-level, exported names should have doc comments, as should non-trivial unexported type or function declarations. See https://golang.org/doc/effective_go.html#commentary for more information about commentary conventions. 8.Don’t Panic不要使用panic处理普通的错误。使用多返回值进行处理。 9.Error Strings日志字符串，一般不进行首字母大写(除非是专有名词)use1fmt.Errorf("something bad") not1fmt.Errorf("Something bad") 10.示例当我们新增一个package时，需要提供一些可运行的例子或者简单的测试例子。 https://blog.golang.org/examples 11.Goroutine生命周期Goroutine会导致内存泄漏当channel阻塞于发送或接收：垃圾收集器并不会终止Goroutine即使channel一直阻塞。尽量使并发代码简单明了，Goroutine生命周期明显。如果过于复杂，可以添加注释表明Goroutine何时和为什么退出. 12.处理错误https://golang.org/doc/effective_go.html#errors 13. Imports避免重命名imports,除非为了避免命名冲突;好的模块名一般不需要进行重命名。 对imports进行分类排序，goimports可以帮你完成这个操作。 14. 控制流use12345if err != nil &#123; // error handling return // or continue, etc.&#125;// normal code not12345if err != nil &#123; // error handling&#125; else &#123; // normal code&#125; 15. 首字母缩略词 专有名词缩略词采用全大写。 如：URL,URLPony,ServeHTTP,appID 16.InterfacesGo接口通常属于使用接口类型的值的包，而不是实现这些值的包。 实现包应返回具体的（通常是指针或结构）类型：这样，可以将新方法添加到实现中，而无需进行大量重构。 不要在“用于模拟”的API的实现者端定义接口； 而是设计API，以便可以使用实际实现的公共API对其进行测试。 在使用接口之前，不要先定义它们：如果没有实际的用法示例，很难知道接口是否是必需的，更不用说接口应该包含什么方法了。 17.模块注释模块注释必须在package main上面，而且不能有空格。12// Package math provides basic constants and mathematical functions.package math 18.模块 Import Blankimport _ &quot;pkg,_操作其实是引入该包,不直接使用包里面的函数,而是调用了该包里的init函数。不要这样使用，除非在main package中或者测试需要－ Import Dotimport . &quot;fmt&quot;可以省略包名调用包中的函数，这样会使代码可读性变差。除了在以一测试这种情况下尽量不要使用这种导入。123456package foo_testimport ( "bar/testutil" // also imports "foo" . "foo") 在上面这种情况下，测试文件不能导入package foo因为bar/testutil中已经导入了foo。使用import .表示假装引入了包。这里还是有点不太明白？ － Package Names所有对包内资源的引用都必须通过包名，所以包名的命名是很重要的而且你可以适当省略掉一些无用的名称。如chubby.Filebetter than chubby.ChubbyFile。 19.传值不要试图通过传递指针来节省空间。常见的实例包括传递指向字符串的指针（ string）或指向接口值的指针（ io.Reader）。 在这两种情况下，值本身都是固定大小，可以直接传递。 此建议不适用于大型struct，甚至不适用于可能增长的小型struct。 20.接收者名称与接收者类型 － 接收者名称接收者名称必须使用有意义的名称，不能使用一些通用的名称，如me,this,self等。 － 接收者类型1.如果接收者是map,func,chan,不要使用指针。如果接收者是slice而且函数没有对slice进行reslice和reallocate操作，不要使用指针。2.如果函数需要对接收都进行修改，则必须使用指针。3.如果接收者是一个结构体并且包含sync.Mutext或者类似的同步字段，必须使用指针防止拷贝。4.如果接收者是大的数组，结构体，使用指针会更高效。Assume it’s equivalent to passing all its elements as arguments to the method. If that feels too large, it’s also too large for the receiver。5.如果接收者是struct,array,slice，它们其中有字段指向可变的结构，最好使用指针。6.如果接收方是一个很小的数组或结构，自然是一个值类型（例如，诸如time.Time类型），没有可变字段且没有指针，或者仅仅是一个简单的基本类型（如int或string），则value接收者是有道理的。 值接收器可以减少可以生成的垃圾数量； 如果将值传递给value方法，则可以使用堆栈上的副本来代替在堆上分配。 （编译器会尽量避免这种分配，但是它不可能总是成功。）由于这个原因，请勿在没有进行概要分析的情况下选择值接收器类型。7.如果不确认使用如种类型，使用指针接收者。 21.同步函数最好使用同步函数，同步函数直接返回结果，在返回之前完成所的调用或channel ops。同步函数更容易使用goroutine,更好的避免内存泄漏和数据竞争。还有一个好处就是使用同步方法可以更好的进行测试。如果调用者需要更高的并发性，可以将这个改造成一个单独的goroutine。 22.出现错误必须提供有用的信息对必要的地方，可能出错的地方打印日志。应该包含如下信息什么错误，输入是什么，实际的结果是什么，预期的结果是什么。123if got != tt.want &#123; t.Errorf("Foo(%q) = %d; want %d", tt.in, got, tt.want) // or Fatalf, if test can't test anything more past this point&#125; 23.变量名称在Go中变量尽量简单明了。对于有限作用域的局部变量，尽量使用简短的变量。如lincCount—&gt;c,sliceIndex—&gt;i。 内建函数new和make。new这个一个用来分配内存的内建函数，但它不初始化内存，只是将其置零。new(T)会为T类型的新项目分配被置零的存储，并且返回它的地址，一个类型为*T的值(返回一个指向新分配的类型为T,值为零的指针)。new([]int)返回一个指向新分配的，被置零的slice结构体的指针，即指向nil slice值的指针。 内建函数make(T, args)与new(T)的用途不一样，它只用于来创建slice,map,channel,并且返回一个初始化的的(而不是置零)，类型为T`的值。 12var p *[]int = new([]int)var v []int = make([]int, 100) Ref：1.https://github.com/kubernetes/community/blob/master/contributors/guide/coding-conventions.md2.effective-go3.https://godoc.org/golang.org/x/tools/cmd/goimports]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang prng]]></title>
    <url>%2F2020%2F06%2F24%2FGolang-prng%2F</url>
    <content type="text"><![CDATA[伪随机数生成器－pseudo-random number generator (PRNG)。实现位于math/rand 1234567891011121314package mainimport ( "fmt" "math/rand" "time")func main() &#123; // math/rand如果不指定种子,则默认的种子为1;程序每次运行产生的随机数都是一样的;所以这里指定种子纳表级别的时间戳为种子,确保每次生成数都是随机的. rand.Seed(time.Now().UnixNano()) r := rand.Intn(100) fmt.Println(r)&#125; Ref：1.https://golang.org/pkg/math/rand/]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio bookinfo]]></title>
    <url>%2F2020%2F06%2F13%2Fistio-bookinfo%2F</url>
    <content type="text"><![CDATA[1.安装istio,如未安装k8s集群，可Refgotok8s安装单机版集群 2.设置default命名空间自动注入1kubectl label namespace default istio-injection=enabled 3.部署应用1kubectl apply -f bookinfo.yaml 4.查看service和pods是否启动 5.设置Istio Gatewayapply -f bookinfo-gateway```123456786.确认`Ingress`端口和`IP`![image](http://img.hysyeah.top/2020/6/13/20200613174652-gateway.png)```pythonexport INGRESS_HOST=10.101.124.143export INGERSS_PORT=31176export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT 7.在浏览器访问服务, 8.访问kiali,在浏览器中输入对应的IP和端口。 Ref：1.https://istio.io/latest/zh/docs/examples/bookinfo/]]></content>
  </entry>
  <entry>
    <title><![CDATA[ubuntu20.04开发开境配置]]></title>
    <url>%2F2020%2F05%2F30%2Fubuntu20-04%E5%BC%80%E5%8F%91%E5%BC%80%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[记录下新系统下开发必须安装的工具 1.小飞机安装注意事项:需要安装python21sudo apt install python 2.vscode安装，直接下载deb包进行安装，不要使用snap命令进行安装，否则会出现不能输入中文的问题。3.安装截图工具flameshot,并设置快捷键1sudo apt install flameshot 4.安装chrome,配置插件SwitchyOmega.5.安装polipo,终端使用代理123456cat &lt;&lt;EOF &gt;&gt; /etc/apt/sources.listdeb http://cz.archive.ubuntu.com/ubuntu xenial main universeEOFsudo apt-get updatesudo apt install polipo 修改配置/etc/polipo/config,加入如下内容1234567891011proxyAddress = &quot;0.0.0.0&quot;socksParentProxy = &quot;127.0.0.1:1080&quot;socksProxyType = socks5chunkHighMark = 50331648objectHighMark = 16384serverMaxSlots = 64serverSlots = 16serverSlots1 = 32 重启服务sudo systemctl restart polipo配置代理12export http_proxy=http://127.0.0.1:8123export https_proxy=http://127.0.0.1:8123 6.使用系统自带的五笔输入法7.安装GoLand,PyCharm8.安装golang1.13.69.安装terminator1sudo apt install terminator 10.安装git,zsh,oh-my-zsh，autojump123456789sudo apt install gitsudo apt install zshsh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;cat &lt;&lt;EOF &gt;&gt; ~/.zshrc. /usr/share/autojump/autojump.shEOFsource ~/.zshrc 11.安装qshell,用于同步文件到七牛云，下载qshell 设置帐户空间1qshell account [&lt;AccessKey&gt; &lt;SecretKey&gt; &lt;Name&gt;] [flags] 设置文件上传策略,vim ~/.qshell/upload.conf1234567&#123; &quot;src_dir&quot;: &quot;/media/hys/65F33762C14D581B/hysyeah_qiniu&quot;, &quot;bucket&quot;: &quot;hysyeah&quot;, &quot;check_exists&quot;: true, &quot;check_hash&quot;: true, &quot;rescan_local&quot;: true&#125; 设置快捷上传命令e123cat &lt;&lt;EOF &gt;&gt; ~/.zshrcalias qsupload=&quot;qs qupload ~/.qshell/upleeeeoad.conf&quot;EOF 设置镜像源,ubuntu20.04可以在界面上设置 问题： 解决双系统时间不一致的问题,在ubuntu终端输入如下命令1timedatectl set-local-rtc 1 --adjust-system-clock Ref:1.ubuntu package2.qshell]]></content>
  </entry>
  <entry>
    <title><![CDATA[github 工作流]]></title>
    <url>%2F2020%2F05%2F23%2Fgithub-%E5%B7%A5%E4%BD%9C%E6%B5%81%2F</url>
    <content type="text"><![CDATA[如果你要参与一个开源项目你就必须知道github工作流，github工作流其实就是多人在参与同一个项目的一种协作方式。 假如你想要参与kubernetes项目，你就得了解github工作流。 1.首先你需要将kubernetes项目的代码Fork到你的github仓库中 2.然后将你仓库中的代码克隆到你本地的工作目录。1git clone https://github.com/$user/kubernetes.git 3.更新本地代码,不建设使用git pull因为git pull是做一个合并操作，会使commit变的杂乱。或者改变git配置，git config branch.autoSetupRebase always。 123git fetch upstreamgit checkout mastergit rebase upstream/master 4.创建新的分支，并在此分支上进行开发git checkout -b mybrach 5.保存更改后的代码12git add &lt;files&gt;git commit -m 6.将更新后的代码，推到你Fork的仓库中git push -f ${your_remote_name} myfeature 7.创建一个合并请求怎样创建合并请求 8.当创建合并请求之后，会给这个合并请求分配reviewers并进行一系列的检查 ，当这些都通过之后代码会进行合并。若存在一些问题，则需要进行一些修改。使用如下命令进行 操作12git commit --amendgit push -f $remotename mybrach 图片来源 Ref：1.https://github.com/kubernetes/community/blob/master/contributors/guide/github-workflow.md2.https://faust.readthedocs.io/en/latest/contributing.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Golang竞争条件与锁]]></title>
    <url>%2F2020%2F03%2F23%2FGolang%E7%AB%9E%E4%BA%89%E6%9D%A1%E4%BB%B6%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[竞争条件指的是程序在多个线程交叉执行时,引发的不可预知的错误。 12345678910111213141516package mainimport &quot;fmt&quot;var data intfunc main() &#123; go func() &#123; data++ &#125;() if data==0&#123; fmt.Printf(&quot;the value is %v.\n&quot;, data) &#125;&#125; 这段代码可能会出现如下三种不同情况： 不打印任何东西;第6行早于第9行执行 打印the value is 0;第9行和第10行早于第6行执行 打印the value is 1;执行第9行后，然后执行第6行，再执行第10行 竞态会导致程序的结果是不可预期的，而且不方便发现和调试。怎么解决这个问题呢？最简单的方法就是对访问到了变量的地方进行加锁操作。1234567891011121314151617181920package mainimport &quot;fmt&quot;import &quot;sync&quot;var data intvar mu sync.Mutexfunc main() &#123; go func() &#123; mu.Lock() data++ mu.Unlock() &#125;() mu.Lock() if data==0&#123; fmt.Printf(&quot;the value is %v.\n&quot;, data) &#125; mu.Unlock()&#125; 这样的话就不会产生竞态问题，但这样看上去不怎么优雅，而且锁用多了会很影响程序的性能。 也可以使用golang的channel,使data++操作早于print,所以下面的程序不会输出任何东西。1234567891011121314151617181920package mainimport &quot;fmt&quot;var data intvar d = make(chan int)func main() &#123; go func() &#123; data++ d &lt;- data &#125;() data = &lt;-d //如果没有数据将一直等待 if data == 0&#123; fmt.Printf(&quot;the value is %v.\n&quot;, data) &#125;&#125; 数据竞争发生的条件： 两个或两个以上的线程同时访问相同的变量 最少一个为写操作 没有使用任何同步机制 一.未使用任何同步机制的例子123456789101112// vim bank.gopackage bankvar balance intfunc Deposit(amount int) &#123; balance = balance + amount&#125;func Balance() int &#123; return balance&#125; 1234567891011121314151617// vim bank_main.gopackage mainimport ( &quot;fmt&quot; &quot;github.com/bank&quot;)func main() &#123; // Alice go func() &#123; bank.Deposit(200) fmt.Println(&quot;=&quot;, bank.Balance()) &#125;() // Bob go bank.Deposit(100)&#125; 执行go run src/bank_main.go,意外的发现没有打印任何信息。这是为什么呢?因为运行Go程序时,至少会启动一个main goroutine,当main goroutine早于其它协程执行之前退出,其它协程还没来得及执行就结束了。执行go run -race src/bank_main.go却打印了= 200。 当两个goroutine同时访问balance变量时,假如Alice读取了balance并要将数据写入到balance的时候,如果此时切换到Bob,并将数据写入到了balance,然后再切换到Alice,并写入balance,这样就会将Bob的操作进行覆盖,这就是发生了数据竞争。二.使用channel避免多个goroutine访问变量,解决数据竞争问题12345678910111213141516171819202122package bankvar deposits = make(chan int) // send amount to depositvar balances = make(chan int) // receive balancefunc Deposit(amount int) &#123; deposits &lt;- amount &#125;func Balance() int &#123; return &lt;-balances &#125;func teller() &#123; var balance int // balance is confined to teller goroutine for &#123; select &#123; case amount := &lt;-deposits: balance += amount case balances &lt;- balance: &#125; &#125;&#125;func init() &#123; go teller() // start the monitor goroutine&#125; 执行go run -race src/bank_main.go未发现竟争条件问题 三.允许多个goroutine访问变量,但是在同一个时刻最多只有一个goroutine访问。1234567891011121314151617181920212223242526272829package mainimport ( &quot;fmt&quot;)var ( sema = make(chan struct&#123;&#125;, 1) balance int)func Deposit(amount int) &#123; sema &lt;- struct&#123;&#125;&#123;&#125; balance += amount fmt.Print(&quot;balance&quot;, balance) &lt;-sema&#125;func Balance() int &#123; sema &lt;- struct&#123;&#125;&#123;&#125; b := balance &lt;-sema return b&#125;func main() &#123; go Deposit(10)&#125; 12345678910111213141516import &quot;sync&quot;var (mu sync.Mutex // guards balancebalance int)func Deposit(amount int) &#123;mu.Lock()balance = balance + amountmu.Unlock()&#125;func Balance() int &#123;mu.Lock()b := balancemu.Unlock()return b&#125; 使用Lock与channel有什么优缺点？ Deadlocks, Livelocks, Starvation Deadlocks(死锁),在维基百科的解释为:In concurrent computing, a deadlock is a state in which each member of a group is waiting for another member, including itself, to take action, such as sending a message or more commonly releasing a lock.图片来源：wikipedia 进程P1持有资源R2,进程P2持有资源R1,当进程P1请求资源R1,进程P2请求资源R2的时候这种情况下就会发生死锁,如果没有外部干预进程会一直卡死。 死锁发生必须的4个条件,因为这个4个条件来源的于G. Coffman, Jr.的一篇，因此也被称为Coffman conditions。 互斥：一个资源在同一时间只能被一个进程访问 等待条件：进程同时占用至少一个资源,而且同时请求对其它的资源 非抢占：资源只能被占用它的那个进程释放 循环等待：比如P1等待P2释放资源,P2等待P1释放,形成了环状结构 使用Golang模拟死锁发生1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)type value struct &#123; mu sync.Mutex value int&#125;var wg sync.WaitGroupvar printSum = func(v1,v2 *value) &#123; defer wg.Done() v1.mu.Lock() defer v1.mu.Unlock() time.Sleep(2 * time.Second) v2.mu.Lock() defer v2.mu.Unlock() fmt.Printf(&quot;sum=%v\n&quot;,v1.value+v2.value)&#125;func main() &#123; var a, b value wg.Add(2) go printSum(&amp;a, &amp;b) //Goroutine p1 go printSum(&amp;b, &amp;a) // Goroutine p2 wg.Wait()&#125; Golang居然检测出来发生了死锁，那Golang是如何进行死锁检测的呢？ 图片来源：Concurrenecy in Go 程序运行时,goroutine p1获取a锁,此时p1持有a的锁,休眠,然后goroutine p2开始执行,并持有资源b的锁,然后p1请求资源b,p2请求资源a,发生死锁。 Livelocks(活锁)活锁就是程序一直在运行，但是一直没有改变进程本身的状态。举个例子，比如你走在一个狭窄的通道，对面迎过来一个人，他看见了你就走到了另一边打算让你通过，而你也走到了另一边，当你们一直重复这两个动作时就发生了活锁。 StarvationIn computer science, resource starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work.在计算机科学中，资源匮乏是并发计算中遇到的一个问题，在该问题中，永久拒绝某个进程来处理其工作所需的资源 Ref：1.https://en.wikipedia.org/wiki/Race_condition2.The Go programing language3.The Go Memory Model4.Concurrency in Go5.https://en.wikipedia.org/wiki/Deadlock6.(https://github.com/kat-co/concurrency-in-go-src(https://github.com/kat-co/concurrency-in-go-src)7.https://en.wikipedia.org/wiki/Starvation_(computer_science))]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用cargo管理外部依赖]]></title>
    <url>%2F2020%2F03%2F22%2F%E4%BD%BF%E7%94%A8cargo%E7%AE%A1%E7%90%86%E5%A4%96%E9%83%A8%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[1.使用cargo创建项目,会在当前目录下生成hello_cargo文件夹cargo new hello_cargo --bin 2.使用Goland打开项目hello_cargo,目录如下。其中Cargo.toml中[package]定义的是关于项目的一些信息,[dependencies]中定义的是外部依赖 3.引入外部包rand,rand在Cargo.toml文件[dependencies]下加入rand = &quot;0.3.14&quot; 4.cargo build将会从远端下载rand包,包括rand的依赖包 5.使用外部包1234567891011121314151617181920extern crate rand; //表明rand是外部依赖use std::io;use rand::Rng;fn main() &#123; println!(&quot;Guess the number!&quot;); let secret_number = rand::thread_rng().gen_range(1, 101); println!(&quot;The secret number is:&#123;&#125;&quot;, secret_number); println!(&quot;Please input your guess.&quot;); let mut guess = String::new(); io::stdin().read_line(&amp;mut guess) .expect(&quot;Failed to read line&quot;); println!(&quot;You guessed:&#123;&#125;&quot;, guess);&#125; 6.编译执行 cargo相关命令 command desc rustup doc 打开本地文档 rustc main.rs 编译 rustup update 更新Rust工具链和rustup cargo new —bin 新建一个可执行应用 cargo build 生成可执行文件 cargo update 更新外部依赖,并更新Cargo.lock文件 cargo build —release 生成可发布的执行文件,Rust有做一些优化 cargo run 如果存在目标文件,且Rust认为代码没有改变,则直接执行可执行文件;如果源码改变或可执行文件不存在,则编译后再执行 cargo check 检查代码是否能正确编译,速度当然比cargo build,因为不产生可执行文件,编码中用于快速检查代码是否有问题 使用过程中遇到的问题：Caused by: error authenticating: failed connecting agent; class=Ssh (23)解决方法：将~/.gitconfig中的两行内容注释,Windows下路径为C:\Users\Administrator\12#[url &quot;git@github.com:&quot;]# insteadOf = https://github.com/ Ref：1.The Rust Programming Language2.https://github.com/rust-lang/cargo/issues/3381]]></content>
      <tags>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录Rust Hello Word]]></title>
    <url>%2F2020%2F03%2F21%2F%E8%AE%B0%E5%BD%95Rust-Hello-Word%2F</url>
    <content type="text"><![CDATA[1.安装Rust官网下载直接点击安装即可 2.在Goland上安装Rust插件在rust plugins上下载对应Goland版本的Rust插件。笔者的Goland是2019.1。 根据如下步骤安装即可,Install Plugin from Disk指定上一步下载的压缩文件。安装好后重启Goland3.将环境变量%USERPROFILE%\.cargo\bin添加到Path4.使用Goland新建一个Rust项目 5.编译执行Rust代码 过程中遇到的问题：缺少一些库:advapi32.lib,userenv.lib解决方法：直接安装一个visual studio community 2019,问题解决 Ref：1.https://github.com/rust-lang/rust/issues/43039]]></content>
      <tags>
        <tag>Rust</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现中使用的命令和工具]]></title>
    <url>%2F2020%2F01%2F20%2F%E4%B8%80%E4%B8%AA%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AE%9E%E7%8E%B0%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%9A%84%E5%91%BD%E4%BB%A4%E5%92%8C%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[对书中所使用到的工具和命令进行记录,方便查看！ ndisasmv bximage dd xxd 12345678910111213141516sudo losetup -fmkdir /mnt/floppylosetup /dev/loop5 pm.imgsudo mount /dev/loop5 /mnt/floppysudo cp pmtest3.com /mnt/floppyumount /mnt/floppysudo mount -t msdos -o loop pm.img /mnt/floppyx/4xw 0x7c14qemu-system-i386 -hda /home/hys/code/6828/src/orange/ch1/a.img -gdb tcp::26000 -S 错误集锦：hys@hys:~/code/6828/src/orange/ch3/b$ sudo mount -t msdos -o loop pm.img /mnt/floppymount: /mnt/floppy: wrong fs type, bad option, bad superblock on /dev/loop22, missing codepage or helper program, or other error. 需启动bochs然后进行初始化: format b:然后再执行mount命令]]></content>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020书单]]></title>
    <url>%2F2020%2F01%2F13%2F2020%E4%B9%A6%E5%8D%95%2F</url>
    <content type="text"><![CDATA[在此先列一个计划 类别 书名 编译原理 Programing Languages:Application and Interpretation(cs3520) SICPpython源码剖析(zpoint/cpython-internals) 计算机基础 深入理解计算机基础Operating System: Principles and PracticeOperating systems three easy piecesConcepts,techniques and models of computer programming 设计模式 Mastering python Design patterns设计模式之禅 编程语言 The Rust Programing LanguageFluent Python 代码能力 Aphilosophy of Software 框架类 grpcakka 容器 k8s, istio, knative 算法 算法(第4版),github上的python算法,https://github.com/wangzheng0822/algohttps://github.com/algorithm-visualizer/algorithm-visualizerhttps://github.com/TheAlgorithms/Python 数据库 高性能MySQLRedis设计与源码分析Redis设计与实现Redis深度历险Database system:Implementation 2nd(cmu-15-721) 系统编程 unix环境高级编程unix网络编程LINUX内核设计与实现 系统设计 Designing Data Intensive ApplicationsReactive Design Patternhttps://github.com/donnemartin/system-design-primer A primer on memory consistency and cache coherenceshared-memory synchronizationprocessor micro architecheture: An Implementation perspectivestatic program Analysis 操作系统 Operating system:Three easy pieces一个操作系统的实现Operating System Principles and Practice 协议 TCP/IP 面试 https://github.com/0voice/interview_internal_reference 多思考！！！]]></content>
  </entry>
  <entry>
    <title><![CDATA[一个操作系统的实现笔记：引导扇区实现]]></title>
    <url>%2F2020%2F01%2F10%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%9A%E5%BC%95%E5%AF%BC%E6%89%87%E5%8C%BA%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[实验环境： · ubuntu18.04 · qemu · nasm 1.安装需软件 qemu安装，请参考 nasm安装 sudo apt-get install nasm 2.编写引导扇区代码a.asm123456789101112131415161718 org 07c00h ; 告诉编译器程序加载到7c00处 mov ax, cs mov ds, ax mov es, ax call DispStr ; 调用显示字符串例程 jmp $ ; 无限循环DispStr: mov ax, BootMessage ;将字符串首地址传递给寄存器ax mov bp, ax ; ES:BP = 串地址,指向BootMessage mov cx, 16 ; CX = 串长度 mov ax, 01301h ; AH = 13, AL = 01h mov bx, 000ch ; 页号为0(BH = 0) 黑底红字(BL = 0Ch,高亮) mov dl, 0 int 10h ; 10h 号中断 retBootMessage: db &quot;Hello, OS world!&quot;times 510-($-$$) db 0 ; 填充剩下的空间，使生成的二进制代码恰好为512字节;表示将这个字节重复510-($-$$)遍。dw 0xaa55 ; 结束标志 代码分析12345678910$表示当前行被汇编后的地址$$表示一个节(section)的开始处被汇编后的地址。在这里,我们的程序只有1个节,所以$$实际上就表示程序被编译后的开始地址。从第10行开始为调用10hk号中断做准备(10h号中断是由BIOS对显示提供的服务),设置寄存器的值AH:往屏幕中写入字符串AL:写模式,bit 0(0位置零):写入字符串后更新光标; bit 1(1位置1):字符串中包括字符和属性;bit 2-7为保留位BH: 页码BL:如果字符串只包括characters,BL为字符串的属性CX:字符串长度DH,DL: 写字符串的起始列与行 3.生成启动镜像12nasm a.asm -o a.bindd if=a.bin of=a.img bs=512 count=1 conv=notrunc 4.使用QEMU引导软盘映像文件a.img1./qemu-system-i386 -hda /home/hys/code/6828/src/orange/ch1/a.img # 根据自己情况适当修改文件路径 启动后效果如下： 至此一个简单的”操作系统”已经完成。 5.调试我们写的引导扇区代码 启动qemu,等待gdb连接 1qemu-system-i386 -hda /home/hys/code/6828/src/orange/ch1/a.img -gdb tcp::26000 -S 启动gdbgdb -n -x .gdbinit,设置断点,观察寄存器的值p/x $cs显示寄存器cs的值 .gdbinit文件内容：设置gdb连接到时QEMU并调试早期启动的16位代码1234567891011121314151617181920212223242526272829set $lastcs = -1define hook-stop # There doesn&apos;t seem to be a good way to detect if we&apos;re in 16- or # 32-bit mode, but we always run with CS == 8 in 32-bit mode. if $cs == 8 || $cs == 27 if $lastcs != 8 &amp;&amp; $lastcs != 27 set architecture i386 end x/i $pc else if $lastcs == -1 || $lastcs == 8 || $lastcs == 27 set architecture i8086 end # Translate the segment:offset into a physical address printf &quot;[%4x:%4x] &quot;, $cs, $eip x/i $cs*16+$eip end set $lastcs = $csendecho + target remote localhost:26000\ntarget remote localhost:26000# If this fails, it&apos;s probably because your GDB doesn&apos;t support ELF.# Look at the tools page at# http://pdos.csail.mit.edu/6.828/2009/tools.html# for instructions on building GDB with ELF support.echo + symbol-file obj/kern/kernel\nsymbol-file obj/kern/kernel gdb相关命令请查看http://visualgdb.com/gdbreference/commands/ Ref：1.https://123xzy.github.io/2019/03/08/MIT-6-828-Lab-Guide/2.Orange’S：一个操作系统的实现3.qemu4.gdb5.Intel 80386 Reference Programmer’s Manual7.lab18.Interrupts9.https://en.wikipedia.org/wiki/INT_10H10.http://visualgdb.com/gdbreference/commands/x]]></content>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-接收者]]></title>
    <url>%2F2019%2F12%2F22%2Fgo-%E6%8E%A5%E6%94%B6%E8%80%85%2F</url>
    <content type="text"><![CDATA[定义一个计算长方形面积的函数1234567891011121314151617181920package mainimport ( &quot;fmt&quot;)type Rectangle struct &#123; width, height float64&#125;func area(r Rectangle) float64 &#123; return r.width * r.height&#125;func main() &#123; r1 := Rectangle&#123;12, 2&#125; r2 := Rectangle&#123;9, 4&#125; fmt.Println(&quot;Area of r1 is: &quot;, area(r1)) fmt.Println(&quot;Area of r2 is: &quot;, area(r2))&#125; 上面这种方式定义是一个可以求长方形面积的函数area。这个方法并不是属于Rectangle这个对象。如果要把area当作Rectangle的method,也就是把area当作Rectangle的一个 属性,可以使用接收者。 1234567891011121314151617181920212223242526272829303132333435// 虽然method名称一样,但是如果接收者不一样,那么method就不一样// method里面可以访问接收者的字段package mainimport ( &quot;fmt&quot; &quot;math&quot;)type Rectangle struct &#123; width, height float64&#125;type Circle struct &#123; radius float64&#125;func (r Rectangle) area() float64 &#123; return r.width*r.height&#125;func (c Circle) area() float64 &#123; return c.radius *c.radius *math.Pi&#125;func main() &#123; r1 := Rectangle&#123;12, 2&#125; r2 := Rectangle&#123;9, 4&#125; c1 :=Circle&#123;10&#125; c2 :=Circle&#123;25&#125; fmt.Println(&quot;Area of r1 is: &quot;, r1.area()) fmt.Println(&quot;Area of r2 is: &quot;, r2.area()) fmt.Println(&quot;Area of c1 is: &quot;, c1.area()) fmt.Println(&quot;Area of c2 is: &quot;, c2.area())&#125; 接收者的值传递OR引用传递 1234567891011121314151617181920212223package mainimport ( &quot;fmt&quot;)type Rectangle struct &#123; width, height float64&#125;func (r Rectangle) area() float64 &#123; r.width = 10 return r.width*r.height&#125;func main() &#123; r1 := Rectangle&#123;12, 2&#125; fmt.Println(&quot;r1.width is: &quot;,r1.width ) r1.area() fmt.Println(&quot;r1.width is: &quot;,r1.width )&#125; 把areamethod改为如下：1234func (r *Rectangle) area() float64 &#123; r.width = 10 return r.width*r.height&#125; 结果如下： 如果接收者前面没有加*号则表示方法使用的是引用传递,指针作为Receiver会对实例对象的内容发生操作,如果没加星号则是以值传递,只是对实例的副本进行操作。 Ref：1.go web编程]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go-import]]></title>
    <url>%2F2019%2F12%2F22%2Fgo-import%2F</url>
    <content type="text"><![CDATA[go使用import 命令来导入包文件 像下面这样1234import ( &quot;fmt&quot;)fmt.Println(&quot;hello go&quot;) 相对路径1import &quot;./model&quot; //当前文件同一目录的model目录,但不建议这种方式 绝对路径1import &quot;shorturl/model&quot; //加载gopath/src/shorturl/model 点操作使用点操作可以省略前缀的包名,fmt.Println可以缩写成Println12345import ( . &quot;fmt&quot;)Println(&quot;hello go&quot;) 别名操作1234import ( f &quot;fmt&quot;)f.Println(&quot;hello, go&quot;) _操作_操作其实是引入该包,不直接使用包里面的函数,而是调用了该包里的init函数123import ( _ &quot;fmt&quot;) Ref：1.go web编程]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[unp之datetime]]></title>
    <url>%2F2019%2F10%2F30%2Funp%E4%B9%8Bdatetime%2F</url>
    <content type="text"><![CDATA[一.配置unp.h1.从www.unpbook.com下载源码2.123456tar -xzvf unpv13e.tar.gzcd unpv13e./configurecp config.h /usr/includecp libunp.a /usr/libcp key/unp.h /usr/include 将/usr/include/unp.h中的#include &quot;../config.h&quot;改为#include &quot;config.h&quot;3.编译如果遇到如下错误,请将unp.h中的struct in_pktinfo定义删除。 datetimecli.c12345678910111213141516171819202122232425262728293031323334353637#include "unp.h"intmain(int argc, char **argv)&#123; int sockfd, n; char recvline[MAXLINE + 1]; struct sockaddr_in servaddr; if (argc != 2) err_quit("usage: a.out &lt;IPaddress&gt;"); // 创建一个字节流套接字,并返回一个描述符 if ( (sockfd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0) err_sys("socket error"); bzero(&amp;servaddr, sizeof(servaddr)); //对网络套接字结构清零 servaddr.sin_family = AF_INET; servaddr.sin_port = htons(13); // 端口号必须使用特定格式,所以需要通过htons函数进行转换 // 把ASCII命令行参数转换为合适的格式 if (inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr) &lt;= 0) err_quit("inet_pton error for %s", argv[1]); if (connect(sockfd, (SA *) &amp;servaddr, sizeof(servaddr)) &lt; 0) err_sys("connect error"); while ( (n = read(sockfd, recvline, MAXLINE)) &gt; 0) &#123; recvline[n] = 0; /* null terminate */ if (fputs(recvline, stdout) == EOF) err_sys("fputs error"); &#125; if (n &lt; 0) err_sys("read error"); exit(0);&#125; datetimesrv.c1234567891011121314151617181920212223242526272829303132333435363738#include "unp.h"#include &lt;time.h&gt;intmain(int argc, char **argv)&#123; int listenfd, connfd; struct sockaddr_in servaddr; char buff[MAXLINE]; time_t ticks; listenfd = Socket(AF_INET, SOCK_STREAM, 0);// Socket是经过封装的socket函数,增加了错误处理 bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; // 指定IP地址为INADDR_ANY,这样要服务器主机有多信网络接口,服务器进程就可以在任意网络接口上接受客户连接 servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(13); /* daytime server */ Bind(listenfd, (SA *) &amp;servaddr, sizeof(servaddr)); //把该套接字转换为一个监听套接字,这样来自客户的外来连接就可在该套接字上由内核接受 Listen(listenfd, LISTENQ); for ( ; ; ) &#123; // 服务器进程在accept调用中被投入睡眠,等待某个客户连接的到来并被内核接受。TCP连接所使用的三路握手来建立连接。握手完毕时accept返回,其返回值是一个称为已连接描述符的新描述符。该描述符用于与新连接的那个客户通信。accept为每个连接到本服务器的客户返回一个新的描述符。 connfd = Accept(listenfd, (SA *) NULL, NULL); ticks = time(NULL); snprintf(buff, sizeof(buff), "%.24s\r\n", ctime(&amp;ticks)); // 往已连接描述符中写入数据 Write(connfd, buff, strlen(buff)); // 服务器通过调用close关闭与客户端的连接 Close(connfd); &#125;&#125; 3.测试12gcc daytimetcpcli.c -o daytimetcpcli -lunpgcc daytimetcpsrv.c -o daytimetcpsrv -lunp 先运行daytimetcpsrv,再运行daytimetcpcli Ref：1.unix网络编程-volume1]]></content>
      <categories>
        <category>unp</category>
      </categories>
      <tags>
        <tag>unp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[认识ucontext函数簇]]></title>
    <url>%2F2019%2F10%2F27%2F%E8%AE%A4%E8%AF%86ucontext%2F</url>
    <content type="text"><![CDATA[ucontext函数簇提供了4个函数用于控制用户态的上下文。利用这几个函数可以实现一个协程。 ucontext_t结构体1234567typedef struct &#123; ucontext_t *uc_link; //当前上下文结束后要恢复到时的上下文, sigset_t uc_sigmask; //上下文要阻塞的信号集合 stack_t uc_stack; //上下文所使用的栈 mcontext_t uc_mcontext; //机器特定的保护上下文的表示，包括协程的机器寄存器 ...&#125; ucontext_t; 1234567891011121314#include &lt;ucontext.h&gt;// 获取当前用户上下文,并初始化指针ucp指向的结构体。int getcontext(ucontext_t *ucp);// 设置当前的上下文为ucp指向的内容int setcontext(const ucontext_t *ucp); // 构造一个上下文,修改通过getcontext初始化的ucp,当切换到ucp指向的上下文时，将会执行func函数。// 执行makecontext前必须为ucp分配栈和指定ucp-&gt;uc_linkvoid makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);// 将当前上下文保存到指针oucp指向的结构,并切换到ucp指向的上下文int swapcontext(ucontext_t *restrict oucp, const ucontext_t *restrict ucp); 1234567891011121314#include &lt;stdio.h&gt;#include &lt;ucontext.h&gt;#include &lt;unistd.h&gt;int main(void)&#123; ucontext_t context; getcontext(&amp;context); printf("Hello world\n"); sleep(1); setcontext(&amp;context); return 0;&#125; 每隔一秒打印一行Hello world。 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;#include &lt;ucontext.h&gt;#include &lt;unistd.h&gt;void foo(void)&#123; printf("foo\n");&#125;int main(void)&#123; ucontext_t context; char stack[1024]; getcontext(&amp;context); // 初始化context context.uc_stack.ss_sp = stack; // 设置栈顶指针 context.uc_stack.ss_size = sizeof(stack); // 设置栈的大小 context.uc_link = NULL; //下一个要恢复的上下文为NULL, 将结束运行 makecontext(&amp;context, foo, 0); //构建上下文，执行context上下文时，将执行foo函数 printf("Hello world\n"); sleep(1); setcontext(&amp;context); //设置上下文为context,在这里会切换到执行foo函数 return 0;&#125; 先打印Hello world,然后打印foo。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;stdio.h&gt;#include &lt;ucontext.h&gt; static ucontext_t ctx[3]; static voidf1(void)&#123; printf("start f1\n"); // 将当前 context 保存到 ctx[1]，切换到 ctx[2] swapcontext(&amp;ctx[1], &amp;ctx[2]); printf("finish f1\n");&#125; static voidf2(void)&#123; printf("start f2\n"); // 将当前 context 保存到 ctx[2]，切换到 ctx[1] swapcontext(&amp;ctx[2], &amp;ctx[1]); printf("finish f2\n");&#125; int main(void)&#123; char stack1[8192]; char stack2[8192]; getcontext(&amp;ctx[1]); ctx[1].uc_stack.ss_sp = stack1; ctx[1].uc_stack.ss_size = sizeof(stack1); ctx[1].uc_link = &amp;ctx[0]; // 将执行 return 0 makecontext(&amp;ctx[1], f1, 0); getcontext(&amp;ctx[2]); ctx[2].uc_stack.ss_sp = stack2; ctx[2].uc_stack.ss_size = sizeof(stack2); ctx[2].uc_link = &amp;ctx[1]; makecontext(&amp;ctx[2], f2, 0); // 将当前 context 保存到 ctx[0]，切换到 ctx[2] swapcontext(&amp;ctx[0], &amp;ctx[2]); // 为什么么ctx[0]不用初始化栈？原因是swapcontext会将当前的上下文设置到ctx中,此时ctx[0]为main函数的上下文 return 0;&#125; swapcontext(&amp;ctx[0], &amp;ctx[2])切换到ctx[2]上下文，此时将执行f2。打印start f2,然后切换上下文到ctx[1];执行f1函数，打印start f1;切换到f2，打印finish f2;f2执行完成后，因为ctx[2].uc_link=&amp;ctx[1],再次进入ctx[1],打印出finish f1。 输出1234start f2start f1finish f2finish f1 Ref：1.http://walkerdu.com/2017/01/09/ucontext-theory/2.Complete Context Control3.https://zhengyinyong.com/post/ucontext-usage-and-coroutine/]]></content>
  </entry>
  <entry>
    <title><![CDATA[x86-architecture gcc calling convention]]></title>
    <url>%2F2019%2F10%2F26%2Fx86-architecture-gcc-calling-convention%2F</url>
    <content type="text"><![CDATA[x86架构常见寄存器 register 8 bit 16 bit 32 bit 64 bit 描述 Accumulator register AH:AL AX EAX RAX Base register BH:BL BX EBX RBX Counter register CH:CL CX ECX RCX Data register DH:DL DX EDX RDX Stack Pointer register SP ESP RSP 栈顶指针,始终指向栈顶元素 Stack Base Pointer register BP EBP RBP 栈基指针,用于维护一个栈帧,在Intel的术语中叫帧指针,表示一个栈帧的开始地址 Source Index register SI ESI RSI Destination Index register DI EDI RDI Instruction Pointer IP EIP RIP 64位新增 R8-R15 段寄存器 Stack Segment(SS)：指向栈的指针 Code Segment(CS):指向代码的指针 Data Segment(DS):指向数据的指针 常见组合： register 描述 CS:IP (CS is Code Segment, IP is Instruction Pointer) points to the address where the processor will fetch the next byte of code. SS:SP (SS is Stack Segment, SP is Stack Pointer) points to the address of the top of the stack, i.e. the most recently pushed byte. DS:SI (DS is Data Segment, SI is Source Index) is often used to point to string data that is about to be copied to ES:DI. ES:DI (ES is Extra Segment, DI is Destination Index) is typically used to point to the destination for a string copy, as mentioned above. 当我们调用一个函数和一个函数被调用，它们之间的参数和返回值是怎样传递的？gcc是怎样使用x86中的寄存器？ 调用规范为了允许单独的程序员共享代码并开发供许多程序使用的库，并简化子例程的使用，程序员通常采用通用的调用约定。调用约定其实就是约定函数间如何调用和返回。例如，给定一组调用约定规则，程序员无需检查子例程的定义来确定应如何将参数传递给该子例程。 此外，给定一组调用约定规则，可以使高级语言编译器遵循这些规则，从而允许手动编码的汇编语言例程和高级语言例程相互调用。 下面我们会描述下C语言的调用约定。C调用约定非常依赖CPU硬件实现的栈结构，基于如下汇编指令push,pop,call,ret。 调用者约定 在调用子例程这前,调用者应该保存一些特定寄存器的内容,这些内容称为caller-saved。被调用者callee允许对这些寄存器进行修改,如果调用者caller在子例程返回之后依然要使用这些值，caller必须把这些值push到栈中。 为了把参数传递给子例程,在调用它们之前需先把参数入栈(现在的CPU设计并不是所有参数都得入栈,x86-64位在参数超过6个的情况下才会将6个之外的参数入栈),参数的入栈顺序为从右到左。 使用call指令调用子例程。这个指令将返回地址压入栈中(在所参数之上)当子例程返回后,caller可从寄存器EAX中取得子例程的返回值,为了恢复调用前的状态，调用者应该做如下处理： 移除栈中的参数。 从栈中恢复caller-saved的寄存器内容。 被调用者(callee)约定 将%rbp入栈,并将%rsp 赋值给%rbp12pushq %rbp # 将调用者的rbp入栈movq %rsp, %rbp # 初始化一个新的栈帧 这个初始化操作维护了base pointer,rbp。rbp用于直接在栈中根据偏移量获取参数和局部变量。 然后,将局部变量入栈,同时修改rsp寄存器的值。 保存callee-saved的寄存器值,将这些值做入栈操作。当子例程执行完毕进行返回时,必须做如下操作： 将返回值放在rax寄存器中 恢复callee-saved寄存器的值 销毁局部变量,一般通过修改栈指针的值来进行 恢复调用者的rbp值,从栈中弹出rbp 最后执行ret指令,这条指令会将return address从栈中移除 我们先来看一段代码，保存为cdecl.c123456789101112131415#include &lt;stdio.h&gt;int callee(int a, int b, int c,int d, int d, int f, int g)&#123; return 0;&#125;int caller(void)&#123; return callee(1, 2, 3, 4, 5, 6, 7) +5;&#125;int main()&#123; return 0;&#125; 将上面这段代码编译为汇编代码，使用命令gcc -S cdecl.c,会生成一个名称为cdecl.s的文件。下面是调用者部分的汇编代码，采用AT&amp;T格式。1234567891011121314151617181920212223242526272829303132333435363738394041callee:.LFB0: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 movl %edi, -4(%rbp) movl %esi, -8(%rbp) movl %edx, -12(%rbp) movl %ecx, -16(%rbp) movl %r8d, -20(%rbp) movl %r9d, -24(%rbp) movl $0, %eax popq %rbp .cfi_def_cfa 7, 8 retcaller:.LFB1: .cfi_startproc pushq %rbp .cfi_def_cfa_offset 16 .cfi_offset 6, -16 movq %rsp, %rbp .cfi_def_cfa_register 6 pushq $7 movl $6, %r9d movl $5, %r8d movl $4, %ecx movl $3, %edx movl $2, %esi movl $1, %edi call callee addq $8, %rsp addl $5, %eax leave .cfi_def_cfa 7, 8 ret .cfi_endproc 由上汇编代码可以看到第一个参数放入到了寄存器edi,第二个参放入到了寄存器esi,第三个参数放入寄存器edx,依此类推。 gcc对x86寄存器的调用规则 Register Purpose Saved across calls Saved across calls %rax temp register; return value No %rbx callee-saved Yes %rcx used to pass 4th argument to functions No %rdx used to pass 3rd argument to functions No %rsp stack pointer Yes %rbp callee-saved; base pointer Yes %rsi used to pass 2nd argument to functions No %rdi used to pass 1st argument to functions No %r8 used to pass 5th argument to functions No %r9 used to pass 6th argument to functions No %r10-r11 temporary No %r12-r15 callee-saved registers Yes 函数调用时栈中的内容 栈地址 描述 16(%ebp) - third function parameter 12(%ebp) - second function parameter 8(%ebp) - first function parameter 4(%ebp) - old %EIP (the function’s “return address”) 0(%ebp) - old %EBP (previous function’s base pointer) -4(%ebp) - first local variable -8(%ebp) - second local variable -12(%ebp) - third local variable 相关指令push ax;将一个寄存器中的数据入栈pop ax; 出栈，用一个寄存器接收出栈的数据.ret指令用栈中数据，修改IP的内容，从而实现近转移。CPU执行ret指令时，进行下面两步操作：(1)(IP) = ((ss)*16+(sp)) (2)(sp) = (sp) + 2 #16位cpucall 标号;将当前的IP压栈后，转到标号处执行指令 Ref：1.X86_Assembly-X86_Architecture2.X86 Assembly/16, 32, and 64 Bits3.http://unixwiz.net/techtips/win32-callconv-asm.html4.http://unixwiz.net/techtips/win32-callconv.html5.X86_calling_conventions6.http://flint.cs.yale.edu/cs421/papers/x86-asm/asm.html#calling7.X86-64 Architecture Guide8.X86_assembly_language9.https://pages.hep.wisc.edu/~pinghc/x86AssmTutorial.htm]]></content>
  </entry>
  <entry>
    <title><![CDATA[协程]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[普通函数与协程函数有栈协程无栈协程 共享栈私有栈 对称协程非对称协程 Ref:1.https://en.wikipedia.org/wiki/Coroutine]]></content>
  </entry>
  <entry>
    <title><![CDATA[python3类型注释]]></title>
    <url>%2F2019%2F08%2F11%2Fpython3%E7%B1%BB%E5%9E%8B%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[python是一门动态语言,若编码不规范会出现很多运行时错误。为了提高代码可读性，减少运行时bug,python提供了类型注释可以对函数参数，返回值，变量等进行标注。通过静态检查就可以发现一些常见的问题。类型注释的代码在python运行时并不会触发，所以没有性能影响。 1.类型注释的好处，看如下这个例子。 这两种方法在python3.5之后都是正确的写法，一点问题都没有。但是看左边的代码就会遇到的问题，因为python是动态语言，函数参数name的类型只有在运行时才知道。如果我们这样调用左边的函数gretting([&quot;world&quot;])，这很明显字符串不能和列表相加，这在运行时就会发生异常。如果我们使用右边的写法通过静态检查器就可以提前检查出错误。右边提示了参数name的类型为str,函数的返回值为str。 通过mypy对代码进行类型检查,可以看到参数传递错误被检查出来了。 2.python提供的类型注释python从3.5版本开始将类型注释并入了标准库中。可通过如下方式引用from typing import List, Dict 对变量进行注释 123456789101112131415161718In [1]: x: int = 1 # 表示变量x为int类型 In [2]: x: float = 0.1 # 表示变量x为float类型 In [3]: x: bool = True # 表示变量x为bool类型 In [4]: x: str = &quot;test&quot; # 表示变量x为str类型 In [5]: x: bytes = b&quot;test&quot; # 表示变量x为bytes类型In [8]: from typing import List,Dict,Sequence,Tuple,SetIn [9]: x: Dict[str, float] = &#123;&apos;field&apos;: 2.0&#125; # x是一个字典key为str类型,值为float类型 In [10]: x: Tuple[int, str, float] = (3, &quot;yes&quot;, 7.5) # x是一个元组，元素类型可以为int, str, float In [11]: x: List[int] = [1] # x是一个列表，元素类型为int In [12]: x: Set[int] = &#123;6, 7&#125; # x是一个集合，元素类型为intIn [16]: x: Union[int,str] = 1 # x类型可以为int或者str In [17]: x: Union[int, None] = 1 # x类型可以为int或者None,等同于Optional[int] In [18]: x: Optional[int] = 1 函数注释 12def gretting(name: str) -&gt; str: return &quot;Hello &quot; + name 12# 对一个可调用的函数进行注释x: Callable[[int, float], float] = f # x是一个可调用的函数，有两个参数，类型分别为int,float,返回值为float类型 类变量1234567891011class Car: seats: ClassVar[int] = 4 # seats是一个类变量，类型为intdef f(x: &apos;A&apos;) -&gt; None: # OK # 表示函数f的参数x的类型为A的实例 ...class A: ...x: A = A() # 表示x的类型为A的实例，若想表示x的类型为类对象，请看下面x: Type[A] = A typing模块中的协议类 协议 方法 Iterable[T] def __iter__(self) -&gt; Iterator[T] Iterator[T] def __next__(self) -&gt; T def __iter__(self) -&gt; Iterator[T] Sized def __len__(self) -&gt; int Container[T] def __contains__(self, x: object) -&gt; bool Collection[T] def __len__(self) -&gt; int def __iter__(self) -&gt; Iterator[T] def __contains__(self, x: object) -&gt; bool Awaitable[T] def __await__(self) -&gt; Generator[Any, None, T] AsyncIterable[T] def __aiter__(self) -&gt; AsyncIterator[T] AsyncIterator[T] def __anext__(self) -&gt; Awaitable[T] def __aiter__(self) -&gt; AsyncIterator[T] ContextManager[T] def __enter__(self) -&gt; T def __exit__(self,exc_type: Optional[Type[BaseException]],exc_value: Optional[BaseException],traceback: Optional[TracebackType]) -&gt; Optional[bool] AsyncContextManager[T] def __aenter__(self) -&gt; Awaitable[T] def __aexit__(self,exc_type:Optional[Type[BaseException]],exc_value: Optional[BaseException],traceback: Optional[TracebackType]) -&gt; Awaitable[Optional[bool]] cast 将静态类型值强制转换为子类型,并不是像其它静态语言真正的进行类型转换，在运行时并不生效。 12345from typing import cast, Listo: object = [1]x = cast(List[int], o) # OKy = cast(List[str], o) # OK (cast performs no actual runtime check) Generics python内置的collection classes都是generic classes。Generic types拥有一个或多个类型参数。如Dict[int, str] List[int]。 自定义一个原生类：1234567891011121314151617from typing import TypeVar, GenericT = TypeVar(&apos;T&apos;)class Stack(Generic[T]): def __init__(self) -&gt; None: # Create an empty list with items of type T self.items: List[T] = [] def push(self, item: T) -&gt; None: self.items.append(item) def pop(self) -&gt; T: return self.items.pop() def empty(self) -&gt; bool: return not self.items 现在Stack类可以用于表示拥有任务类型的stack,如Stack[int],Stack[str],Stack[Tuple[int, str]]等。可以像使用内置容器类型一样使用类Stack12345# Construct an empty Stack[int] instancestack = Stack[int]()stack.push(2)stack.pop()stack.push(&apos;x&apos;) # Type error Variance of generic types关于它们之间的子类型关系，存在三种主要类型的通用类型：invariant(不变的),covariant(协变的),contravariant(逆变的) 12345class Animal: ...class Cat(Animal): ... 如上,Animal有子类型Cat,Cat可能可以使用Animal中的某些方法，这个可能取决于类型是covariant或者contravariant。 如果Cat可以使用它的超类型Animal中的一些方法，这种情况被称为convariant;如果超类型Animal可以使用子类型Cat中的某些方法，则被称为contravariant。如果convariant和contravariant都不成立，则被称作invariant,即Cat不能使用Animal中的方法，Animal 也不能使用Cat中的方法。 12345678910111213141516171819202122232425262728from typing import TypeVar, Generic, Iterable, IteratorList_co = TypeVar(&apos;List_co&apos;, contravariant=True)class ImmutableList(Generic[List_co]): def __init__(self, items: Iterable[List_co]) -&gt; None: ... def __iter__(self) -&gt; Iterator[List_co]: ...class Employee: ...class Manager(Employee): ...def list_employees(employees: ImmutableList[Employee]) -&gt; None: for employee in employees: print(employee)managers = ImmutableList([Manager()])list_employees(managers) 当contravariant=True时，使用命令mypy co.py,出现如下错误： 默认情况下mypy认为所有用户自定义的原生类型都是invariant的。 请试下当convariant=True时又是怎样的结果 Ref：1.pep4842.pep5263.mypy4.https://blog.magrathealabs.com/pythons-covariance-and-contravariance-b422c63f57ac]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp之拥塞控制]]></title>
    <url>%2F2019%2F08%2F10%2Ftcp%E4%B9%8B%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[要了解拥塞控制可以先了解下在途字节数这一概念。 关于TCP拥塞控制的一些术语：12345678910SMSS--SENDER MAXIMUM SEGMENT SIZE 发送端可发送的最大分节大小RMSS--RECEIVER MAXIMUM SEGMENT SIZE 接收端可接收的最大分节大小FULL-SIZED SEGMENT 一个包含了SMSS字节数据的分节RECEIVER WINDOW--rwnd 推荐的接收窗口CONGESTION WINDOW--cwnd: 限制TCP可以发送多少数据的状态变量。在任何时刻TCP不能发送序号大于最大确认序号加上rwnd,cwnd中的的较小值的数据。INITIAL WINDOW--IW: 初始窗口。三次握手之后的拥塞窗口大小LOSS WINDOW--LW: 丢失窗口。丢失窗口是在一个TCP根据它的重传定时器检测到了数据丢失之后，拥塞窗口的尺寸RESTART WINDOW-RW: 重启窗口。重启窗口是TCP在一段闲置期之后重新开始传送后拥塞窗口的尺寸。FLIGHT SIZE: 在途字节数。已经发送但未确认的数大小 1.拥塞控制的目的为什么要进行拥塞控制? 假如把网络路径想象成一条河流,发送方是水源,接收方是入海口,那在途字节数就是河里的水量。当水源的流速超过了入海口的流速,河里的水就会越来越多,直到溢出。发生拥塞时的在途字节数就是该时刻的网络拥塞点。发生拥塞时TCP的表现为丢包。当发送方发送数据过慢时,网络等资源就会造成一定的浪费，当发送过快时，就是造成网络拥塞(出现丢包),拥塞控制就是为了找到最合适的发送速度。 2.拥塞控制算法拥塞控制算法包括4个部分：慢启动,拥塞避免,快速重传,快速恢复。 慢启动与拥塞避免慢启动与拥塞避免算法用于控制发送方，避免一下发送过多的数据到网络中。为了实现这些算法，对于每个TCP连接加入这两个变量，cwnd rwnd。另一个状态变量，ssthresh用于标识此时应该使用慢启动(cwnd &lt; ssthresh)还是拥塞避免(cwnd &gt; ssthresh)。 当开始传输数据时，TCP并不知道网络可用的容量，为了避免拥塞，这个阶段TCP会逐渐数据的发送量。慢启动算法为了避免注入过多的数据到网络中会应用于开始传输数据和重传定时器修复丢包之后。 数据传输开始时，IW的大小设置: 123456If SMSS &gt; 2190 bytes:IW = 2 * SMSS bytes and MUST NOT be more than 2 segmentsIf (SMSS &gt; 1095 bytes) and (SMSS &lt;= 2190 bytes):IW = 3 * SMSS bytes and MUST NOT be more than 3 segmentsif SMSS &lt;= 1095 bytes:IW = 4 * SMSS bytes and MUST NOT be more than 4 segments 在慢启动阶段，TCP通过如下规则在每接收一个ACK时增加cwnd,每次都会增加前一个`cwnd```的一倍大小。 当出现丢包，或者当接收者提示rwnd是受限因素时，或者cwnd &gt; ssthresh时，会进入拥塞避免阶段。 在拥塞避免阶段，在每一个RTT时间，cwnd增加FULL-SIZED SEGMENT大小。 当TCP发送者通过重传定时器意识到丢包时，会重新设置ssthresh的大小。ssthresh = max (FlightSize / 2, 2*SMSS) 快速重传与快速恢复快速重传是为减少丢包重传等待时间的一种机制。如果发送方接收到3个或3个以上的重复ACK,发送方就会重传丢失的数据报,而无需等待超时定时器溢出。(1)当收到第3个重复的ACK时，将ssthresh设置为当前拥塞窗口cwnd的一半(2)每次收到另一个重复的ACK时，cwnd增加1个报文段大小并发送1个分组（如果新的cwnd允许发送）(3)当下一个确认新数据的ACK到达时，设置cwnd为ssthresh（在第1步中设置的值）。这个ACK应该是在进行重传后的一个往返时间内对步骤1中重传的确认。另外，这个ACK也应该是对丢失的分组和收到的第1个重复的ACK之间的所有中间报文段的确认。这一步采用的是拥塞避免，因为当分组丢失时我们将当前的速率减半 [图片来源网络] Ref：1.TCP congestion control2.RFC25813.《TCP/IP协议详解1》]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP之在途字节数]]></title>
    <url>%2F2019%2F08%2F05%2FTCP%E4%B9%8B%E5%9C%A8%E9%80%94%E5%AD%97%E8%8A%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[在途字节数(bytes in flight):已经发送出去，但尚未被确认的字节数。数据发送方抓到的包才能用来分析在途字节数。 下图是在客户端抓的包： 假如我们想知道第0.400000秒时的在途字节数，该如何计算? 在该时间点之前客户端发送的是10号包,即”Seq=265248,Len=180”字节,表示序号在265248+180之前 的字节已经发送出去了。而第0.400000之前的服务器的Ack为3284,表示序号在3284之前的字节已经收到,那么在途字节数就是265248+180-3284=262144字节。 公式可以表示为: 在途字节数=Seq + Len - Ack(Seq和Len是来自上一个数据发送方的包,而Ack则是来自上一个数据接收方的包) 再看一个例子：如果我们要求0.460000秒的在途字节数,套用公式,0.460000秒的上一个数据发送方的包是9号包,上一个数据接收方的包为6包。则在途字节数为1+6-1=6 我们也可以使用Wireshark提供的功能来查看在途字节数,这样就不用我们手动来计算了。1.使用Wireshark打开所抓的包。2.点击菜单栏【编辑】—&gt; 【首选项】—&gt;【Appearance】—&gt; 【Columns】3.点击+号,添加自定义列。 4.保存,移动新列【byte in flight】到合适位置。这列显示的数据就是对应时刻的在途字数,单位为字节。 如图可知结果和我们使用上面公式计算的结果是一致的。 Ref：1.《Wireshark网络分析的艺术》2.https://www.youtube.com/watch?v=sIxv3YO2eYw]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s之hello-world]]></title>
    <url>%2F2019%2F08%2F01%2Fk8s%E4%B9%8Bhello-world%2F</url>
    <content type="text"><![CDATA[环境：CentOs 7.2 一、使用k8s搭建hello-world,安装kubernetes1.关闭CentOS自带的防火墙12systemctl disable firewalldsystemctl stop firewalld 2.安装etcd和kubernetes1yum install -y etcd kubernetes 3.修改配置文件/etc/sysconfig/docker和/etc/kubernetes/apiserver为如下。123/etc/sysconfig/docker/OPTIONS=&apos;--selinux-enabled=false --log-driver=journald --signature-verification=false&apos; 123# 将--admission-control参数中的ServiceAccount删除/etc/kubernetes/apiserverKUBE_ADMISSION_CONTROL=&quot;--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota&quot; 4.按如下顺序启动所有的服务1234567systemctl start etcdsystemctl start dockersystemctl start kube-apiserversystemctl start kube-controller-managersystemctl start kube-schedulersystemctl start kubeletsystemctl start kube-proxy 二、搭建hello-world案例1.拉取镜像123docker pull kubeguide/guestbook-php-frontend # php web服务docker pull kubeguide/redis-masterdocker pull kubeguide/guestbook-redis-slave 创建redis-master Pod和服务 为redis-master服务创建名为redis-master的RC定义文件：redis-master-controller.yaml 1234567891011121314151617181920apiVersion: v1kind: ReplicationControllermetadata: name: redis-master labels: name: redis-masterspec: replicas: 1 selector: name: redis-master template: metadata: labels: name: redis-master spec: containers: - name: master image: kubeguide/redis-master ports: - containerPort: 6379 发布到集群中kubectl create -f redis-master-controller.yaml 创建与之关联的Service,编辑文件redis-master-service.yaml 123456789101112apiVersion: v1kind: Servicemetadata: name: redis-master labels: name: redis-masterspec: ports: - port: 6379 targetPort: 6379 selector: name: redis-master 发布Servicekubectl create -f redis-master-service.yaml 3.创建redis-slave Pod和服务 创建Podvim redis-slave-controller.yaml 1234567891011121314151617181920212223apiVersion: v1kind: ReplicationControllermetadata: name: redis-slave labels: name: redis-slavespec: replicas: 2 selector: name: redis-slave template: metadata: labels: name: redis-slave spec: containers: - name: slave image: kubeguide/guestbook-redis-slave env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 6379 发布Pod到集群kubectl create -f redis-slave-controller.yaml 创建Servicevim redis-slave-service.yaml 1234567891011apiVersion: v1kind: Servicemetadata: name: redis-slave labels: name: redis-slavespec: ports: - port: 6379 selector: name: redis-slave 发布服务kubectl create -f redis-slave-service.yaml 4.创建frontend Pod和服务 创建Podvim frontend-controller.yaml 1234567891011121314151617181920212223apiVersion: v1kind: ReplicationControllermetadata: name: frontend labels: name: frontendspec: replicas: 3 selector: name: frontend template: metadata: labels: name: frontend spec: containers: - name: frontend image: kubeguide/guestbook-php-frontend env: - name: GET_HOSTS_FROM value: env ports: - containerPort: 80 发布Pod到集群kubectl create -f frontend-controller.yaml 创建服务vim frontend-service.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: frontend labels: name: frontendspec: type: NodePort ports: - port: 80 nodePort: 30001 selector: name: frontend 发布Servicekubectl create -f frontend-service.yaml 5.查看Pod和服务状态 集群中其它Pod如何访问redis-master中的服务？redis-master服务被分配了一个值为10.254.187.40的虚拟IP,kubernetes集群中的其它Pod就可以通过这个IP访问redis-master服务。由于IP地址是在服务创建后由kebernetes系统自动分配的,其它Pod中无法预先知道某个Service的虚拟IP,为此kuberntes通过使用环境变量来实现服务发现,在每个Pod的容器里都增加了一组Service相关的环境变量,用来记录从服务名到虚拟IP地址的映射关系。 5.通过浏览器访问网页,输入URL: http://虚拟机IP:30001 在创建过程中可能会用到的命令12345678kubectl get pods #查看pod状态kubectl get rc #查看RCkubectl get service #查看服务kubectl logs [pod-name] # 查看日志kubectl delete pods [pod-name] #删除podkubectl delete rc [rc-name] #删除rckubectl delete service [service-name] #删除服务 所遇问题：1.docker启动失败,提示Error starting daemon: SELinux is not supported with the overlay2 graph driver on this kernel. Either boot into a newer kernel or disable selinux in docker (—selinux-enabled=false) vim /etc/sysconfig/docker1OPTIONS=&apos;--selinux-enabled --log-driver=journald --signature-verification=false&apos; 改为 1OPTIONS=&apos;--selinux-enabled=false --log-driver=journald --signature-verification=false&apos; 2.创建pod一直处于ContainerCreating状态,原因是不能拉取pod的基础镜像,[https://blog.csdn.net/weixin_34054866/article/details/87525597]12345yum install -y *rhsm* wget http://mirror.centos.org/centos/7/os/x86_64/Packages/python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm rpm2cpio python-rhsm-certificates-1.19.10-1.el7_4.x86_64.rpm | cpio -iv --to-stdout ./etc/rhsm/ca/redhat-uep.pem | tee /etc/rhsm/ca/redhat-uep.pem 3.启动frontend pod时报错, AH00534: apache2: Configuration error: No MPM loaded12345systemctl stop docker //停掉docker服务rm -rf /var/lib/docker //注意会清掉docker images的镜像,需重新拉取镜像vi /etc/sysconfig/docker-storage //将文件里的overlay2改成devicemapper即可DOCKER_STORAGE_OPTIONS=&quot;--storage-driver overlay2 &quot; #修改前DOCKER_STORAGE_OPTIONS=&quot;--storage-driver devicemapper &quot; #修改后 重启docker服务systemctl start docker Ref：1.kubernetes权威指南2.https://blog.csdn.net/a1010256340/article/details/801061563.https://blog.csdn.net/weixin_34054866/article/details/875255974.https://www.cnblogs.com/guyeshanrenshiwoshifu/p/9147238.html]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[流式处理图]]></title>
    <url>%2F2019%2F07%2F16%2F%E6%B5%81%E5%BC%8F%E5%A4%84%E7%90%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令]]></title>
    <url>%2F2019%2F07%2F14%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[hexo新建命令hexo new [layout] &quot;title&quot; layout path 描述 post source/_post 新建文章 draft source/_drafts/ 新建草稿 page source 新建一个页面文件 hexo发布草稿中的文章hexo publish &lt;title&gt; hexo clean #清除缓存文件hexo g #生成静态文件hexo d #部署网站 Ref:1.官方文档]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker容器使用keepalived模拟mysql主备切换]]></title>
    <url>%2F2019%2F07%2F14%2Fdocker%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8keepalived%E6%A8%A1%E6%8B%9Fmysql%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[1.使用制作的centos/keepalived镜像,运行两次，生成两个容器docker run -idt --privileged centos/keepalived init容器ip分别为 172.17.0.3, 172.17.0.4(通过命令docker inspect [container_id] | grep IP查看) 2.分别进入容器IP: 172.17.0.3vim /etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829global_defs &#123; router_id mysql_master&#125;vrrp_script chk_mysql &#123; script &quot;netstat -lntp | grep 3306&quot; interval 1&#125;vrrp_instance mysql &#123; state BACKUP interface eth0 #在容器中执行ifconfig查看 virtual_router_id 210 priority 150 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.0.5/16 &#125; track_script &#123; chk_mysql &#125;&#125; IP: 172.17.0.4vim /etc/keepalived/keepalived.conf1234567891011121314151617181920212223242526272829global_defs &#123; router_id mysql_slave&#125;vrrp_script chk_mysql &#123; script &quot;netstat -lntp | grep 3306&quot; interval 1&#125;vrrp_instance mysql &#123; state BACKUP interface eth0 virtual_router_id 210 priority 150 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 172.17.0.5/16 &#125; track_script &#123; chk_mysql &#125;&#125; 3.启动keepalivedsystemctl start keepalivedsystemctl enable keepalived 验证： 停止172.17.0.4中的MySQL服务(systemctl stop mysqld),发现VIP飘到了172.17.0.3上 所遇问题：1.systemctl status keepalived查看keepalived状态,发现提示Can’t initialize ipvs: Protocol not available解决方法：加载ip_vs模块(在宿主机执行如下命令)12modprobe ip_vsmodprobe ip_vs_wrr 查看内核是否加载ip_vslsmod | grep ip_vs]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker centos镜像安装mysql]]></title>
    <url>%2F2019%2F07%2F13%2Fdocker-centos%E9%95%9C%E5%83%8F%E5%AE%89%E8%A3%85mysql%2F</url>
    <content type="text"><![CDATA[1.拉取CentOs基础镜像docker pull centos2.启动容器，要加--privileged,不然使用systemctl会报错docker run -idt --privileged centos init3.进入容器docker exec -it 83c2d9bc6aef /bin/bash 4.安装MySQL 1234567yum -y install wgetwget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpmyum -y install mysql57-community-release-el7-8.noarch.rpmyum -y install mysql-community-server.x86_64 启动MySQL systemctl start mysqld.service systemctl enable mysqld.service cat /var/log/mysqld.log | grep password 5.安装keepalivedyum -y install keepalived 6.登录MySQL,修改密码，授权远程访问。 7.导出容器快照到本地文件docker export 83c2d9bc6aef -o keepalived.tar8.将容器快照导入为镜像,下次就可以直接通过此镜像启动有MySQL和keepalived的容器cat keepalived.tar | docker import - centos/keepalived]]></content>
  </entry>
  <entry>
    <title><![CDATA[docker容器apt-get失败]]></title>
    <url>%2F2019%2F07%2F13%2Fdocker%E5%AE%B9%E5%99%A8apt-get%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[1.进入容器后,执行apt-get update失败 原因是容器中的DNS不对。 2.查找宿主机的DNS 3.修改容器中的DNS echo &quot;nameserver 192.168.2.1&quot; | tee /etc/resolv.conf 4.执行apt-get update]]></content>
  </entry>
  <entry>
    <title><![CDATA[通过keepalived实现neo4j主备切换]]></title>
    <url>%2F2019%2F07%2F11%2F%E9%80%9A%E8%BF%87keepalived%E5%AE%9E%E7%8E%B0neo4j%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[1.背景：我们使用的是社区版Neo4j不支持集群功能,因此通过对Neo4j进行双写，再通过keepalived实现主备切换 2.环境：Neo4j服务器A(10.23.4.123),Neo4j服务器B(10.23.4.124),VIP(10.23.4.126),服务器均为CentOs 3.安装keepalived1yum install keepalived 4.修改主机10.23.4.123 keepalived的配置文件/etc/keepalived/keepalived.conf123456789101112131415161718192021222324252627282930global_defs &#123; router_id neo4j_master&#125;vrrp_script chk_neo4j &#123; script &quot;netstat -lntp | grep 7687&quot; interval 1&#125;vrrp_instance neo4j &#123; state BACKUP interface ens160 lvs_sync_daemon_interface ens160 virtual_router_id 210 priority 150 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.23.4.126 &#125; track_script &#123; chk_neo4j &#125;&#125; 5.修改主机10.23.4.124 keepalived的配置文件/etc/keepalived/keepalived.conf123456789101112131415161718192021222324252627282930global_defs &#123; router_id neo4j_slave&#125;vrrp_script chk_neo4j &#123; script &quot;netstat -lntp | grep 7687&quot; interval 1&#125;vrrp_instance neo4j &#123; state BACKUP interface ens160 lvs_sync_daemon_interface ens160 virtual_router_id 210 priority 150 nopreempt advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 10.23.4.126 &#125; track_script &#123; chk_neo4j &#125;&#125; 6.启动keepalived12systemctl enable keepalived.servicesystemctl start keepalived.service 7.验证 停止Neo4j-A,在Neo4j-B通过命令ip a查看VIP是否飘到Neo4j-B上 停止Neo4j-B,在Neo4j-A通过命令ip a查看VIP是否飘到Neo4j-A上 未完]]></content>
  </entry>
  <entry>
    <title><![CDATA[记wordpress一次坑爹的问题]]></title>
    <url>%2F2019%2F06%2F13%2F%E8%AE%B0wordpress%E4%B8%80%E6%AC%A1%E5%9D%91%E7%88%B9%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天晚上发再更新文章时总是提醒/wp-admin/post.php,但 查看服务器发现文件存在，最近也没有安装插件。 多方尝试最后将下图红框中的数据删除，点击保存，问题解决。]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tcp/ip中常见的术语]]></title>
    <url>%2F2019%2F06%2F12%2Ftcp-ip%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84%E6%9C%AF%E8%AF%AD%2F</url>
    <content type="text"><![CDATA[MSS--maximum segment size 最大分节大小 MTU--maximum transmission unit 最大传输单元 # MSS通常被设置为MTU送去IP和TCP首部的固定长度，IPv4和TCP的首部都是20字节 PDU--protocol data unit 协议数据单元 # 网络各层对等实体间交换的单位信息 SDU--Service data uni 服务数据单元 # N层的PDU就是N-1层的SDU MSL--maximum segment lifetime 最长分节生命期 listening socket 监听套接字 connected socket 已连接套接字 local area networks(LANs) 局域网 wide area networks(WANs) 广域网 SMSS--SENDER MAXIMUM SEGMENT SIZE 发送端可发送的最大分节大小 RMSS--RECEIVER MAXIMUM SEGMENT SIZE 接收端可接收的最大分节大小 FULL-SIZED SEGMENT 一个包含了SMSS字节数据的分节 RECEIVER WINDOW--rwnd 推荐的接收窗口 CONGESTION WINDOW--cwnd: 限制TCP可以发送多少数据的状态变量。在任何时刻TCP不能发送序号大于最大确认序号加上rwnd,cwnd中的的较小值的数据。 INITIAL WINDOW--IW: 初始窗口。三次握手之后的拥塞窗口大小 LOSS WINDOW--LW: 丢失窗口。丢失窗口是在一个TCP根据它的重传定时器检测到了数据丢失之后，拥塞窗口的尺寸 RESTART WINDOW-RW: 重启窗口。重启窗口是TCP在一段闲置期之后重新开始传送后拥塞窗口的尺寸。 FLIGHT SIZE: 在途字节数。已经发送但未确认的数大小。 可Ref [在途字节数](http://hysyeah.top/2019/08/05/TCP%E4%B9%8B%E5%9C%A8%E9%80%94%E5%AD%97%E8%8A%82%E6%95%B0/) RTT--round-trip time 表示从发送端发送数据开始，到发送端收到来自接收端的确认（接收端收到数据后便立即发送确认），总共经历的时延 RTO-Retransmission Timeout 重传超时 未完]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp端口与并发服务器]]></title>
    <url>%2F2019%2F06%2F12%2Ftcp%E7%AB%AF%E5%8F%A3%E4%B8%8E%E5%B9%B6%E5%8F%91%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一、端口的作用是什么？ 在一台服务器中可能有多个进程同时使用TCP,UDP这两种协议中的一种，端口就是用来区分这些进程的。 二、套接字对 一个TCP连接的套接字对是一个定义该连接的两个端点的四元组:本地IP地址、本地TCP端口号、外地IP地址、外地TCP端口号。套接字对唯一标识一个网络上的每个TCP连接。 三、并发服务器 并发服务器中主服务器循环通过派生一个子进程来处理每个新的链接。 我们通过如下代码来理解socket如何处理多个请求。 import socket import threading bind_ip =&quot;0.0.0.0&quot; bind_port = 8080 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((bind_ip, bind_port)) server.listen(3) def handle_client(client_socket): request = client_socket.recv(1024) print(&quot;[*] Received: %s&quot; % request) client_socket.close() while True: client, addr = server.accept() print(f&quot;client is {client}, addr is {addr}&quot;) client_handler = threading.Thread(target=handle_client, args=(client,)) client_handler.start() 1.python3 server.py启动服务器，此时服务器的状态如下图，等待客户端的连接。 我们使用{0.0.0.0:8080, :}指出服务器的套接字对。服务器在本地端口8080上等待连接请求。外地IP和外地端口号都没有指定，我们称它为监听套接字(listening socket) 2.在同一台主机上启动一个客户端连接服务器. 客户端会随机生成一个端口与服务端进行连接，在进行连接时的状态如下图: 当三次握手后，客户端与服务端建立连接后 3.我再启动一个客户端 连接状态如下： 总结: TCP必须通过查看套接字对的所有4个元素才能确定由哪个端点接收某个到达的分节。如一个分节来自127.0.0.1端口52822,目的地为127.0.0.1端口8080，它就会被传递给子进程2进行处理。 Ref： 1.《UNIX网络编程》]]></content>
      <categories>
        <category>unp</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[osi模型与tcp-ip]]></title>
    <url>%2F2019%2F06%2F11%2Fosi%E6%A8%A1%E5%9E%8B%E4%B8%8Etcp-ip%2F</url>
    <content type="text"><![CDATA[OSI(open systems interconnection)开放系统互连。 OSI是一个七层的概念模型，提供给开发者一个必须的、通用的概念以便开发完善、可以用来解释连接不同系统的框架。 OSI模型与TCP/IP对应的关系： TCP/IP通常被认作一个四层协议系统。1.链路层/网络接口层：处理底层物理接口细节2.网络层：处理分组在网络中的活动3.传输层：主要为两台主机上的应用程序提供端到端的通信。TCP和UDP在这一层。 4.应用层：负责处理特定的应用程序细节 从上层到下层涉及到数据的封装 Ref：1.《TCP/IP协议详解1》]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过tcpdump分析tcp中的三次握手和四次挥手]]></title>
    <url>%2F2019%2F06%2F07%2F%E9%80%9A%E8%BF%87tcpdump%E5%88%86%E6%9E%90tcp%E4%B8%AD%E7%9A%84%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[最近在看《unix网络编程》刚好总结下tcp中的三次握手和四次挥手。 一、tcp中的三次握手和四次挥手的过程 TCP三次握手： 1.连接请求由客户端发起，发送一个SYN分节，Seqence number为0(也就是图中的J) 2.服务端接收到客户的SYN之后，确认(ACK)客户的SYN,同时也发送一个SYN分节。此时Acknowledgment number=1(J+1) 3.客户端确认服务端的SYN。 下图为tcp分节的模式（图来源于网络) TCP四次挥手：上图为客户端执行主动关闭的情况，客户，服务端都可以执行主动关闭 1.客户端调用close，该端称为主动关闭,发送FIN分节. 2.接收到这个FIN分节的一端称为被动关闭。这个FIN由TCP确认。它的接收也作为一个文件结束符传递给接收端应用进程（放在已排队等候该应用进程接收的任何其它数据之后），因为FIN的接收意味着接收端应用进程在相应连接上现无额外数据可接收。 3.一段时间后，接收到这个文件结束符的应用进程将调用close关闭它的socket。同时也会发一个FIN分节。 4.主动关闭那一端，发送ACK确认FIN。 步骤2和步骤3都由被动关闭一端发出，有可能会被合并成一个分节。二、通过tcpdump抓包进行分析 1.使用socket编写tcp客户端和服务程序。 # server.py import socket bind_ip =&quot;0.0.0.0&quot; bind_port = 8080 server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((bind_ip, bind_port)) #将指定的ip和port绑定到该套接字 server.listen(3) #调用listen函数把该套接字转换成一个监听套接字，参数3指定系统内核允许在这个监听描述符上排队的最大客户连接数 while True: client, addr = server.accept() #等待客户端连接，完成三路握手后返回一个已连接描述符，accept函数为每个连接到本服务器的客户端返回一个新的新描述符 rcv = client.recv(1024) print(rcv) client.close() # client.py import socket host = &quot;127.0.0.1&quot; port = 8080 client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect((host, port)) client.close() 2.在linux终端中执行命令tcpdump -i lo port 8080 -w tcphandshake.pcap 3.在两个窗口分别执行python server.py,python client.py 4.将抓包的在wireshake中打开。 可以看到和上面描述的过程是一致的，四次挥手步骤2和步骤3合并成了一个分节。5.查看分节中的详细信息。 - tcp三次握手中客户端发送的SYN分节。 客户端会告诉对端它的最大分节大小(maximum segment size—MSS),就是在这个连接中每个TCP分节可以接受的最大数据量。 告诉对端它的窗口大小是多少。 - tcp四次挥手，步骤2和步骤3发送的分节（合并成了一个分节），图如下。 三,https协议握手过程https握手在tcp的三次握手的基础上加上了TLS/SSL安全基础层，用于在HTTP协议上安全地传输数据。 1.客户端首次请求服务端，告诉服务端自己支持的协议版本,支持的加密算法及压缩算法，并生成一个客户端随机数(Client Random)并告知服务端 2.服务端确认双方使用的加密算法，并返回组客户端证书及一个服务端生成的服务端随机数(Server Random) 3.客户端收到证书后，首先验证证书的有效性,然后生成一个新的随机数(Premaster Secret),使用数字证书中的公钥来加密这个随机数，并发送给服务端。 4.服务端接收到已加密的随机数后，使用私钥进行解密,获取这个随机数(Premaster Secret) 最后服务端和客户端根据约定的加密算法，使用前面的3个随机数(Client Random,Server Random, Premaster Secret),生成对话密钥，用来加密接下来的事个会话过程。 总结： 看十遍，不如实际动手操作一遍！！！ Ref： 1.《unix网络编程-套接字》2.https://www.freesoft.org/CIE/Course/Section4/8.htm3.https://cs.fit.edu/~mmahoney/cse4232/tcpip.html]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql设置远程连接]]></title>
    <url>%2F2019%2F06%2F03%2Fpostgresql%E8%AE%BE%E7%BD%AE%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[1.vim /var/lib/pgsql/10/data/pg_hba.conf(文件所在的目录可能会不一样) 添加一行 host all all 0.0.0.0/0 md5 2.vim /var/lib/pgsql/data/postgresql.conflisten_addresses修改为如下： listen_addresses = &#39;*&#39; 3.重启服务pg_ctl restart]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中有关网络相关参数的设置]]></title>
    <url>%2F2019%2F06%2F01%2Flinux%E4%B8%AD%E6%9C%89%E5%85%B3%E7%BD%91%E7%BB%9C%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在/etc/sysctl.conf文件下存放着一些系统级别的参数，可以通过修改这个文件修改系统参数，当然也包括网络，比如TCP协议的相关设置。 修改完文件后，执行sysctl -p可使修改内容生效。 执行sysctl -a查看当前系统中生效的所有参数。 Linux常用网络内核参数： 参数 描述 net.core.rmem_default 默认的 TCP 数据接收窗口大小（字节） net.core.rmem_max 最大的 TCP 数据接收窗口（字节） net.core.wmem_default 默认的 TCP 数据发送窗口大小（字节） net.core.wmem_max 最大的 TCP 数据发送窗口（字节） net.core.netdev_max_backlog 在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目 net.core.somaxconn 定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数 net.core.optmem_max 表示每个套接字所允许的最大缓冲区的大小 net.ipv4.tcp_mem 确定 TCP 栈应该如何反映内存使用，每个值的单位都是内存页（通常是 4KB） 第一个值是内存使用的下限；第二个值是内存压力模式开始对缓冲区使用应用压力的上限；第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。对于较大的 BDP 可以增大这些值（注意：其单位是内存页而不是字节） net.ipv4.tcp_rmem 为自动调优定义 socket 使用的内存。第一个值是为 socket 接收缓冲区分配的最少字节数；第二个值是默认值（该值会被 rmem_default 覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是接收缓冲区空间的最大字节数（该值会被 rmem_max 覆盖） net.ipv4.tcp_wmem 为自动调优定义 socket 使用的内存。第一个值是为 socket 发送缓冲区分配的最少字节数；第二个值是默认值（该值会被 wmem_default 覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；第三个值是发送缓冲区空间的最大字节数（该值会被 wmem_max 覆盖） net.ipv4.tcp_keepalive_time TCP 发送 keepalive 探测消息的间隔时间（秒），用于确认 TCP 连接是否有效 net.ipv4.tcp_keepalive_intvl 探测消息未获得响应时，重发该消息的间隔时间（秒） net.ipv4.tcp_keepalive_probes 在认定 TCP 连接失效之前，最多发送多少个 keepalive 探测消息 net.ipv4.tcp_sack 启用有选择的应答（1 表示启用），通过有选择地应答乱序接收到的报文来提高性能，让发送者只发送丢失的报文段，（对于广域网通信来说）这个选项应该启用，但是会增加对 CPU 的占用 net.ipv4.tcp_fack 启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生，这个选项也应该启用 net.ipv4.tcp_timestamps TCP 时间戳（会在 TCP 包头增加 12 B），以一种比重发超时更精确的方法（Ref RFC 1323）来启用对 RTT 的计算，为实现更好的性能应该启用这个选项 net.ipv4.tcp_window_scaling 启用 RFC 1323 定义的 window scaling，要支持超过 64KB 的 TCP 窗口，必须启用该值（1 表示启用），TCP 窗口最大至 1GB，TCP 连接双方都启用时才生效 net.ipv4.tcp_syncookies 表示是否打开 TCP 同步标签（syncookie），内核必须打开了 CONFIG_SYN_COOKIES 项进行编译，同步标签可以防止一个套接字在有过多试图连接到达时引起过载。默认值 0 表示关闭 net.ipv4.tcp_tw_reuse 表示是否允许将处于 TIME-WAIT 状态的 socket （TIME-WAIT 的端口）用于新的 TCP 连接 net.ipv4.tcp_tw_recycle 能够更快地回收 TIME-WAIT 套接字 net.ipv4.tcp_fin_timeout 对于本端断开的 socket 连接，TCP 保持在 FIN-WAIT-2 状态的时间（秒）。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。 net.ipv4.ip_local_port_range 表示 TCP/UDP 协议允许使用的本地端口号 net.ipv4.tcp_max_syn_backlog 对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字。默认为 1024 net.ipv4.tcp_low_latency 允许 TCP/IP 栈适应在高吞吐量情况下低延时的情况，这个选项应该禁用 net.ipv4.tcp_westwood 启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化，对于 WAN 通信来说应该启用这个选项 net.ipv4.tcp_bic 为快速长距离网络启用 Binary Increase Congestion，这样可以更好地利用以 GB 速度进行操作的链接，对于 WAN 通信应该启用这个选项 net.ipv4.tcp_max_tw_buckets 该参数设置系统的 TIME_WAIT 的数量，如果超过默认值则会被立即清除。默认为 180000 net.ipv4.tcp_synack_retries 指明了处于 SYN_RECV 状态时重传 SYN+ACK 包的次数 net.ipv4.tcp_abort_on_overflow 设置改参数为 1 时，当系统在短时间内收到了大量的请求，而相关的应用程序未能处理时，就会发送 Reset 包直接终止这些链接。建议通过优化应用程序的效率来提高处理能力，而不是简单地 Reset。默认值： 0 net.ipv4.route.max_size 内核所允许的最大路由数目 net.ipv4.ip_forward 接口间转发报文 net.ipv4.ip_default_ttl 报文可以经过的最大跳数 net.netfilter.nf_conntrack_tcp_timeout_established 让 iptables 对于已建立的连接，在设置时间内若没有活动，那么则清除掉 net.netfilter.nf_conntrack_max 哈希表项最大值 Ref：1.https://help.aliyun.com/knowledge_detail/41334.html2.https://www.ibm.com/support/knowledgecenter/en/linuxonibm/liaag/wkvm/wkvm_c_tune_tcpip.htm]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令tcpdump与tcp zero window]]></title>
    <url>%2F2019%2F06%2F01%2Flinux%E5%91%BD%E4%BB%A4tcpdump%E4%B8%8Etcp%20zero%20window%2F</url>
    <content type="text"><![CDATA[example: tcpdump -i ens250 port 34495 -w t.pcap # -i: 指定监听网卡 # port: 指定监听端口 # -w file: 将数据保存在外部文件中，以.pcap结尾的文件可以通过wireshark打开 常用选项： -c count # 捕获count个packet后退出 tcp zero windowTCP Window size是指机器在一个TCPsession中可以接收多大的数据量。类似于TCP中的接收缓冲，当客户端和服务端建立连接时，客户端会通过Window Size告诉服务端可以接收多少数据。 当建立TCP连接后，服务端开始向客户端发送数据，客户端会减少Window Size当接收的数据存在于缓冲中时，同时客户端会在缓冲中处理数据，清空缓冲以便接收更多的数据。通过TCP ACK frames客户端会告诉服务端还可以接收多大的数据。当TCP Window Size等于0时，我们称为零窗口，此时客户端不能再接收数据，直到缓冲中的数据被清空。 wireshark提示的[TCP window Full]和[TCP zero windwo]意义不同,前者表示这个包的发送方意识到”在途字节数”已经达到对方所声明的窗口,不能再发了;而后者表示这个包的发送意识到自民 的缓存区已经满了,无法接收更多数据。 Ref： 1.https://wiki.wireshark.org/TCP%20ZeroWindow]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>tcp/ip</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql元数据相关SQL]]></title>
    <url>%2F2019%2F06%2F01%2Fpostgresql%E5%85%83%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3SQL%2F</url>
    <content type="text"><![CDATA[# 查看schemas SELECT schema_name FROM information_schema.schemata; SELECT nspname FROM pg_catalog.pg_namespace; # 查看连接状态 SELECT * FROM pg_stat_activity; # kill 连接数据库ptldwh的所有session SELECT pg_terminate_backend(a.pid) FROM pg_stat_activity a WHERE datname=&#39;ptldwh&#39;;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[postgresql中将所有的char改为varchar类型]]></title>
    <url>%2F2019%2F05%2F26%2Fpostgresql%E4%B8%AD%E5%B0%86%E6%89%80%E6%9C%89%E7%9A%84char%E6%94%B9%E4%B8%BAvarchar%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在将Postgresql数据转换为Neo4j的过程中，char类型的字段的在转换成Neo4j的属性后会带有空格。比如char(10)字段的数据为hello在Neo4j属性中则会带有空格，虽然可以在查询的时候加上trim函数，但这样对于原来的查询代码改动太大。所以决定从源头将这个问题解决。 假如有一个字段为char(10)如果插入的数据长度未达到10,关系型数据库默认在后面添加空格，所以将char类型改为varchar可以解决这个问题。 在char类型改为varchar的过程中，会默认把char类型中的空格给去掉。 在postgresql中有一个表information_schema.columns记录了所有字段的信息，所以可以从这个表中提取信息进行char到varchar的转换。 # 这个语句会生成将char改为varchar的sql语句 SELECT CONCAT(&#39;alter table &#39;, table_schema, &#39;.&#39;, table_name, &#39; alter column &#39;, column_name, &#39; type &#39;, &#39;varchar &#39;, &#39;(&#39;,character_maxinum_length,&#39;);&#39;) from information_schema.columns where table_catalog=&#39;ptl&#39; and date_type=&#39;character&#39;; information_schema.columns中的常用字段。 字段 描述 table_catalog 数据库名 table_schema 数据库的schema table_name 表名 column_name 字段名 data_type 数据类型 character_maximum_length 如果类型为character或bit string type,则为可存储最大字符数 Ref：1.information_schema.columns]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Dgraph]]></title>
    <url>%2F2019%2F05%2F19%2F%E5%88%9D%E8%AF%86Dgraph%2F</url>
    <content type="text"><![CDATA[1.Dgraph介绍 关键字：快速，支持事务，分布式图数据库 ２.安装，启动Dgraph docker pull dgraph/dgraph:v0.7.7 mkdir -p ~/dgraph docker run -it -p 127.0.0.1:8080:8080 -p 127.0.0.1:9080:9080 -v ~/dgraph:/dgraph --name dgraph dgraph/dgraph dgraph --bindall=true 3.访问http://127.0.0.1:8080/ 4.在输入框中，分别输入如下数据，点击run mutation { set { _:luke &lt;name&gt; &quot;Luke Skywalker&quot; . _:leia &lt;name&gt; &quot;Princess Leia&quot; . _:han &lt;name&gt; &quot;Han Solo&quot; . _:lucas &lt;name&gt; &quot;George Lucas&quot; . _:irvin &lt;name&gt; &quot;Irvin Kernshner&quot; . _:richard &lt;name&gt; &quot;Richard Marquand&quot; . _:sw1 &lt;name&gt; &quot;Star Wars: Episode IV - A New Hope&quot; . _:sw1 &lt;release_date&gt; &quot;1977-05-25&quot; . _:sw1 &lt;revenue&gt; &quot;775000000&quot; . _:sw1 &lt;running_time&gt; &quot;121&quot; . _:sw1 &lt;starring&gt; _:luke . _:sw1 &lt;starring&gt; _:leia . _:sw1 &lt;starring&gt; _:han . _:sw1 &lt;director&gt; _:lucas . _:sw2 &lt;name&gt; &quot;Star Wars: Episode V - The Empire Strikes Back&quot; . _:sw2 &lt;release_date&gt; &quot;1980-05-21&quot; . _:sw2 &lt;revenue&gt; &quot;534000000&quot; . _:sw2 &lt;running_time&gt; &quot;124&quot; . _:sw2 &lt;starring&gt; _:luke . _:sw2 &lt;starring&gt; _:leia . _:sw2 &lt;starring&gt; _:han . _:sw2 &lt;director&gt; _:irvin . _:sw3 &lt;name&gt; &quot;Star Wars: Episode VI - Return of the Jedi&quot; . _:sw3 &lt;release_date&gt; &quot;1983-05-25&quot; . _:sw3 &lt;revenue&gt; &quot;572000000&quot; . _:sw3 &lt;running_time&gt; &quot;131&quot; . _:sw3 &lt;starring&gt; _:luke . _:sw3 &lt;starring&gt; _:leia . _:sw3 &lt;starring&gt; _:han . _:sw3 &lt;director&gt; _:richard . _:st1 &lt;name&gt; &quot;Star Trek: The Motion Picture&quot; . _:st1 &lt;release_date&gt; &quot;1979-12-07&quot; . _:st1 &lt;revenue&gt; &quot;139000000&quot; . _:st1 &lt;running_time&gt; &quot;132&quot; . } } mutation { schema { name: string @index . release_date: date @index . revenue: float . running_time: int . } } 5.查询数据, { me(func:allofterms(name, &quot;Star Wars&quot;)) @filter(ge(release_date, &quot;1980&quot;)) { name release_date revenue running_time director { name } starring { name } } } 点击RUN 总结：刚开始是安装的dgraph/dgraph:v1.0.12,但是启动后访问浏览器显示disconneted，为了快速看到浏览器中节点和关系的效果所以安装了dgraph/dgraph:v0.7.7。后续继续了解。 Ref：1.https://yq.aliyun.com/articles/237205 2.https://docs.dgraph.io/get-started/#dataset]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker切换成阿里的源]]></title>
    <url>%2F2019%2F05%2F19%2Fdocker%E5%88%87%E6%8D%A2%E6%88%90%E9%98%BF%E9%87%8C%E7%9A%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[环境：ubuntu18.04 1.访问镜像加速,会生成一个镜像加速地址。 2.按如下命令操作即可 sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json &lt;&lt;-&#39;EOF&#39; { &quot;registry-mirrors&quot;: [&quot;https://xxxxxxx.mirror.aliyuncs.com&quot;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker]]></content>
      <categories>
        <category>未分类</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pg_dump和pg_restore的使用]]></title>
    <url>%2F2019%2F05%2F07%2Fpg_dump%E5%92%8Cpg_restore%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[pg_dump是用于postgresql数据备份的工具。即使数据库在并发的情况下也能保持一致性，并且不阻塞其它用户对数据库进行读和写。 将表account和host表中数据dump到szd.dump文件中 pg_dump -n &#39;public&#39; -Uroot --no-owner --no-privileges --host=100.2.2.1 --port=5432 --disable-triggers -d szd -t account -t host &gt; szd.dump 将数据恢复到指定的数据库中 pg_restore -n &#39;public&#39; -Uroot --no-owner --host=100.2.2.3 --port=6434 -t account -t host szd.dump 但是发再使用上述命令备份的并不包含索引，查看文档未发现可以恢复索引的选项。经实战发现如下方法可以备份索引，1.通过pg_dump备份表的结构信息，2.通过psql恢复表结构，3.最后通过pg_restore恢复数据。 1.备份表结构信息 pg_dump -n &#39;public&#39; -Uroot --no-owner --schema-only --no-privileges --host=100.2.2.1 --port=5432 --disable-triggers -d szd -t account -t host &gt; szd2.dump 2.恢复表结构信息以及索引 psql -h100.2.2.3 -p6434 -Uroot szd &lt; szd2.dump 3.恢复数据 pg_restore -n &#39;public&#39; -Uroot --no-owner --data-only --host=100.2.2.3 --port=6434 -t account -t host szd.dump create database ptldwh template template0; #To make an empty database without any local additions pg_dump -Fc -v -n &#39;public&#39; --host=127.0.0.1 -p5433 --no-owner --no-privileges --username=ptldwhdata ptldwh -t host &gt;ptldwh.dump -t &#39;t_*&#39; #指定要同步的表名 pg_restore -n &#39;public&#39; -Uroot --no-owner --no-privileges --disable-triggers -d ptldwh -t host ptldwh.dump 常用参数 参数 描述 -n schema 指定schema,只备份schema下的数据 —data-only 只备份数据 —format=format 指定输出格式 -N schema 不备份此schema下的数据 —no-owner 不输出设置对象ownership的命令 —schema-only 只备份表结构信息 -t table 指定需要备份的表 -T table 不备份此表 Ref：1.pg_dump2.pg_restore]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>postgresql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql之隔离级别(isolation level)]]></title>
    <url>%2F2019%2F04%2F27%2Fmysql%E4%B9%8B%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB(isolation%20level)%2F</url>
    <content type="text"><![CDATA[一、ACID：事务的4个特性 原子性(atomicity):在一个事务中，操作要么成功，要么失败。如果事务中有一个命令失败，必须回滚操作，回到事务执行之前的状态。 一致性(consistency):事务在完成时，必须使所有的数据都保持一致状态。 隔离性(isolation):事务在查看数据时数据所处的状态，要么是另一并发事务修改它之前的状态，要么是另一事务修改它之后的状态，事务是不会查看中间状态的。 持久性(durability):事务完成之后，它对于系统的影响是永久性的。 二、隔离级别(ACID中的I)InnoDB提供四种隔离级别 READ UNCOMMITTED: 读未提交 READ COMMITTED: 读已提交 REPEATABLE READ: 重复读 SERIALIZABLE: 串行化 不一致情况： 脏读：一个事务读取了另一个未提交事务写入的数据。 不可重复读：指一个事务重新读取前面读取过的数据时，发现该数据已经被另一个已提交事务修改了。 幻读：一个事务开始后，需要根据数据库中现有的数据做一些更新，于是重新执行一个查询，返回一套符合查询条件的行，这是发现这些行因为其他最近提交的事务而发生了改变，导致现有的事务如果再进行下去就可能会在逻辑上出现一些错误。（幻读，并不是说两次读取获取的结果集不同，幻读侧重的方面是某一次的 select 操作得到的结果所表征的数据状态无法支撑后续的业务操作。更为具体一些：select 某记录是否存在，不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入，此时就发生了幻读。） 事务隔离级别的行为 隔离级别 脏读 不可重复读 幻读 读未提交 可能 可能 可能 读已提交 不可能 可能 可能 重复读 不可能 不可能 可能 可串行化 不可能 不可能 不可能 模拟发生幻读： 事务1 事务2 事务1：检查表中是否有id=10的记录，没有则插入，这是正常逻辑。 事务2: 干拢了事务1的正常执行。 用户可以通过SET TRANSACTION命令改变隔离级别，或通过--transaction-isolation配置server级别的隔离性。 MySQL通过使用不同的锁策略实现不同的隔离级别，默认隔离级别是REPEATABLE READ，以提供更高级别的一致性。你也可以指定隔离级别为READ COMMITTED甚至READ UNCOMMITTED一致性要求没那么高的场景(减少锁以提高性能)。SERIALIZABLE提供比REPEATABLE更严格的 规则，一般会用于一些特殊的场景，比如XA事务和用于调试并发或死锁问题。 REPEATABLE READ这是MySQL的默认隔离级别。在同一事务中的读取的数据是一致的，在第一次读时会建立一个快照，同一事务中读取到的都是第一个快照中的值。也就是说在同一个事务中，SELECT(noblocking)所获取到的数据都是一致的。对于REPEATABLE READ隔离级别总是读取事务开始时的行数据。 READ COMMITTED 读已提交每一次读都会建立一个新的快照并从中读取数据,它总是读取行的最新版本，如果行被锁定，则读取该行版本的最新一个快照。 对于READ COMMITTED只支持 row-based的binlog。 其它影响： - 对于UPDATE,DELETE操作，InnoDB只会锁住需要需要更新或删除的行。对于不匹配的行，在执行完WHERE语句后锁会释放。这极大的降低了死锁的概率，但还是有可能发生。 - 对于UPDATE操作，如果行已经被加锁，InnoDB``表现为semi-consistent读，将返回最近提交的版本，MySQL再决定是否符合WHERE条件。如果匹配，MySQL```会重新读取这行，然后获取锁或者等待锁。 READ UNCOMMITTEDSELECT表现为nonlocking fashion,但是可能会读取到旧版本的数据，所以这种隔离级别不保持一致性读，也称为脏读。 SERIALIZABLESERIALIZABLE和REPEATABLE READ隔离级别类似，当autocommit=false情况下，所有SELECT语句会被转换为SELECT ... LOCK IN SHARE MODE。 示例： MySQL默认隔离级别为： 在REPEATABLE READ隔离级别情况下，开启两个session,先左边的窗口开启一个事务读取表a中的数据，然后右边的窗口再开启个事务对表a数据进行修改，右边的事务提交后，在左边的事务再次查看表a中的数据。发现两次得到的数据是一致性的，因为对于REPEATABLE READ隔离级别总是读取事务开始时的行数据。 在READ COMMITTED隔离级别重复上述的操作,发现在右边的事务提交后再查看表a的数据，此时表a的数据已更新。因为在这个隔离级别下，它总是读取行的最新版本，如果行被锁定，则读取该行版本的最新一个快照。 Ref：1.innodb-transaction-isolation-levels]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysqldump]]></title>
    <url>%2F2019%2F04%2F12%2Fmysqldump%2F</url>
    <content type="text"><![CDATA[mysqldump是mysql提供的用于数据备份的工具，通过再次执行SQL语句以达到恢复数据的目的（生产环境慎用)。mysqldump的优势在于方便和在恢复之前可以查看dump文件甚至修改文件内容。通常用于开发测试，及数据量比较小的情况下，当数据量大的情况下可能dump所花费的时间的时间是可以接受的，但恢复的时候会消耗大量的磁盘I/O。 1.mysqldump所需的权限 mysqldump所需要的权限跟其所dump的对象有关，若备份表则需要SELECT权限，若备份视图，则需要SHOW VIEW权限，若备份触发器，则需要TRIGGER权限。 2.锁表问题 mysqldump默认会锁住需要dump的表 3.mysqldump常用参数 mysqldump [options] &gt; dump.sql —options Format 描述 —all-databases 备份所有数据库 —databases 指定所需备份的数据库 —no-data 不备份数据 —user 用户名 —host IP或主机名 —port 端口 —lock-all-table 锁住需要dump数据库的所有表，会使选项—lock-tables和—single-transaction关闭 —lock-tables 会锁住每个数据库中需要dump的表 —password 密码 —single-transaction 只适用于InnoDB引擎，当执行dump命令时，客户端会发一条START TRANSACTION的语句给server,会dump事务开始时的数据库状态，不会锁表，所以不会对应用造成影响 —quick 一行一行进行dump，不会将数据缓存在内存中，每次都需要写文件，速度会变慢 —skip-quick 先将数据缓存在内存中，再一次写到文件中，速度较快，但不适用的大表 —tables 指定表,会覆盖—databases选项 详细参数请参考4.示例 # dump mysqldump -hlocalhost -uroot -pxxxx --databases test --tables table_a table_b &gt; test.sql # restore mysql -uroot -pxxxx test &lt; test.sql]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql批量导出恢复指定库中的特定表]]></title>
    <url>%2F2019%2F04%2F11%2Fmysql%E6%89%B9%E9%87%8F%E5%AF%BC%E5%87%BA%E6%81%A2%E5%A4%8D%E6%8C%87%E5%AE%9A%E5%BA%93%E4%B8%AD%E7%9A%84%E7%89%B9%E5%AE%9A%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[目的：批量导出和恢复多个库中的特定表 1.先建立一个文件用于存储库名和表名 cat databases.txt # 每行第一个为库名，其余为表名，以英文逗号分隔 portal,if_zone,uc_order,ecs_instance cdn,cdn_domain 2.编写dump脚本 #!/bin/bash for elt in $(cat databases.txt) do echo &quot;start dump $(echo $elt|cut -d \, -f 1)&quot; mysqldump -hlocalhost -uroot -pxxxx --databases $(echo $elt|cut -d \, -f 1) --tables $(echo $elt|cut -d \, -f 2- | sed &#39;s/,/ /g&#39;) &gt; $(echo $elt|cut -d \, -f 1).sql echo &quot;done&quot; done # cut -d \, -f 1 对输入字符以逗号进行切割，取第 一个 # cut -d \, -f 2- 对输入字符以逗号进行切割，取第 二个到最后一个 # sed &#39;s/,/ /g&#39; 将输入中的全部&#39;,&#39;替换为空格 3.编写restore脚本 #!/bin/sh for elt in $(ls *.sql) do echo &quot;start restore ${elt%%.*}&quot; mysql -uroot -pxxxx -e &quot;create database if not exists ${elt%%.*}&quot; mysql -uroot -pxxxx ${elt%%.*} &lt; $elt echo &quot;done&quot; done 警示：在运行的系统中执行mysqldump命令，需考虑风险，因为mysqldump会有一些锁表的操作，详情详见下面Ref中的链接。 Ref：1.linux sed2.linux cut3.mysqldump]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sbt简介,scala项目hello,world]]></title>
    <url>%2F2019%2F04%2F09%2Fsbt%E7%AE%80%E4%BB%8B%2Cscala%E9%A1%B9%E7%9B%AEhello%2Cworld%2F</url>
    <content type="text"><![CDATA[sbt(Simple Build Tool)是官方推荐的Scala项目构建工具,为你的项目提供编译，运行，测试等各种功能。 １.sbt安装 - 依赖jdk8,如没安装，需要先安装jdk8 echo &quot;deb https://dl.bintray.com/sbt/debian /&quot; | sudo tee -a /etc/apt/sources.list.d/sbt.list sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823 sudo apt-get update sudo apt-get install sbt 2.启动sbt,第一次启动需要下载一些包速度会比较慢,更换源请Ref更换国内源 3.新建一个简单的项目，结构如下,创建目录脚本 ４.在目录src/main/scala下，新建一个文件hello.scala hys@hys:~/code/scala/helloscala/src/main/scala$ cat hello.scala package foo.bar.baz object Main extends App { println(&quot;Hello, Scala&quot;) } 5.编译，运行 6.sbt命令 Ref： １.https://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Linux.html 2.scala编程实战]]></content>
      <categories>
        <category>Scala</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scala REPL安装]]></title>
    <url>%2F2019%2F04%2F09%2Fscala%20REPL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1.直接使用apt-get命令安装 sudo apt-get install scala 安装scala版本为2.11.12，却发现进入REPL后，不能输入。2.直接下载最新版本2.12.8二进制文件进行安装,解压，然后设置环境变量。 ３.输入scala,直接进入REPL。]]></content>
      <categories>
        <category>Scala</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu常见dpkg错误]]></title>
    <url>%2F2019%2F04%2F09%2Fbuntu%E5%B8%B8%E8%A7%81dpkg%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[Ref：１.https://itsfoss.com/could-not-get-lock-error/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sbt设置国内源]]></title>
    <url>%2F2019%2F04%2F09%2Fsbt%E8%AE%BE%E7%BD%AE%E5%9B%BD%E5%86%85%E6%BA%90%2F</url>
    <content type="text"><![CDATA[sbt默认使用的是国外的源，速度太慢，因此改为国内的源。 1.修改下面两个文件，在末尾添加-Dsbt.override.build.repos=true /usr/share/sbt/conf/sbtconfig.txt /usr/share/sbt/conf/sbtopts 2.vim ~/.sbt/repositories如果不存在则创建 hys@hys:~$ cat ~/.sbt/repositories [repositories] local aliyun-nexus: http://maven.aliyun.com/nexus/content/groups/public/ typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly sonatype-oss-releases maven-central sonatype-oss-snapshots 3.生效。 Ref： １.https://www.jianshu.com/p/a867b2a7c3c8]]></content>
      <categories>
        <category>Scala</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shell创建目录]]></title>
    <url>%2F2019%2F04%2F09%2Fshell%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[在使用shell脚本创建目录的过程中发现一个现象 当解释器为#!/bin/sh时，结果如下 当解释器为!/bin/bash时，结果如下 原因可能是sh解释器不支持{}语法。 #!/bin/bash mkdir -p src/{main,test}/{java,resources,scala} mkdir lib project target echo &#39;name := &quot;myproject&quot; version := &quot;1.0&quot; scalaVersion := &quot;2.12.0&quot;&#39; &gt; build.sbt]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux命令strace]]></title>
    <url>%2F2019%2F03%2F30%2Flinux%E5%91%BD%E4%BB%A4strace%2F</url>
    <content type="text"><![CDATA[strace是Linux/Unix下的一款可以追踪系统调用的工具。 1.安装strace apt-get install strace # ubuntu yum install strace # centos 2.使用示例 3.strace -pstrace -p pid # 根据进程id,追踪此进程系统调用 查看nginx: worker process进程的系统调用 4.strace -c统计时间，调用数，错误 5.strace -e type=exprexpr = set, file, process, network, signal,ipc, desc,memory 指定追踪系统调用类型 6.strace -o out.txt将输出重定向一 个文件 7.strace -i输出系统调用的指针地址 8.详细参数可通过下面命令查看man strace]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zero-copy]]></title>
    <url>%2F2019%2F03%2F20%2Fzero-copy%2F</url>
    <content type="text"><![CDATA[在许多web应用中需要从磁盘中读取数据然后再将数据发送到socket中，而数据复制这一操作是非常消耗资源的。 当执行content_to_write = content[1024:]时，这个操作是复制从1024字节后的所有数据到变量content_to_write中，这个过程中增加了9.789MB内存。 有没有方法可以在这种操作中不复制数据呢，在实现了buffer protocol的对象中是可以实现的。当一个对象实现了这个协议，你可以使用memoryview构造一个memoryview对象，实现对源对象内存数据的引用。 使用memoryview 并没有产生多余的内存。 在web应用中将数据从磁盘复制，然后发送到socket中。 普通的复制操作是这样的 file.read(fileDesc, buf, len); socket.send(socket, buf, len); 用户请求向web应用请求数据，数据复制的过程如下： 1.web应用调用系统调用read()方法，此处从用户态切换到内核态。1过程使用DMA从磁盘中复制数据到Read buffer中。 2.当read()返回时，此处从内核态切换到用户态，将数据复制到应用缓存中。 3.web应用调用send()方法，此处从用户态切换到内核态，将数据复制到内核空间的内存中。 4. 4过程使用DMA方法从内核空间的缓存中复制数据到socket中。 当数据量很大时，这一过程是极其缓慢的。 零拷贝方法:直接将数据从源端拷贝到目标端 os.sendfile(out, in, offset, count, [headers, ][trailers, ]flags=0) 传统拷贝方法所消耗的时间： 零拷贝方法所消耗的时间： 使用传统方法的客户端： 使用零拷贝方法的客户端：没产生大内存 pip3 install memory_profiler pip3 install line_profiler Ref:1.Efficient data transfer through zero copy2.high-performance-in-python-with-zero-copy-and-the-buffer-protocol/]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql shell 批量恢复]]></title>
    <url>%2F2019%2F03%2F20%2Fmysql%20shell%20%E6%89%B9%E9%87%8F%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[有多个mysql的数据文件在文件夹在tmp文件夹下 ls /root/tmp network.dump igw.dump vpn.dump vpc.dump 编写shell恢复数据 #!/root/tmp for elt in $(ls *.dump) do echo &quot;restore database $elt&quot; mysql -uroot -pxxxxx ${elt%%.*} &lt; $elt done]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pip3的一个bug]]></title>
    <url>%2F2019%2F03%2F11%2Fpip3%E7%9A%84%E4%B8%80%E4%B8%AAbug%2F</url>
    <content type="text"><![CDATA[import sys from pip import main if __name__ == &#39;__main__&#39;: sys.exit(main()) sudo vim /usr/bin/pip3 #将上面代码改为如下import sys from pip import __main__ if __name__ == &#39;__main__&#39;: sys.exit(__main__._main()) Ref： 1.stackoverflow]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之cascade delete]]></title>
    <url>%2F2019%2F03%2F09%2Fmysql%E4%B9%8Bcascade%20delete%2F</url>
    <content type="text"><![CDATA[ON DELETE CASCADE表示如果父表中记录删除，则子表中引用了父表记录的行都会被删除。 # 新建表 CREATE TABLE a ( id int not null, name varchar(20), primary key(id) ); CREATE TABLE b ( id int not null, a_id int not null, CONSTRAINT `fk_a__id` FOREIGN KEY (`a_id`) REFERENCES `a` (`id`) ON DELETE CASCADE ); 当设置SET GLOBAL FOREIGN_KEY_CHECKS=0;(下次登录生效)时，级联删除并不会生效。 当我们使用语句delete from a时，mysql会将b表中的数据也会删除，那删除b表中的数据这一操作会不会记在binlog中呢？？ 经过验证不管设置何种binlog_formatb表中的数据变化并不会记录在日志文件中。级联操作是根据存储引擎通过外键关系来进行操作的，并不会体现在日志文件中。mysql文档中有详细的解释]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之binlog-format]]></title>
    <url>%2F2019%2F03%2F09%2Fmysql%E4%B9%8Bbinlog-format%2F</url>
    <content type="text"><![CDATA[mysql支持3种日志记录格式，STATEMENT,ROW,MIXED 可在启动时通过--binlog_format=type指定。 STATEMENT:也被称作logical logging,记录mysql执行的语句 ROW:也被称作physical logging,记录每行数据的变化 MIXED:即STATEMENT和ROW的混合形式，mysql会根据场景决定使用哪种日志格式 查看目前使用的日志格式show variables like ‘binlog_format’; 设置全局binlog_format，重新登录后生效SET GLOBAL binlog_format = ‘STATEMENT’;SET GLOBAL binlog_format = ‘ROW’;SET GLOBAL binlog_format = ‘MIXED’; 设置会话级别的binlog_formatSET SESSION binlog_format = ‘STATEMENT’;SET SESSION binlog_format = ‘ROW’;SET SESSION binlog_format = ‘MIXED’; 设置会话级别binlog_format的几种情况： - 假如对数据库做了比较小的改动，可能会把binlog_format设置为ROW - 假如一次会话中在一条语句中更新了多行记录，此时使用STATEMENT可能会更高效 - 有些语句可能执行需要的时间比较长，但结果只是几行数据被修改，这种情况也可以将binlog_format改为ROW 但一般不建议在运行时改变binlog_format,下面几种不建议改变binlog_format - 如果使用NDB存储引擎 - 如果当前使用的是row-base replication而且存在临时表 当存在任何临时表时都不应该在运行时改变binlog_format,因为只有在STATEMENT的情况下日志才会记录临时的信息，而在ROW的格式下是不会记录的。在MIXED的模式下临时表通常也会记录。 每个mysql实例都可以设置自己的binlog_format，每个实例会以自己的binlog_format方式去解析日志文件，如果主从复制的架构中，master修改了binlog_format而slave没有修改，则会报错。为会确保改变binlog_format安全，需要停止复制并确保各个实例使用的是同一种日志格式。 Ref： 1.MYSQL Binary Log Format]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之％]]></title>
    <url>%2F2019%2F02%2F23%2Fmysql%E4%B9%8B%EF%BC%85%2F</url>
    <content type="text"><![CDATA[mysql在创建用户时可以指定host,通配符%表示可以匹配任何host(当然localhost除外，localhost优先级大于%) create user &#39;hys&#39;@&#39;%&#39; identified by &#39;hello&#39;; create user &#39;hys&#39;@&#39;localhost&#39; identified by &#39;wtf&#39;; create user &#39;hys&#39;@&#39;127.0.0.1&#39; identified by &#39;666&#39;; flush privileges; 使用命令进行连接mysql -uhys -phello,报错如下 使用命令进行连接mysql -uhys -pwtf则可以正常连接。 使用命令进行连接mysql -uhys -h127.0.0.1 -p666 说明mysql在进行连接时把127.0.0.1转换成了localhost 删除掉hys@localhost 使用命令进行连接mysql -uhys -h127.0.0.1 -p666可正常连接 结论： 1.通配符不包括localhost 2.当同一用户host同时存在localhost和%时，会优先匹配localhost 3.当同一用户host同时存在localhost和127.0.0.1时,虽然连接时指定了-h127.0.0.1但也会先匹配localhost 4.当存在127.0.0.1而不存在localhost，指定-h127.0.0.1可以进行连接 Ref： https://dev.mysql.com/doc/mysql-security-excerpt/8.0/en/problems-connecting.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之plugin]]></title>
    <url>%2F2019%2F02%2F23%2Fmysql%E4%B9%8Bplugin%2F</url>
    <content type="text"><![CDATA[mysql支持可插拔式的认证方式。 插拔式认证拥有以下能力： 1.可选择内置(native)的认证方式 2.外部认证方式，如PAM, Windows login IDs, LDAP, Kerberos 3.代理用户 mysql包括两种native认证方式mysql_native_password, mysql_old_password都是通过系统表mysql.user和密码hash算法实现的。 认证的插件都在存在于客户端和服务端的，服务端的是内置于mysql服务器中。客户端的则在于libmysqlclient中。 - SHA-256 Pluggable Authentication 这种认证方法是比mysql_native_password认证方法安全性更高，采用SHA-256哈希算法对明文进行加密。 创建SHA-256认证的用户 CREATE USER &#39;sha256user&#39;@&#39;localhost&#39; IDENTIFIED WITH sha256_password BY &#39;password&#39;; Client-Side Cleartext Pluggable Authentication，存储明文 PAM认证 外部认证方式，使mysql服务器通过PAM服务来进行用户认证 Windows认证 外部认证方式，使mysql服务器通过原生Windows服务授权客户端连接，当用户登录了 Windows也可以连接mysql服务器(通过环境信息确认用户，不用额外的密码) LDAP认证 外部认证方式，Lightweight Directory Access Protocol No-Login 认证 阻止任何使用此plugin的用户连接mysql服务。此种方式不允许直接登录可以使用代理 方式登录 Socket Peer-Credential认证 本地用户通过连接Unix socket文件进行认证。 Test Pluggable Authentication 测试插件用于测试用户认证是否成功，而且会把是否成功的日志打印到server error log。这个插件的目的是用于测试和开发，也可以作为一个如何写认证插件的例子。 Ref：1.https://dev.mysql.com/doc/refman/5.7/en/pam-pluggable-authentication.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之ubuntu环境下安装]]></title>
    <url>%2F2019%2F02%2F19%2Fmysql%E4%B9%8Bubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[直接上命令 sudo apt-get install mysql-server sudo apt-get install mysql-client sudo apt install libmysqlclient-dev #客户端认证plugin需要依赖这个库 但是在这过程中没有提示设置密码。 在/etc/mysql/目录下有一个debian.cnf文件，文件中有一个系统分配的用户和密码。用户名为debian-sys-maint,通过这个用户就可以操作mysql了。 可以新建一个root用户并对其进行授权,然后用新用户进行登录。 create user &#39;root&#39;@&#39;%&#39; ; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;hello&#39;; flush privileges;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之flush privileges]]></title>
    <url>%2F2019%2F02%2F19%2Fmysql%E4%B9%8Bflush%20privileges%2F</url>
    <content type="text"><![CDATA[FLUSH PRIVILEGES # 重新从grant tables加载权限 使用GRANT,CREATE USER, CREATE SERVER, INSTALL PLUGIN产生的缓存并不会因为使用对应的REVOKE, DROP USER, DROP SERVER, UNINSTALL PLUGIN而释放缓存。这些缓存可以通过调用FLUSH PRIVILEGES而释放。 GRANT TABLES IN MYSQLuser: 用户帐户，全局权限 db: database级别的权限 tables_priv: 表级别的权限 columns_priv: 列级别的权限 procs_priv: 存储过程和函数权限 proxies_priv: 代理用户权限 Ref：1.grant tables 2.flush privileges]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql之修改用户密码]]></title>
    <url>%2F2019%2F02%2F18%2Fmysql%E4%B9%8B%E4%BF%AE%E6%94%B9%E7%94%A8%E6%88%B7%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[mysql中有几种方式可以修改用户密码。 1.直接修改mysql.user中的password字段 update mysql.user set password=password(&#39;hello&#39;) where user=&#39;t&#39; flush privileges; 2.使用create user,grant或set password create user t identified by &#39;password&#39;; grant select on cloud.* to &#39;root&#39;@&#39;10.68.6.66&#39; identified by &#39;password&#39; set password for &#39;root&#39;@&#39;localhost&#39; =&#39;auth_string&#39;; 在5.5.60中执行报错 set password for &#39;root&#39;@&#39;localhost&#39; = password(&#39;auth_string&#39;); 在5.7.6中已废弃这种语法，将来会移除]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sqlacodegen]]></title>
    <url>%2F2019%2F02%2F17%2Fsqlacodegen%2F</url>
    <content type="text"><![CDATA[Generates SQLAlchemy model code from an existing database. positional arguments: url SQLAlchemy url to the database optional arguments: -h, --help show this help message and exit --version print the version number and exit --schema SCHEMA load tables from an alternate schema(指定schema) --tables TABLES tables to process (comma-separated, default: all) --noviews ignore views --noindexes ignore indexes --noconstraints ignore constraints(忽略外键) --nojoined don&#39;t autodetect joined table inheritance --noinflect don&#39;t try to convert tables names to singular form --noclasses don&#39;t generate classes, only tables --outfile OUTFILE file to write output to (default: stdout) 例： sqlacodegen mysql+pymysql://root:root@127.0.0.1/cloud --noviews --noconstraints --tables host,vpn --outfile out.txt]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pgloader的安装与使用]]></title>
    <url>%2F2018%2F11%2F13%2Fpgloader%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[pgloader是由postgresql的作者用lisp语言编写的一个用于将数据从其它数据库迁移到postgresql数据库中的命令行工具，亦可编写简单的脚本，保存为xxx.load文件。 在centos下安装pgloader1.安装依赖 sudo yum -y install yum-utils rpmdevtools @&quot;Development Tools&quot; sqlite-devel zlib-devel sudo yum -y install epel-release sbcl sudo yum -y install freetds freetds-devel 2.下载源码 tar -xv pgloader-bundle-3.5.1.tgz cd pgloader-bundle-3.5.1.tgz make pgloader 3.编译成功后bin文件夹下会出现pgloader可执行文件，将路径添加到环境变量。 vim ~/.bash_profile source ~/.bash_profile pgloader的使用 from mysql to postgresqlload database from mysql://username:password@localhost/test into postgresql://username:password@110.110.110.110/datahub with include drop, create tables, create indexes, reset sequences, disable triggers, foreign keys, workers=8, concurrency=1 set work_mem to &#39;16MB&#39;, maintenance_work_mem to &#39;512MB&#39; INCLUDING ONLY TABLE NAMES MATCHING &#39;base_job_type&#39; CAST type timestamp when default &quot;0000-00-00 00:00:00&quot; with extra on update current timestamp to timestamptz drop default drop not nul drop extra using zero-dates-to-null, type smallint when unsigned to integer drop typemod # FROM: 源数据库信息 # INTO: 目标数据库信息 # 迁移默认选项：no truncate, create schema, create tables, include drop, create indexes,reset sequences,foreign keys,downcase identifiers, uniquify index names # include drop: 丢弃目标库的表并重新创建 # include no drop: 不做任何drop操作 # truncate: 在迁移数据之前清空表 # no truncate: 不做truncate操作 # disable triggers: 这个选项允许，迁移数据到已经存在的表并忽略外键约束和用户定义的触发器，迁移后可能会出现无效的外键约束 # create tables: 根据元数据自动创建表结构 # create no tables: 不创建表结构，当迁移数据时必须保证表存在 # create indexes: 创建索引 # create no indexes: 不创建索引 # drop indexes: 在迁移数据之前drop indexes,当数据迁移完成之后重新创建索引 # uniquify index names: 因为mysql的索引是表唯一，postgresql的索引是schema唯一，所以当从mysql到postgresql时索引名称可能会出现冲突，pgloader会通过特定规则对索引进行重新命名 # preserve index names: 保留索引名称，尽管如此 mysql primary key也会被重新命名，以保持唯一性 # drop schema: 迁移数据行 drop schema，然后重建 # foreign keys: 创建外键 # no foreign keys: 不创建外键 # reset sequences: 当数据迁移完成后，sequences会设置为当前列的最大值 # reset no sequences: 不设置sequences # downcase identifiers: 将表名，索引名，列名改为小写 # quote identifiers: 保留大小写 # schema only: 只迁移表结构 # data only: 只复制数据 pgloader转换规则中的坑假设mysql中有一个字段的类型为datetime,默认值为”0000-00-00 00:00:00” 如果转换规则写为type datetime to timestamptz此时pgloader并不会匹配上，因为pgloader必须全部匹配时，才会执行类型转换。可写为cast type datetime when default &quot;0000-00-00 00:00:00&quot; to timestamptz才能匹配,如果还有其它的限制也必须加上，如with extra on update current timestamp。这种情况和代码异常处理是不一样的。 Ref： 1.https://pgloader.readthedocs.io/en/latest/ref/mysql.html]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[记一次粗心的错误]]></title>
    <url>%2F2018%2F11%2F07%2F%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%B2%97%E5%BF%83%E7%9A%84%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[最近在学习使用alembic,有一个地方是要填写数据库的url如下： sqlalchemy.url = &#39;mysql://root:123456@10.10.20.11/test&#39; 不小心将mysql写成了msyql,结果报错提示msyql模块未安装。然后自己以为是mysql驱动没安装好，然后就是各种安装，结果还是一样。 一小时后，重新将url写一遍却正确了，于是查看日志才发现是mysql写错了。]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[neo4j安装]]></title>
    <url>%2F2018%2F10%2F14%2Fneo4j%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Neo4j是一个高性能的,NOSQL图形数据库，它将结构化数据存储在网络上而不是表中。它是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎，但是它将结构化数据存储在网络(从数学角度叫做图)上而不是表中。 应用场景： - 欺诈检测 - 实时推荐引擎 - Master data management(MDM) - Identity and access management(身份和访问管理-IAM) 来源 1.安装依赖java,下载jdk 2.设置环境变量 export JAVA_HOME=/opt/jdk1.8.0_191 export JRE_HOME=/opt/jdk1.8.0_191/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 1.下载neo4j安装包 2.解压，添加环境变量 cp neo4j-community-3.4.8-unix.tar.gz /opt tar -xf neo4j-community-3.4.8-unix.tar.gz export PATH=&quot;/opt/neo4j-community-3.4.8/bin:$PATH&quot; 3.修改配置文件 neo4j默认只能从本机访问，因些修改conf/neo4j.conf,改为可从其它主机访问 dbms.connectors.default_listen_address=0.0.0.0 4.开启neo4j服务neo4j console 5.打开浏览器输入xxx.xxx.xxx.xxx:7474,即可连接neo4j服务。默认用户名密码为:neo4j/neo4j,第一次会提示修改密码 6.连接后的界面如下]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python-rocksdb的基本使用]]></title>
    <url>%2F2018%2F10%2F14%2Fpython-rocksdb%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.打开一个数据库 import rocksdb db = rocksdb.DB(&quot;test.db&quot;,rocksdb.Options(create_if_missing=True)) # create_if_missing=True 如果不存在则创建名为&#39;test.db&#39;的数据库 2.设置键值对和获取值 # RocksDB中存储的是byte strings。在python2中为str类型，在python3中为bytes类型。 db.put(b&quot;key&quot;,b&quot;value&quot;) #设置一个键值对，key =&gt; value db.get(b&quot;key&quot;) #获取键为key的值，如果不在则返回为空 b&#39;value&#39; db.delete(b&quot;key&quot;) #删除键为key为的键值对 # 将几个操作合并为一个操作 batch = rocksdb.WriteBatch() batch.put(b&quot;key&quot;, b&quot;v1&quot;) batch.delete(b&quot;key&quot;) batch.put(b&quot;key&quot;, b&quot;v2&quot;) batch.put(b&quot;key&quot;, b&quot;v3&quot;) db.write(batch) # 一次获取多个键值对 &gt;&gt;&gt; db.put(b&quot;key1&quot;,b&quot;v1&quot;) &gt;&gt;&gt; ret = db.multi_get([b&quot;key&quot;,b&quot;key1&quot;]) &gt;&gt;&gt; ret {b&#39;key&#39;: b&#39;v3&#39;, b&#39;key1&#39;: b&#39;v1&#39;} 3.迭代 &gt;&gt;&gt; it = db.iterkeys() # 默认是无效的，先得调用seek方法 &gt;&gt;&gt; it.seek_to_first() &gt;&gt;&gt; print(list(it)) [b&#39;a&#39;, b&#39;key&#39;, b&#39;key1&#39;] &gt;&gt;&gt; it.seek_to_last() &gt;&gt;&gt; print(list(it)) [b&#39;key1&#39;] &gt;&gt;&gt; it = db.itervalues() #对值进行迭代 &gt;&gt;&gt; it.seek_to_first() &gt;&gt;&gt; print(list(it)) [b&#39;b&#39;, b&#39;v3&#39;, b&#39;v1&#39;] &gt;&gt;&gt; it = db.iteritems() #对键值对进行迭代 &gt;&gt;&gt; it.seek_to_first() &gt;&gt;&gt; print(list(it)) [(b&#39;a&#39;, b&#39;b&#39;), (b&#39;key&#39;, b&#39;v3&#39;), (b&#39;key1&#39;, b&#39;v1&#39;)] # 反向迭代 &gt;&gt;&gt; it.seek_to_last() &gt;&gt;&gt; print(list(reversed(it))) [(b&#39;key1&#39;, b&#39;v1&#39;), (b&#39;key&#39;, b&#39;v3&#39;), (b&#39;a&#39;, b&#39;b&#39;)] 4.快照 &gt;&gt;&gt; snapshot = db.snapshot() &gt;&gt;&gt; it = db.iteritems(snapshot=snapshot) &gt;&gt;&gt; it &lt;rocksdb._rocksdb.ItemsIterator object at 0x7f77ae179608&gt; &gt;&gt;&gt; it.seek_to_first() &gt;&gt;&gt; print(dict(it)) {b&#39;a&#39;: b&#39;b&#39;, b&#39;key&#39;: b&#39;v3&#39;, b&#39;key1&#39;: b&#39;v1&#39;} 5.备份和恢复 # 备份 backup = rocksdb.BackupEngine(&quot;test.db/backups&quot;) backup.create_backup(db, flush_before_backup=True) # 恢复 backup = rocksdb.BackupEngine(&quot;test.db/backups&quot;) backup.restore_latest_backup(&quot;test.db&quot;, &quot;test.db&quot;) Ref：1.https://python-rocksdb.readthedocs.io/en/latest/]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[rocksdb的编译安装]]></title>
    <url>%2F2018%2F10%2F14%2Frocksdb%E7%9A%84%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[rocksdb是一个基于level-db开发的高性能本地型键值对数据库。 系统环境：centos 7.6 git clone https://github.com/gflags/gflags.git cd gflags git checkout v2.0 ./configure &amp;&amp; make &amp;&amp; sudo make install # export LD_LIBRARY_PATH=/usr/lib yum install snappy snappy-devel yum install zlib zlib-devel yum install bzip2 bzip2-devel yum install lz4-devel wget https://github.com/facebook/zstd/archive/v1.1.3.tar.gz mv v1.1.3.tar.gz zstd-1.1.3.tar.gz tar zxvf zstd-1.1.3.tar.gz cd zstd-1.1.3 make &amp;&amp; sudo make install # 将共享库安装在/usr/lib/目录下，头文件在/usr/include/rocksdb wget https://codeload.github.com/facebook/rocksdb/tar.gz/v5.14.3 tar -xf rocksdb-5.14.3.tar.gz cd rocksdb-5.14.3 make install-shared INSTALL_PATH=/usr # 安装python-rocksdb pip3 install python-rocksdb 测试是否安装好 import rocksdb db = rocksdb.DB(&quot;test.db&quot;, rocksdb.Options(create_if_missing=True)) db.put(b&quot;a&quot;, b&quot;b&quot;) print(db.get(b&quot;a&quot;)) # 如果未出错表示rocksdb已安装好 Ref：1.https://github.com/facebook/rocksdb/blob/master/INSTALL.md2.https://python-rocksdb.readthedocs.io/en/latest/installation.html]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之ipaddress]]></title>
    <url>%2F2018%2F09%2F26%2Fpython%E4%B9%8Bipaddress%2F</url>
    <content type="text"><![CDATA[ip地址一共32位阿,20的掩码表示使用20位作为网络地址选项,那么就还剩下12位来做主机地址了。粗略就是2^12个主机ip了。 我们通过ipaddress模块，生成类似于”123.45.67.89/27”这样的CIDR(Classless InterDomain Routing)网络地址可表示的全部IP地址范围。 In [1]: import ipaddress In [2]: net = ipaddress.ip_network(&#39;123.45.67.64/27&#39;) In [3]: net Out[3]: IPv4Network(&#39;123.45.67.64/27&#39;) In [4]: for elt in net: ...: print(elt) ...: 123.45.67.64 123.45.67.65 123.45.67.66 123.45.67.67 123.45.67.68 123.45.67.69 123.45.67.70 123.45.67.71 123.45.67.72 123.45.67.73 123.45.67.74 123.45.67.75 123.45.67.76 123.45.67.77 123.45.67.78 123.45.67.79 123.45.67.80 123.45.67.81 123.45.67.82 123.45.67.83 123.45.67.84 123.45.67.85 123.45.67.86 123.45.67.87 123.45.67.88 123.45.67.89 123.45.67.90 123.45.67.91 123.45.67.92 123.45.67.93 123.45.67.94 123.45.67.95 In [5]: net.num_addresses Out[5]: 32 In [6]: net[0] Out[6]: IPv4Address(&#39;123.45.67.64&#39;) In [7]: a = ipaddress.ip_address(&#39;123.45.67.69&#39;) In [8]: a in net #判断ip是否在CIDR中 Out[8]: True ipaddress模块同其他网络相关的模块比如socket库之间的交互是有局限性的。通常不能用IPv4Address的实例作为地址字符串的替代。必须显式的通过str()将其转换为字符串。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[工厂模式(Factory Pattern)-创建型]]></title>
    <url>%2F2018%2F09%2F25%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F(Factory%20Pattern)-%E5%88%9B%E5%BB%BA%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[简单工厂设计模式：这允许接口创建对象而不暴露对象创建逻辑当客户端调用create_type()方法时，工厂会根据type参数的不同，返回Product1或者Product2。 from abc import ABCMeta, abstractmethod class Animal(metaclass = ABCMeta): @abstractmethod def do_say(self): pass class Dog(Animal): def do_say(self): print(&quot;Bhow Bhow!!&quot;) class Cat(Animal): def do_say(self): print(&quot;Meow Meow!!&quot;) class ForestFactory: def make_sound(self, obj_type): return eval(obj_type)().do_say() #根据类型直接调用不同的方法 if __name__ == &#39;__main__&#39;: ff = ForestFactory() ff.make_sound(&#39;Cat&#39;) [out]:Meow Meos!! # 根据不同的参数返回不同的对象，打印不同的信息 工厂设计模式提供接口创建对象，但是推迟对子类决定使用哪个类产生对象。定义了一个创建对象的接口，但不是工厂负责创建对象，而是将责任推迟到决定要实例化的类的子类。 from abc import ABCMeta, abstractmethod class Section(metaclass=ABCMeta): @abstractmethod def describe(self): pass class PersonalSection(Section): def describe(self): print(&quot;Personal Section&quot;) class AlbumSection(Section): def describe(self): print(&quot;Album Section&quot;) class PatentSection(Section): def describe(self): print(&quot;Patent Section&quot;) class PublicationSection(Section): def describe(self): print(&quot;Publication Section&quot;) class Profile(metaclass=ABCMeta): def __init__(self): self.sections = [] self.createProfile() @abstractmethod def createProfile(self): pass def getSections(self): return self.sections def addSections(self, section): self.sections.append(section) class linkedin(Profile): def createProfile(self): self.addSections(PersonalSection()) self.addSections(PatentSection()) self.addSections(PublicationSection()) class facebook(Profile): def createProfile(self): self.addSections(PersonalSection()) self.addSections(AlbumSection()) if __name__ == &#39;__main__&#39;: profile_type = &#39;linkedin&#39; profile = eval(profile_type)() print(&quot;Creating profile..&quot;, type(profile).__name__) print(&quot;Profile has sections --&quot;, profile.getSections()) 抽象工厂设计模式抽象工厂是创建相关对象而不指定/公开其类的接口。该模式提供另一个工厂的对象，该工厂在内部创建其他对象。抽象工厂模式的主要目标是提供一个接口来创建相关对象的族，而无需指定具体类。 from abc import ABCMeta, abstractmethod class PizzaFactory(metaclass=ABCMeta): @abstractmethod def createVegPizza(self): pass @abstractmethod def createNonVegPizza(self): pass class IndianPizzaFactory(PizzaFactory): def createVegPizza(self): return DeluxVeggiePizza() def createNonVegPizza(self): return ChickenPizza() class USPizzaFactory(PizzaFactory): def createVegPizza(self): return MexicanVegPizza() def createNonVegPizza(self): return HamPizza() class VegPizza(metaclass=ABCMeta): @abstractmethod def prepare(self, VegPizza): pass class NonVegPizza(metaclass=ABCMeta): @abstractmethod def serve(self, NonVegPizza): pass class DeluxVeggiePizza(VegPizza): def prepare(self): print(&quot;Prepare &quot;, type(self).__name__) class ChickenPizza(NonVegPizza): def serve(self, VegPizza): print(type(self).__name__, &quot; is served with Chicken on &quot;, type(VegPizza).__name__) class MexicanVegPizza(VegPizza): def prepare(self): print(&quot;Prepare &quot;, type(self).__name__) class HamPizza(NonVegPizza): def serve(self, VegPizza): print(type(self).__name__, &quot; is served with Ham on &quot;, type(VegPizza).__name__) class PizzaStore: def __init__(self): pass def makePizzas(self): for factory in [IndianPizzaFactory(), USPizzaFactory()]: self.factory = factory self.NonVegPizza = self.factory.createNonVegPizza() self.VegPizza = self.factory.createVegPizza() self.VegPizza.prepare() self.NonVegPizza.serve(self.VegPizza) pizza = PizzaStore() pizza.makePizzas() 工厂方法和抽象工厂方法的区别 在面象对象编程中，工厂意味着负责创建其它类型对象的类 工厂方法模式：这允许接口创建对象，但是推迟对子类决定使用哪个类产生对象 抽象工厂模式：抽象工厂是创建相关对象而不指定/公开其类的接口。该模式提供另一个工厂的对象，该工厂在内部创建其他对象 工厂模式的优点： 1.松散耦合，其中对象创建可以独立于类实现 2.客户端不必知道具体实现，只需要知道接口，方法和传递参数就可以了 3.可以轻松地将另一个类添加到工厂以创建另一种类型的对象,客户端不用修改代码 4.可以重用已存在的对象]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[寻找数组中字符串最长公共前缀]]></title>
    <url>%2F2018%2F09%2F12%2F%E5%AF%BB%E6%89%BE%E6%95%B0%E7%BB%84%E4%B8%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%89%8D%E7%BC%80%2F</url>
    <content type="text"><![CDATA[# 外层循环为列表中第一个元素的长度，内层循环为列表元素的个数 # 即列表的长度 def LongestCommonPrefix(lst): if not lst: return &quot;&quot; else: index = 0 for i in range(len(lst[0])): for j in range(1, len(lst)): if i &gt;= len(lst[j]) or lst[0][i] != lst[j][i]: return lst[0][:index] index += 1 return lst[0][:index] In [2]: lst = [&quot;abcdefg&quot;, &quot;abcdefghijk&quot;, &quot;abcdfghijk&quot;, &quot;abcef&quot;] In [3]: print(LongestCommonPrefix(lst)) abc]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[不排序找出前2个最大值]]></title>
    <url>%2F2018%2F09%2F11%2F%E4%B8%8D%E6%8E%92%E5%BA%8F%E6%89%BE%E5%87%BA%E5%89%8D2%E4%B8%AA%E6%9C%80%E5%A4%A7%E5%80%BC%2F</url>
    <content type="text"><![CDATA[In [90]: lst = [1,2,3,6,3,4,5,4] ...: ...: max = -9999 ...: secMax = -99999 ...: for x in lst: ...: ...: if x &gt; max: ...: max = x ...: if x &gt; secMax and x &lt; max: ...: secMax = x ...: In [91]: print(max) 6 In [92]: print(secMax) 5]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之copy,deepcopy]]></title>
    <url>%2F2018%2F09%2F11%2Fpython%E4%B9%8Bcopy%2Cdeepcopy%2F</url>
    <content type="text"><![CDATA[赋值# c与a指向同一对象 In [71]: a = [0, [1, 2], 3] In [75]: c = a In [76]: id(c) Out[76]: 140506513737928 In [77]: id(a) Out[77]: 140506513737928 浅拷贝# 只拷贝对象的引用 In [71]: a = [0, [1, 2], 3] In [72]: b = a[:] In [73]: id(a) Out[73]: 140506513737928 In [74]: id(b) Out[74]: 140506514004104 In [78]: b[1].append(3) In [79]: b Out[79]: [0, [1, 2, 3], 3] In [80]: a Out[80]: [0, [1, 2, 3], 3] 图片来源于网络 深拷贝# 复制真实的对象 In [81]: import copy In [82]: a = [0, [1, 2], 3] In [83]: b = copy.deepcopy(a) In [84]: b Out[84]: [0, [1, 2], 3] In [85]: id(b[1]) Out[85]: 140506534178056 In [86]: id(a[1]) Out[86]: 140506522688136 In [87]: b[1].append(3) In [88]: b Out[88]: [0, [1, 2, 3], 3] In [89]: a Out[89]: [0, [1, 2], 3] 图片来源于网络 浅拷贝和深拷贝的不同之处是对于可变对象来说的，因为对于不可变对象复制对象的引用和复制对象是一样的(因为当要改变值时，会重新创建个对象)。当使用浅拷贝时，对于嵌套的列表只会拷贝其引用，而深拷贝则会复制真实的对象。切片也属于浅拷贝。 Ref：1.https://my.oschina.net/leejun2005/blog/145911]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之round]]></title>
    <url>%2F2018%2F09%2F10%2Fpython%E4%B9%8Bround%2F</url>
    <content type="text"><![CDATA[round(number[, ndigits]) Return number rounded to ndigits precision after the decimal point. If ndigits is omitted or is None, it returns the nearest integer to its input. For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus ndigits; if two multiples are equally close, rounding is done toward the even choice (so, for example, both round(0.5) and round(-0.5) are 0, and round(1.5) is 2). Any integer value is valid for ndigits (positive, zero, or negative). The return value is an integer if called with one argument, otherwise of the same type as number. # 如果小数部分为0.5,```round```向最近的偶数取整 In [5]: round(10.5) Out[5]: 10 In [6]: round(11.5) Out[6]: 12 In [7]: round(10.4) Out[7]: 10 In [8]: round(10.6) Out[8]: 11 In [9]: round(-1.5) Out[9]: -2 In [10]: round(2.678, 2) Out[10]: 2.68 In [11]: round(2.676, 2) Out[11]: 2.68 In [12]: round(2.686, 2) Out[12]: 2.69 # 当末位小数为5时，如果前一位也是小数而且是奇数，则返回指定位数; # 如果前一位也是小数而且是偶数，则返回进1后的指定位数 In [13]: round(2.675, 2) Out[13]: 2.67 In [14]: round(2.685, 2) Out[14]: 2.69]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之鸭子类型(Duck typing)]]></title>
    <url>%2F2018%2F09%2F08%2Fpython%E4%B9%8B%E9%B8%AD%E5%AD%90%E7%B1%BB%E5%9E%8B(Duck%20typing)%2F</url>
    <content type="text"><![CDATA[“如果它像鸭子一样走路，它像鸭子一样呱呱叫，那它一定是鸭子” - 以确定一个物体是否可以用于特定目的。 在duck typing中，对象的适用性取决于某些方法和属性的存在，而不是对象本身的类型。我们并不关心对象是什么类型，到底是不是鸭子，只关心行为。 class Duck: def fly(self): print(&quot;Duck flying&quot;) class Airplane: def fly(self): print(&quot;Airplane flying&quot;) class Whale: def swim(self): print(&quot;Whale swimming&quot;) def lift_off(entity): entity.fly() duck = Duck() airplane = Airplane() whale = Whale() lift_off(duck) # prints `Duck flying` lift_off(airplane) # prints `Airplane flying` lift_off(whale) # Throws the error `&#39;Whale&#39; object has no attribute &#39;fly&#39; # 在lift_off函数中我们并不关心传入的参数是什么类型，只关心这个对象拥有什么方法。 比如在python中，有很多file-like的东西，比如StringIO,GzipFile,socket。它们有很多相同的方法，我们把它们当作文件使用。 再如只要一个类实现了__iter__方法，我们就说这个对象是可迭代的。 In [1]: class A: ...: def __iter__(self): ...: pass ...: In [2]: a = A() In [3]: from collections import Iterable In [4]: isinstance(a, Iterable) Out[4]: True Ref：1.wikipedia2.csdn]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之Queue]]></title>
    <url>%2F2018%2F09%2F05%2Fpython%E4%B9%8BQueue%2F</url>
    <content type="text"><![CDATA[当必须在多个线程之间安全地交换信息时，它在线程编程中特别有用。此模块中的Queue类实现了所有必需的锁定原语。 如果数据共享时可能被修改，就需要加锁来保护它，以确保同一时刻只能有一个线程访问这个数据。线程模块提供了许多同步原语，包括锁(Lock)、信号量(Semaphore)、条件变量(Condition)和事件(Event)。但最好的做法是使用Queue模块。Queue是线程安全的，使用它可以降低程序的复杂度，代码清晰，可读性更强。 # queue定义了三种类型的队列,maxsize为队列元素最大值，默认为无限大 # 定义一个先进先出的队列 class queue.Queue(maxsize=0) # 定义一个后进先出的队列 class queue.LifoQueue(maxsize=0) # 定义一个优先级队列，元素通常为(priority_number,data),值越小优先级越高 class queue.PriorityQueue(maxsize=0) 队列对象方法 Queue.qsize() qsize（）&gt; 0不保证后续的get（）不会阻塞，qsize（）&lt;maxsize也不保证put（）不会阻塞?? Queue.empty() 如果队列为空,返回True,否则返回False。 Queue.full() 如果队列已满，返回True,否则返回False，如果返回True不保证get()不会阻塞，如果返回False也不保证put()不会阻塞。 Queue.put(item, block=True, timeout=None) 将元素插入到队列中。如果block=True和timeout等于None,将会发生阻塞直到有空闲的空间。如果timeout大于0,将会阻塞timeout秒，如果还是没有多余空间将会触发Full异常。如果block=False,如果队列不是满的直接放入队列，否则触发Full异常(timeout会被忽略) Queue.put_nowait(item) 等于put(item, False). Queue.get(block=True, timeout=None) 移除和返回队列中的元素。如果block=True和timeout等于None,将会发生阻塞直到队列不为空。如果timeout大于0,将会阻塞timeout秒，如果队列还是为空将会触发Empty异常。如果block=False,如果队列不为空返回元素，否则触发Empty异常(timeout会被忽略) Queue.get_nowait() 等于get(False) Queue.task_done() 表时以前排队的任务已经完成。用于队列消费者线程。对于用于获取任务的每个get()，对task_done()的后续调用会告知队列任务的处理已完成。 Queue.join() 阻塞直到队列中所有元素都被处理了。 import Queue import threading def print_url(queue): while True: url = queue.get() print(url) queue.task_done() def use_queue(): queue = Queue.Queue() for i in range(5): t = threading.Thread(target=print_url,args=(queue,)) t.setDaemon(True) t.start() for url in [&#39;baidu.com&#39;,&#39;xina.com&#39;,&#39;weibo.com&#39;,&#39;qq.com&#39;,&#39;hysky.com&#39;,&#39;hehe.com&#39;]: queue.put(url) queue.join() if __name__ == &quot;__main__&quot;: use_queue() # 输出： baidu.com weibo.com xina.com qq.com hysky.com hehe.com]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之在匿名函数中绑定变量的值]]></title>
    <url>%2F2018%2F09%2F05%2Fpython%E4%B9%8B%E5%9C%A8%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0%E4%B8%AD%E7%BB%91%E5%AE%9A%E5%8F%98%E9%87%8F%E7%9A%84%E5%80%BC%2F</url>
    <content type="text"><![CDATA[In [17]: [x(2) for x in mu()] Out[17]: [0, 2, 4, 6] In [18]: x = 10 In [19]: a = lambda y: x + y In [20]: x = 20 In [21]: b = lambda y: x + y In [22]: a(10) Out[22]: 30 In [23]: b(10) Out[23]: 30 # lambda表达式中用到的x是一个自由变量，在运行进才进行绑定而不是定义的时候进行绑定。因此，lambda表达式中的x是在执行的时候确定的。 再来看这么一个例子： In [27]: def multipliers(): ...: return [lambda x: i*x for i in range(4)] In [28 print([m(2) for m in multipliers()]) [6, 6, 6, 6] # 因为i值是在运行的时候确定的，当调用lambda函数时，for循环已经结束，所以i都是3 # 可以把添加一个默认参数绑定变量，默认值只会函数定义的时候初始化一次 In [33]: def multipliers(): ...: return [lambda x, i=i: i*x for i in range(4)] ...: ...: In [34]: print([m(2) for m in multipliers()]) [0, 2, 4, 6] # 或者可以通过如下方式实现 In [16]: from functools import partial In [17]: from operator import mul In [35]: def multipliers(): ...: return [partial(mul, i) for i in range(4)] ...: ...: In [36]: print([m(2) for m in multipliers()]) [0, 2, 4, 6] Ref：1.python cookbook]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[找出距离自己最近的指定元素]]></title>
    <url>%2F2018%2F09%2F05%2F%E6%89%BE%E5%87%BA%E8%B7%9D%E7%A6%BB%E8%87%AA%E5%B7%B1%E6%9C%80%E8%BF%91%E7%9A%84%E6%8C%87%E5%AE%9A%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[# 找出距离H最近的G # input lst = [&#39;H&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;] # output [2, 1, 0, 1, 0, 1, 1, 0, 1] # 第一个&quot;H&quot;最近的&quot;G&quot;相差距离为2 lst = [&#39;H&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;, &#39;H&#39;, &#39;G&#39;, &#39;H&#39;] def find_min_distance(lst): lst = &#39;&#39;.join(lst) ret = [] n = len(lst) for i in range(n): if lst[i] == &#39;G&#39;: ret.append(0) continue else: index1 = lst.find(&#39;G&#39;, i) index2 = lst[::-1].find(&#39;G&#39;, n-i) if index1 and index2: if abs(index1 - i) &lt; abs(index2-n+i+1): ret.append(abs(index1-i)) else: ret.append(abs(index2-n+i+1)) elif index1: ret.append(abs(index1-i)) elif index2: ret.append(abs(index2-n+i)) return ret print(find_min_distance(lst))]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之属性继承]]></title>
    <url>%2F2018%2F09%2F03%2Fpython%E4%B9%8B%E5%B1%9E%E6%80%A7%E7%BB%A7%E6%89%BF%2F</url>
    <content type="text"><![CDATA[class A: a = 1 a, b, c = A(), A(),A() print(a.a, b.a, c.a) a.a = 2 A.a = 3 print(a.a, A.a, c.a) # 未对实例c.a赋值，则会继承类属性a的值 print(a.__dict__, A.__dict__, c.__dict__) # output: (1, 1, 1) (2, 3, 3) ({&#39;a&#39;: 2}, {&#39;a&#39;: 3, &#39;__module__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: None}, {}) # 实例c的__dict__为空]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之不要在函数中定义可变对象作为默认值]]></title>
    <url>%2F2018%2F09%2F03%2Fpython%E4%B9%8B%E4%B8%8D%E8%A6%81%E5%9C%A8%E5%87%BD%E6%95%B0%E4%B8%AD%E5%AE%9A%E4%B9%89%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1%E4%BD%9C%E4%B8%BA%E9%BB%98%E8%AE%A4%E5%80%BC%2F</url>
    <content type="text"><![CDATA[def f(a=[]): a.append(len(a)+1) print(a) print(id(a)) f() f() # output: [1] 140542070277688 [1, 2] 140542070277688 # 参数的默认值只会函数定义的时候初始化一次。 # 函数中的默认值为一个可变对象，对它的操作就和定位了指针地址一样,在内存里进行修改。而如果是不可变对象则会复制一份新的对象。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之dict与set]]></title>
    <url>%2F2018%2F09%2F01%2Fpython%E4%B9%8Bdict%E4%B8%8Eset%2F</url>
    <content type="text"><![CDATA[In [5]: def f(a,*args,**kwargs): print(a) print(args) print(kwargs) In [6]: keys =[1, 2, 3] In [7]: kwargs = {&#39;a&#39;,&#39;b&#39;} In [8]: f(&#39;b&#39;,*keys,**kwargs) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-8-5ffd5def4f0f&gt; in &lt;module&gt;() ----&gt; 1 f(&#39;b&#39;,*keys,**kwargs) TypeError: f() argument after ** must be a mapping, not set # 这里报错，这里传入的参数必须是字典，而不能是集合，当定义一个字典时，若只有键没有值，那这是一个集合，而不是字典，这应该看作两种完全不一样的数据 In [9]: k = {&#39;a&#39;:&#39;b&#39;,&#39;c&#39;:&#39;d&#39;} In [10]: f(&#39;b&#39;,*keys,**k) # *表示可选参数列表，**表示可选参数字典 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-10-8147c79d6830&gt; in &lt;module&gt;() ----&gt; 1 f(&#39;b&#39;,*keys,**k) TypeError: f() got multiple values for argument &#39;a&#39; # 若k中有把&#39;a&#39;作为键的键值对，则会报错 In [13]: f(&#39;a&#39;,*keys,{&#39;b&#39;:&#39;c&#39;}) # 没加**会把{&#39;a&#39;:&#39;c&#39;}当作可参数列表中的一部分 a (1, 2, 3, {&#39;b&#39;: &#39;c&#39;}) {} In [14]: f(&#39;a&#39;,*keys,**{&#39;b&#39;:&#39;c&#39;}) a (1, 2, 3) {&#39;b&#39;: &#39;c&#39;}]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python之else]]></title>
    <url>%2F2018%2F08%2F24%2Fpython%E4%B9%8Belse%2F</url>
    <content type="text"><![CDATA[In [1]: def print_prime(n): ...: for i in range(2, n): ...: for j in range(2, i): ...: if i % j == 0: ...: break ...: else: ...: print(&quot;%d is a prime number&quot; %i) ...: In [2]: print_prime(10) 2 is a prime number 3 is a prime number 5 is a prime number 7 is a prime number 当循环自然结束时(循环条件为假)时else从句会被执行一次，而当循环是由break语句中断时，else子句不会被执行。与for语句相似，while语句中的else子句的语意是一样的：else块在循环正常结束和循环条件不成立时被执行。 异常处理中的elsetry块没有抛出任何异常时，执行else块 def save(db, obj): try: db.execute(&quot;a sql&quot;, obj.attr1) except DBErrror: db.rollback() else: db.commit() # 未发生异常时执行 Ref：1.编写高质量代码-改善Python程序的91个建议]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hyperledger Fabric编译和安装]]></title>
    <url>%2F2018%2F08%2F22%2FHyperledger%20Fabric%E7%BC%96%E8%AF%91%E5%92%8C%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境：ubuntu 16.04 64位1.依次安装go 1.9.x,docker,docker-compose 2. mkdir -p $GOPATH/src/github.com/hyperledger cd $GOPATH/src/github.com/hyperledger git clone http://gerrit.hyperledger.org/r/fabric cd fabric git checkout release-1.0 # release-1.0才支持go 1.9.x 3.安装依赖软件 go get github.com/golang/protobuf/protoc-gen-go mkdir -p $GOPATH/src/github.com/hyperledger/fabric/build/docker/gotools/bin # go get之后编译好的文件会放到$GOBIN对应的目录中，如果没有设置$GOBIN的值，生成的文件将默认存放到$GOPATH/bin下面 cp protoc-gen-go $GOPATH/src/github.com/hyperledger/fabric/build/docker/gotools/bin 4.编译 cd $GOPATH/src/github.com/hyperledger/fabric make release make docker //生成docker镜像文件 5.编译成功后会在$GOPATH/src/github.com/hyperledger/fabric/release/linux-amd64/bin生成如下可执行文件。 6.make docker 经过漫长的等待后 mysql镜像除外 遇到的问题： 1.执行make release时报如下错误 gotools.mk:22: *** target pattern contains no &#39;%&#39;. Stop 原因是我的$GOPATH设置了多个路径，改成一个路径之后错误消失。 export GOPATH=&quot;/home/hys/mycode/go:/home/hys/mycode/go/gopl:/home/hys/mycode/go/block&quot; ==&gt; export GOPATH=&quot;/home/hys/mycode/go/block&quot; 2.golang.org/x不能访问的问题 解决方法]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[go get golang.org/x 包失败解决方法]]></title>
    <url>%2F2018%2F08%2F22%2Fgo-get-golang-orgx%2F</url>
    <content type="text"><![CDATA[由于网络问题不能访问golang.org,通过go get安装golang官方包会失败，如 go get lint package golang.org/x/lint: unrecognized import path &quot;golang.org/x/lint&quot; (https fetch: Get https://golang.org/x/lint?go-get=1: dial tcp 216.239.37.1:443: i/o timeout) 不翻墙的情况下怎么解决这个问题？其实 golang 在 github 上建立了一个镜像库，如 https://github.com/golang/lint 即是 https://golang.org/x/lint 的镜像库 解决方法 mkdir -p $GOPATH/src/golang.org/x cd $GOPATH/src/golang.org/x git clone https://github.com/golang/lint.git go install golang.org/x/lint Ref：1.https://blog.csdn.net/alexwoo0501/article/details/73409917]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python面试之名称修饰(name-mangling)]]></title>
    <url>%2F2018%2F08%2F21%2Fpython%E9%9D%A2%E8%AF%95%E4%B9%8B%E5%90%8D%E7%A7%B0%E4%BF%AE%E9%A5%B0(name-mangling)%2F</url>
    <content type="text"><![CDATA[&gt;&gt;&gt; class MyClass(): ... def __init__(self): ... self.__superprivate = &quot;Hello&quot; ... self._semiprivate = &quot;, world!&quot; ... &gt;&gt;&gt; mc = MyClass() &gt;&gt;&gt; print mc.__superprivate Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; AttributeError: myClass instance has no attribute &#39;__superprivate&#39; &gt;&gt;&gt; print mc._semiprivate , world! &gt;&gt;&gt; print mc.__dict__ {&#39;_MyClass__superprivate&#39;: &#39;Hello&#39;, &#39;_semiprivate&#39;: &#39;, world!&#39;} foo :一种约定,Python内部的名字,用来区别其他用户自定义的命名,以防冲突. _foo :一种约定,用来指定变量私有.程序员用来指定私有变量的一种方式. __foo :这个有真正的意义:解析器用 _classname__foo 来代替这个名字,以区别和其他类相同的命名. In [1]: class A: ...: __p = 1 ...: In [2]: A.__p --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-2-f532f70a0839&gt; in &lt;module&gt;() ----&gt; 1 A.__p AttributeError: type object &#39;A&#39; has no attribute &#39;__p&#39; In [6]: A._A__p Out[6]: 1 In [7]: print(A.__dict__) {&#39;__module__&#39;: &#39;__main__&#39;, &#39;_A__p&#39;: 1, &#39;__dict__&#39;: &lt;attribute &#39;__dict__&#39; of &#39;A&#39; objects&gt;, &#39;__weakref__&#39;: &lt;attribute &#39;__weakref__&#39; of &#39;A&#39; objects&gt;, &#39;__doc__&#39;: None} # 在python中名称修饰用于私有成员，当一个成员名称以双下划线开头和不超过一个下划线结尾， # 它会在运行时被解释器重命名，以避免与任何子类中的方法产生命名冲突。重命名规则为&quot;_类名+变量&quot;,如_A__p。 Ref： 1.https://en.wikipedia.org/wiki/Name_mangling#Python]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python面试之类变量访问顺序]]></title>
    <url>%2F2018%2F08%2F20%2Fpython%E9%9D%A2%E8%AF%95%E4%B9%8B%E7%B1%BB%E5%8F%98%E9%87%8F%E8%AE%BF%E9%97%AE%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[class A: a = 1 a, b, c = A(), A(),A() print(a.a, b.a, c.a) a.a = 2 A.a = 3 print(a.a, A.a, c.a) print(a.__dict__, A.__dict__, c.__dict__) # 输出 (1, 1, 1) #实例对象字典为空，从类的字典寻找变量，所以输出都为1 (2, 3, 3) #实例a有了自己的字典，c没有自己的字典，从类的字典寻找变量，所以为3 ({&#39;a&#39;: 2}, {&#39;a&#39;: 3, &#39;__module__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: None}, {})]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python面试之装饰器]]></title>
    <url>%2F2018%2F08%2F20%2Fpython%E9%9D%A2%E8%AF%95%E4%B9%8B%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[import functools def a(f): print(&#39;a&#39;) @functools.wraps(f) def wraps(*args, **kwargs): print(&#39;wraps&#39;) return f(*args, **kwargs) return wraps @a def b(): print(&#39;b&#39;) b() b() 第二次调用b()函数，并没有输出’a’,其实调用b(),b()相当于 import functools def a(f): print(&#39;a&#39;) @functools.wraps(f) def wraps(*args, **kwargs): print(&#39;wraps&#39;) return f(*args, **kwargs) return wraps def b(): print(&#39;b&#39;) b = a(b) //此时打印&#39;a&#39; b() b() # 当导入b的时候装饰器已经运行，而不是自己原以为的调用b时装饰器才运行 In [1]: from t1 import b a update_wrapper是wraps的主要功能提供者，它负责考贝原函数的属性，默认是：’module‘, ‘name‘, ‘doc‘， ‘dict‘ functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES) # 通过wraps和update_wrapper实现结果是一致的 In [28]: from functools import wraps, update_wrapper In [29]: def time1(func): ...: @wraps(func) ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: func(*args, **kwargs) ...: end = time.time() ...: print(func.__name__, end-start) ...: return wrapper ...: In [30]: def time2(func): ...: ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: func(*args, **kwargs) ...: end = time.time() ...: print(func.__name__, end-start) ...: return update_wrapper(wrapper,func) ...: ...: In [31]: @time1 ...: def fori(): ...: for i in range(1000): ...: pass ...: In [32]: fori()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[你真的了解python tuple不可变吗]]></title>
    <url>%2F2018%2F08%2F20%2Fpython%E9%9D%A2%E8%AF%95%E4%B9%8Btuple%E4%B8%8D%E5%8F%AF%E5%8F%98%2F</url>
    <content type="text"><![CDATA[最近遇到个有关tuple的面试题，挺有意思的。 # 下面会输出什么？为什么 In [18]: tu = (1, [1, 2]) In [19]: tu[1].append(3) In [20]: print(tu) (1, [1, 2, 3]) tupple不可变是指对象不可变，不可以对tuple中的元素进行赋值操作。它所包含的元素的可变性取决于该元素的属性。12345678910111213141516171819202122232425262728293031323334 In [22]: tu[1] = [1,2,3,4] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-22-1e2059dd0201&gt; in &lt;module&gt;() ----&gt; 1 tu[1] = [1,2,3,4] TypeError: &apos;tuple&apos; object does not support item assignment In [23]: print(tu) #报错，赋值操作未成功 (1, [1, 2, 3]) In [24]: tu[1] += [1,2,3,4] ---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-6-e1dc81ae6e45&gt; in &lt;module&gt;----&gt; 1 tu[1] += [1,2,3,4]TypeError: &apos;tuple&apos; object does not support item assignmentIn [25]: tu # 报错,虽然报错,但操作成功了 Out[26]: (1, [1, 2, 3, 1, 2, 3, 4]) In [27]: class A: ...: a = 1 ...: In [28]: a = A() In [29]: t = (1, a) In [30]: t[1].a = 2 In [31]: print(t[1].a) 2]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go接口]]></title>
    <url>%2F2018%2F08%2F18%2FGo%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[接口类型是对其它类型行为的抽象和概括；因为接口类型不会和特定的实现细节绑定在一起，通过这种抽象的方式我们可以让我们的函数更加灵活和更具有适应能力。 接口类型具体描述了一系列方法的集合，一个实现了这些方法的具体类型是这个接口类型的实例。 一个类型如果拥有一个接口需要的所有方法，那么这个类型就实现了这个接口。 /* Abs()是interface类型Abser定义的方法，而MyFloat实现了该方法，所以，MyFloat实现了Abser接口 package main import ( &quot;fmt&quot; &quot;math&quot; ) type Abser interface { Abs() float64 } type MyFloat float64 func (f MyFloat) Abs() float64 { if f &lt; 0 { return float64(-f) } return float64(f) } func main() { var a Abser f := MyFloat(-math.Sqrt2) a = f // a MyFloat implements Abser fmt.Println(a.Abs()) interface{}被称为空接口，因为空接口类型对实现它的类型没有要求，所以我们可以将任意一个值赋给空接口类型。 var any interface{} any = true any = 12.34 any = &quot;hello&quot; any = map[string]int{&quot;one&quot;: 1} any = new(bytes.Buffer) 格式化输出 // 实现了String方法的类型，称作实现了Stringer接口。Stringer接口为此类型提供了原始的输出格式 type Stringer interface { String() string } package main import &quot;fmt&quot; type IPAddr [4]byte func main() { addrs := map[string]IPAddr{ &quot;loopback&quot;: {127, 0, 0, 1}, &quot;googleDNS&quot;: {8, 8, 8, 8}, } for n, a := range addrs { fmt.Printf(&quot;%v: %v\n&quot;, n, a) } } 为类型IPAddr实现Stringer接口 package main import &quot;fmt&quot; type IPAddr [4]byte func (ip IPAddr) String() string { return fmt.Sprintf(&quot;%v.%v.%v.%v&quot;, ip[0], ip[1], ip[2], ip[3]) } func main() { addrs := map[string]IPAddr{ &quot;loopback&quot;: {127, 0, 0, 1}, &quot;googleDNS&quot;: {8, 8, 8, 8}, } for n, a := range addrs { fmt.Printf(&quot;%v: %v\n&quot;, n, a) } } Ref：1.The Go Programming Language2.https://studygolang.com/articles/2652]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go方法]]></title>
    <url>%2F2018%2F08%2F18%2FGo%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在函数声明时，在其名字之前放上一个变量，即是一个方法。这个附加的参数会将函数附加这种类型上，即相当于这这种类型定义了一个独占的方法。 /* 这个附加的参数p,叫做方法的接收器(receiver),早期的面向对象语言留下的遗产将调用一个方法称为&quot;向一个对象发送消息&quot; 在Go语言中，我们并不会像其它语言那样用this或者self作为接收器；我们可以任意的选择接收器的名字。由于接收器的名字经常会被使用到，所以保持其在方法间传递时的一致性和简短性是不错的主意。建议可以使用其类型的第一个字母，比如这里使用了Point的首字母p*/ type Point struct{ X, Y float64 } func (p Point) Distance(q Point) float64 { return math.Hypot(q.X-p.X, q.Y-p.Y) } 每种类型都有其方法的命名空间，我们在用Distance这个名字的时候，不同的Distance调用指向了不同类型里的Distance方法。 基于指针对象的方法 当调用一个函数时，会对其每一个参数值进行拷贝，如果一个函数需要更新一个变量，或者函数的其中一个参数实在太大我们希望能够进行这种默认的拷贝，这种情况我们可以使用指针。 // 这个方法的名字是(*Point).ScaleBy func (p *Point) ScaleBy(factor float64) { p.X *= factor p.Y *= factor } p := Point{1, 2} pptr := &amp;p pptr.Distance(q) (*pptr).Distance(q) // 编译器在这里也会给我们隐式地插入*这个操作符，所以上面这两种写法等价 不管你的method的receiver是指针类型还是非指针类型，都是可以通过指针/非指针类型进行调用的，编译器会帮你做类型转换。 在声明一个method的receiver该是指针还是非指针类型时，你需要考虑两方面的内部，第一方面是这个对象本身是不是特别大，如果声明为非指针变量时，调用会产生一次拷贝；第二方面是如果你用指针类型作为receiver，那么你一定要注意，这种指针类型指向的始终是一块内存地址，就算你对其进行了拷贝。 通过嵌入结构体来扩展类型 package main import ( &quot;fmt&quot; &quot;image/color&quot; ) type Point struct{X, Y float64} type ColoredPoint struct { Point Color color.RGBA } func (p Point)Distance() float64{ return p.X } func main() { red := color.RGBA{255, 0, 0, 255} var p = ColoredPoint{Point{1, 1}, red} fmt.Println(p.Distance()) } /* p这里可以直接调用p.Distance()方法。这里可以将Point看作一个基类，ColoredPoint是其子类或继承类。 */ 封装 一个对象的变量或者方法如果对调用方不可见的话，一般就被定义为”封闭”。封装有时候也被叫信息隐藏，同时也是面向对象编程最关键的一个方面。 Go语言只有一种控制可见性的手段：大写首字母的标识符会从定义它们的包中被导出，小写字母的则不会。这种限制包内成员的方式同样适用于struct或者一个类型的方法。因而如果我们想要封装一个对象，我们必须将其定义为一个struct。 Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go函数]]></title>
    <url>%2F2018%2F08%2F18%2FGo%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[函数声明包括函数名、形式参数列表、返回值列表(可省略)以及函数体。 func name(parameter-list) (result-list) { body } 实参通过值的方式传递，因此函数的形参是实参的拷贝。对形参进行修改不会影响实参。但是，如果实参包括引用类型，如指针，slice，map，function， channel等类型，实参可能会由于函数的间接引用被修改。 递归:Go语言使用可变栈，栈的大小按需增加(初始时很小)。这使得我们使用递归时不必考虑溢出和安全问题。 虽然Go的垃圾回收机制会回收不被使用的内存，但不包括操作系统层面的资源，比如打开的文件、网络连接。 log包中的所有函数会为没有换行符的字符串增加换行符。 fmt.Errorf函数使用fmt.Sprintf格式化错误信息并返回。 匿名函数： func squares() func() int { var x int return func() int { x++ return x * x } } 可变参数: func sum(vals...int) int { total := 0 for _, val := range vals { total += val } return total } defer语句经常用于处理成对的操作，如打开、关闭、连接、断开连接、加锁、释放锁。通过defer机制，不论函数逻辑多复杂，都能保证在执行路径下，资源被释放。释放资源的defer应该直接跟在请求资源的语句后。 package ioutil func ReadFile(filename string) ([]byte, error) { f, err := os.Open(filename) if err != nil { return nil, err } defer f.Close() return ReadAll(f) } var mu sync.Mutex var m = make(map[string]int) func lookup(key string) int { mu.Lock() defer mu.Unlock() return m[key] } Panic异常 Go的类型系统会在编译时捕获很多错误，但有些错误只能在运行时检查，如数组访问越界、 空指针引用等。这些运行时错误会引起painc异常。 一般而言，当panic异常发生时，程序会中断运行，并立即执行在该goroutine（可以先理解成线程)中被延迟的函数（defer 机制）。随后，程序崩溃并输出日志信息。日志信息包括panic value和函数调用的堆栈跟踪信息。panic value通常是某种错误信息。 Recover捕获异常 如果在deferred函数中调用了内置函数recover，并且定义该defer语句的函数发生了panic异 常，recover会使程序从panic中恢复，并返回panic value。导致panic异常的函数不会继续运 行，但能正常返回。在未发生panic时调用recover，recover会返回nil。 func Parse(input string) (s *Syntax, err error) { defer func() { if p := recover(); p != nil { err = fmt.Errorf(&quot;internal error: %v&quot;, p) } }() // ...parser... } Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go复合数据类型]]></title>
    <url>%2F2018%2F08%2F15%2FGo%E5%A4%8D%E5%90%88%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Go复合数据类型包括：数组，切片，字典，结构体。 数组： var a [3]int q := [...]int{1, 2, 3} //数组长度根据初始化元素数量决定 for i, v := range a { fmt.Printf(&quot;%d %d\n&quot;, i, v) //打印索引、值 } //数组的大小也是其类型的一部分，[3]int和[4]int是不同的类型。 r :=[...]int{99:-1} //定义了一个大小为100，最后一个元素初始为-1,其余为0 c1 := sha256.Sum256([]byte(&quot;x&quot;)) // 切片 // 切片共享底层数组数据 months := [...]string{1: &quot;January&quot;, 2: &quot;February&quot;, 3: &quot;March&quot;, 4: &quot;April&quot;, 5: &quot;June&quot;} Q1 := months[0:3] Q2 := months[2:3] fmt.Println(Q1[2], &amp;Q1[2]) //February 0xc042044080 fmt.Println(Q2[0], &amp;Q2[0]) //February 0xc042044080 Q2[0] = &quot;Hello&quot; // 修改Q2[0]后，Q1中的数据也发生改变 fmt.Println(Q1, Q2) //[ January Hello] [Hello] //切片的创建 var array=[5]int{1, 2, 3, 4, 5} slice := array[:4] //基于底层数组创建 var slice = []int{1, 2, 3, 4, 5} //直接创建 var slice = make([]int 5, 10) //使用make函数创建有5个元素的切片，cap为10 字典： ages := map[string]int{ &quot;alice&quot;: 31, &quot;joke&quot;: 33, } //或者 ages := make(map[string]int) ages[&quot;alice&quot;] = 31 ages[&quot;joke&quot;] = 33 age, ok := ages[&quot;bob&quot;] if !ok {/* &quot;bob&quot; is not in this map&quot;*/} //也可以写成如下形式 if age, ok := ages[&quot;bob&quot;]; !ok {/*...*/} //不能对字典元素进行取地址的操作，因为字典元素的地址是会发生变化的，当装载因子超过一定数值后，会对字典进行rehash操作。 结构体 type Point struct { X, Y int } pp := &amp;Point{1, 2} pp := new(Point) *pp = Point{1, 2} //等价于上种方式 pp.X //1 pp.Y //2 结构体嵌套 type Point struct { X, Y int } type Circle struct { Center Point Radius int } type Wheel struct { Circle Circle Spokes int } var w Wheel w.Circle.Center.X = 8 //这样访问显得过繁琐 //结构体中的匿名字段 type Circle struct { Point Radius int } type Wheel struct { Circle Spokes int } // 这样我们就可以省略掉中间的名称直接访问w.X //两种初始化方法 var w Wheel = Wheel{Circle{Point{8, 8}, 5}, 20} var w Wheel = Wheel { Circle: Circle { Point: Point{X: 8, Y: 8}, Radius: 5, }, Spokes: 20, } JSON // 只有导出的结构体成员才会被编码，这也就是我们为什么选择用大写字 母开头的成员名称。 type Movie struct { Title string Year int `json:&quot;released&quot;` //输出json格式时指定的别名 Color bool `json:&quot;color, omitempty&quot;` //omitempty表示结构体成员为空时不生成JSON对象 Actors []string } 文本和HTML模版 const templ = `{ {.TotalCount}} issues: { {range .Items}}---------------------------------------- Number: { {.Number}} User: { {.User.Login}} Title: { {.Title | printf &quot;%.64s&quot;}} Age: { {.CreatedAt | daysAgo}} days { {end}}` func daysAgo(t time.Time) int { return int(time.Since(t).Hours() / 24) } // 创建一个模版，指定过滤函数，调用Parse函数分析模板 report, err := template.New(&quot;report&quot;). Funcs(template.FuncMap{&quot;daysAgo&quot;: daysAgo}). Parse(templ) //执行模版，result为传入的参数 report.Execute(os.Stdout, result) Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go基础数据类型]]></title>
    <url>%2F2018%2F08%2F14%2FGo%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Go将数据类型分为四类: 基础类型、复合类型、引用类型和接口类型。 基础类型包括：数字、字符串、布尔型。 整型 整型分为有符号和无符号类型的整数。 int8 int16 int32 int64 //分别对应8、16、32、64位大小的有符号整型数 uint8 uint16 uint32 uint64 //分别对应8、16、32、64位大小的无符号整型数 这些类型的变量不允许互相赋值或操作。如以下代码会产生错误:123var a int8var b int32c := a+b Unicode字符rune类型是和int32等价的类型，通常用于表示一个Unicode码点。这两个名称可 以互换使用。同样byte也是uint8类型的等价类型，byte类型一般用于强调数值是一个原始的数 据而不是一个小的整数。 // Go中运算符优先级 * / % &lt;&lt; &gt;&gt; &amp; &amp;^ + - | ^ == != &lt; &lt;= &gt; &gt;= &amp;&amp; || 浮点数 float32 float64 复数 complex64 //对应float32 complex128 //对应float64 布尔型 true false 字符串 原生字符串使用反引号代替双引号。在原生字符串面值中，不会进行转义操作。 const t = `Go is best language` 一个字符串是包含的只读字节数组，一旦创建，是不可变的。相比之下，一个字节slice的元 素则可以自由地修改。 // 字符串和字节slice之间可以相互转换 s := &quot;abc&quot; b := []byte(s) s2 := string(b) strconv包提供了布尔型、整型数、浮点数和对应字符串的相互转换，还提供了双引号转义相关的转换。 常量 常量表达式的值在编译期计算，而不是在运行期。 所有常量的运算都可以在编译期完成，这样可以减少运行时的工作，也方便其他编译优化。 当操作数是常量时，一些运行时的错误也可以在编译时被发现，例如整数除零、字符串索引 越界、任何导致无效浮点数的操作等 注意默认类型是规则的：无类型的整数常量默认转换为int，对应不确定的内存大小，但是浮 点数和复数常量则默认转换为float64和complex128。 const ( a = 1 b c = 2 d ) fmt.Println(a, b, c, d) //1, 1, 2, 2 const ( a uint = 1 &lt;&lt; iota b c d ) fmt.Println(a, b, c, d) //1, 2, 4, 8 123i := 1在函数中，简洁赋值语句 := 可在类型明确的地方代替 var 声明。函数外的每个语句都必须以关键字开始（var, func 等等），因此 := 结构不能在函数外使用。 Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go程序结构]]></title>
    <url>%2F2018%2F08%2F13%2FGo%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Go 25个关键字 break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var 内置常量，类型，函数 true false iota nil int int8 int16 int32 int64 unit uint8 unit16 unit32 uint64 unitptr float32 float64 complex128 complex64 bool byte rune string error make len cap new append copy close delete complex real imag panic recover 如果一个名称是以大写字母开头的，表示此名称对于其它的package是可见和可获取的。 // 常量a是package级别的声明，f是一个局部变量，package级别的实体是整个package可见的，不止在声明它的文件中。 package main const a = 123; func main() { var f = a } 变量 var name type = expression var s string //定义了一个名为s,类型为string的变量 var i, j, k int // int, int, int i := 100 i, j: = 0, 1 i, j = j, i //交换变量 指针 指针的&quot;零值&quot;是nil x := 1 p := &amp;x //通过&amp;取变量x的地址 函数返回一个局部变量的地址是安全，当函数返回后这个局部依然存在(在C语言中则会发生错误(段错误)) func f() *int { v := 1 return &amp;v } func main() { fmt.Println(*f()) //1 } //尽管x是局部变量，当函数返回后x还是可以被指针global访问，所以x必须分配在堆上，这种情况我们称为x逃离了f var global *int func f() { var x int x = 1 global = &amp;x } 赋值 var s string = &quot;hello&quot; s := &quot;hello&quot; x, y := 0, 1 m := make(map[string]int) v, ok = m[key] //返回两个值 medals := []string{&quot;hello&quot;, &quot;world&quot;} //定义一个map并初始化 m3 := map[string]string{ &quot;a&quot;: &quot;aa&quot;, &quot;b&quot;: &quot;bb&quot;, } 类型声明 type name underlying-type type new_int int //给int定义一个别名new_int 作用域 if f, err := os.Open(fname); err != nil { // compile error: unused: f return err } f.ReadByte() // compile error: undefined f f.Close() // compile error: undefined f //变量f的作用域只有在if语句内，因此后面的语句将无法引入它，这将导致编译错误 包的初始化 包的初始化首先是解决包级变量的依赖顺序，然后安照包级变量声明出现的顺序依次初始 化： var a = b + c // a 第三个初始化, 为 3 var b = f() // b 第二个初始化, 为 2, 通过调用 f (依赖c) var c = 1 // c 第一个初始化, 为 1 func f() int { return c + 1 } // init初始化函数除了不能被调用或引用外，其也行为和普通函数类似。在每个文件中的init初始化函数，在程序开始执行时按照它们声明的顺序被自动调用。 func init() { } 相关代码 Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go概览]]></title>
    <url>%2F2018%2F08%2F12%2Fgo%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[Go是编译型语言。 如果缺少import或者导入之后没有使用，程序都不会进行编译。 Go每行结尾并不需要加分号(加分号也不会报错，但不符合规范) Go提供了gofmt命令来处理编码规范问题。 $ gofmt --help usage: gofmt [flags] [path ...] -cpuprofile string write cpu profile to this file -d display diffs instead of rewriting files -e report all errors (not just the first 10 on different lines) -l list files whose formatting differs from gofmt&#39;s -r string rewrite rule (e.g., &#39;a[b:len(a)] -&gt; a[b:]&#39;) -s simplify code -w write result to (source) file instead of stdout os.Args //命令行输入，os.Args[0]表示命令本身 for initialization; condition; post { //zero or more statements } // 相当于while for condition { } // 相当于无限循环 for { } range //返回当前索引的索引和值 ch := make(chan string) //初始化字符串类型的通道 &lt;-ch //从通道中取值 ch &lt;- //往通道中写入值 strings.Join(&lt;list&gt;, sep) // 功能和python中join一样 input := bufio.NewScanner(os.Stdin) //读取标准输入的数据，并分解为行或单词 input.Scan() //读取下一行，并去掉换行符 input.Text() //获取输入内容 %d 十进制整数 %x, %o, %b 十六进制，八进制，二进制整数。 %f, %g, %e 浮点数： 3.141593 3.141592653589793 3.141593e+00 %t 布尔：true或false %c 字符（rune） (Unicode码点) %s 字符串 %q 带双引号的字符串&quot;abc&quot;或带单引号的字符&#39;c&#39; %v 变量的自然形式（natural format） %T 变量的类型 %% 字面上的百分号标志（无操作数） fmt.Sprintf //返回格式化后的字符串 fmt.Println //打印到标准输出，返回字节数和err // 格式化字符串并写入到w中，返回写入的字节数和err func Fprintf(w io.Writer, format string, a ...interface{}) (n int, err error) // 返回一个可读文件对象 func Open(name string) (*File, error) //os.Open func Exit(code int) //os.Open,结束程序，0表示成功 func ReadFile(filename string) ([]byte, error) //ioutil.ReadFile,读取整个文件并返回内容 func ReadAll(r io.Reader) ([]byte, error) //ioutil.ReadAll,从r中读取直到EOF或产生错误 func Join(a []string, sep string) string //strings.Join func Split(s, sep string) []string //strings.Split func (c *Client) Get(url string) (resp *Response, err error)//http.Get func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) //http.HandleFunc,根据pattern注册处理函数 func Fatal(v ...interface{}) //log.Fatal,等同于Print()+os.Exit(1) func Now() Time //time.Now,返回本地当前时间 func Since(t Time) Duration //time.Since //sync.Mutex func (m *Mutex) Lock() func (m *Mutex) Unlock() //io func Copy(dst Writer, src Reader) (written int64, err error) //copy src to dst, ioutil.Discard相当于/dev/null 相关代码 Ref： 1.The Go Programming Language]]></content>
      <categories>
        <category>GO</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis之字典实现]]></title>
    <url>%2F2018%2F08%2F11%2Fredis%E4%B9%8B%E5%AD%97%E5%85%B8%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[哈希表节点使用dictEntry结构表示，每个dictEntry结构都保存着一个键值对： /* key属性保存着键值对中的键，而v属性则保存着键值对中的值，其中键值对的值可以是一个指针，或者是一个uint64_t整数，又或者是一个int64_t整数 */ typedef struct dictEntry { void *key; union { void *val; uint64_t u64; int64_t s64; } v; struct dictEntry *next; } dictEntry; typedef struct dictht { // 哈希表数组 dictEntry **table; // 哈希表大小 unsigned long size; // 哈希表大小掩码，用于计算索引值 // 总是等于 size - 1 unsigned long sizemask; // 该哈希表已有节点的数量 unsigned long used; } dictht; 图片来源 typedef struct dict { // 类型特定函数 dictType *type; // 私有数据 void *privdata; // 哈希表 dictht ht[2]; // rehash 索引 // 当 rehash 不在进行时，值为 -1 int rehashidx; /* rehashing not in progress if rehashidx == -1 */ // 目前正在运行的安全迭代器的数量 int iterators; /* number of iterators currently running */ } dict; type属性是一个指向dictType结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数，Redis会为用途不同的字典设置不同的类型特定函数。 而privdata属性则保存了需要传给那些类型特定函数的哥选参数。 图片来源 当要将一个新的键值对添加到字典里面时，程度需要先根据键值对的键计算出哈希值和索引值，然后再根据索引值，将包含新键值对的哈希表节点放到哈希表数组的指定索引上面。 Redis计算哈希值和索引值的方法如下： hash = dict-&gt;type-&gt;hashFunction(key); index = hash &amp; dict-&gt;ht[x].sizemask; Redis哈希表使用链接地址法来解决键冲突，当多个哈希表节点在一个索引上时，多个哈希表节点可以用next指针构成一个单向链表。 rehash随着操作的不断执行， 哈希表保存的键值对会逐渐地增多或者减少， 为了让哈希表的负载因子（load factor）维持在一个合理的范围之内， 当哈希表保存的键值对数量太多或者太少时， 程序需要对哈希表的大小进行相应的扩展或者收缩。 扩展和收缩哈希表的工作可以通过执行 rehash （重新散列）操作来完成， Redis 对字典的哈希表执行 rehash 的步骤如下： 为字典的 ht[1] 哈希表分配空间， 这个哈希表的空间大小取决于要执行的操作， 以及 ht[0] 当前包含的键值对数量 （也即是 ht[0].used 属性的值）： 如果执行的是扩展操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （2 的 n 次方幂）； 如果执行的是收缩操作， 那么 ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n 。 将保存在 ht[0] 中的所有键值对 rehash 到 ht[1] 上面： rehash 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 ht[1] 哈希表的指定位置上。 当 ht[0] 包含的所有键值对都迁移到了 ht[1] 之后 （ht[0] 变为空表）， 释放 ht[0] ， 将 ht[1] 设置为 ht[0] ， 并在 ht[1] 新创建一个空白哈希表， 为下一次 rehash 做准备. 渐进式rehash扩展或收缩哈希表需要将 ht[0] 里面的所有键值对 rehash 到 ht[1] 里面， 但是， 这个 rehash 动作并不是一次性、集中式地完成的， 而是分多次、渐进式地完成的。 这样做的原因在于， 如果 ht[0] 里只保存着四个键值对， 那么服务器可以在瞬间就将这些键值对全部 rehash 到 ht[1] ； 但是， 如果哈希表里保存的键值对数量不是四个， 而是四百万、四千万甚至四亿个键值对， 那么要一次性将这些键值对全部 rehash 到 ht[1] 的话， 庞大的计算量可能会导致服务器在一段时间内停止服务。 因此， 为了避免 rehash 对服务器性能造成影响， 服务器不是一次性将 ht[0] 里面的所有键值对全部 rehash 到 ht[1] ， 而是分多次、渐进式地将 ht[0] 里面的键值对慢慢地 rehash 到 ht[1] 。 以下是哈希表渐进式 rehash 的详细步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表。 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始。 在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一。 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成。 渐进式 rehash 的好处在于它采取分而治之的方式， 将 rehash 键值对所需的计算工作均滩到对字典的每个添加、删除、查找和更新操作上， 从而避免了集中式 rehash 而带来的庞大计算量. 简单代码 Ref：1.Redis设计与实现2.Redis源码]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis之链表实现]]></title>
    <url>%2F2018%2F08%2F11%2Fredis%E4%B9%8B%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Redis链表的实现代码在src/[adlist.h, adlist.c] typedef struct listNode { // 指向前节点的指针 struct listNode *prev; // 指向后节点的指针 struct listNode *next; // 节点的值 void *value; } listNode; typedef struct list { // 表头节点 listNode *head; // 表尾节点 listNode *tail; // 节点值复制函数 void *(*dup)(void *ptr); // 节点值释放函数 void (*free)(void *ptr); // 节点值对比函数 int (*match)(void *ptr, void *key); // 链表所包含的节点数量 unsigned long len; } list; // 返回给定链表所包含的节点数量 // T = O(1) #define listLength(l) ((l)-&gt;len) // 返回给定链表的表头节点 // T = O(1) #define listFirst(l) ((l)-&gt;head) // 返回给定链表的表尾节点 // T = O(1) #define listLast(l) ((l)-&gt;tail) // 返回给定节点的前置节点 // T = O(1) #define listPrevNode(n) ((n)-&gt;prev) // 返回给定节点的后置节点 // T = O(1) #define listNextNode(n) ((n)-&gt;next) // 返回给定节点的值 // T = O(1) #define listNodeValue(n) ((n)-&gt;value) // 将链表 l 的值复制函数设置为 m // T = O(1) #define listSetDupMethod(l,m) ((l)-&gt;dup = (m)) // 将链表 l 的值释放函数设置为 m // T = O(1) #define listSetFreeMethod(l,m) ((l)-&gt;free = (m)) // 将链表的对比函数设置为 m // T = O(1) #define listSetMatchMethod(l,m) ((l)-&gt;match = (m)) // 返回给定链表的值复制函数 // T = O(1) #define listGetDupMethod(l) ((l)-&gt;dup) // 返回给定链表的值释放函数 // T = O(1) #define listGetFree(l) ((l)-&gt;free) // 返回给定链表的值对比函数 // T = O(1) #define listGetMatchMethod(l) ((l)-&gt;match) // 创建一个新的链表 list *listCreate(void) { struct list *list; // 分配内存 if ((list = zmalloc(sizeof(*list))) == NULL) return NULL; // 初始化属性 list-&gt;head = list-&gt;tail = NULL; list-&gt;len = 0; list-&gt;dup = NULL; list-&gt;free = NULL; list-&gt;match = NULL; return list; } /* * 释放整个链表，以及链表中所有节点 * * T = O(N) */ void listRelease(list *list) { unsigned long len; listNode *current, *next; // 指向头指针 current = list-&gt;head; // 遍历整个链表 len = list-&gt;len; while(len--) { next = current-&gt;next; // 如果有设置值释放函数，那么调用它 if (list-&gt;free) list-&gt;free(current-&gt;value); // 释放节点结构 zfree(current); current = next; } // 释放链表结构 zfree(list); } /* * 将一个包含有给定值指针 value 的新节点添加到链表的表头 * * 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL * * 如果执行成功，返回传入的链表指针 * * T = O(1) */ list *listAddNodeHead(list *list, void *value) { listNode *node; // 为节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 添加节点到空链表 if (list-&gt;len == 0) { list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 添加节点到非空链表 } else { node-&gt;prev = NULL; //新节点前驱置为NULL node-&gt;next = list-&gt;head; //新节点的后驱置为头节点 list-&gt;head-&gt;prev = node; //头结点的前驱更新为node节点 list-&gt;head = node; //头结点更新为node节点 } // 更新链表节点数 list-&gt;len++; return list; } /* * 将一个包含有给定值指针 value 的新节点添加到链表的表尾 * * 如果为新节点分配内存出错，那么不执行任何动作，仅返回 NULL * * 如果执行成功，返回传入的链表指针 * * T = O(1) */ list *listAddNodeTail(list *list, void *value) { listNode *node; // 为新节点分配内存 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值指针 node-&gt;value = value; // 目标链表为空 if (list-&gt;len == 0) { list-&gt;head = list-&gt;tail = node; node-&gt;prev = node-&gt;next = NULL; // 目标链表非空 } else { node-&gt;prev = list-&gt;tail; //node节点前驱为尾结点 node-&gt;next = NULL; //node节点后驱置为NULL list-&gt;tail-&gt;next = node; //尾结点的后驱更新为node结点 list-&gt;tail = node; //尾结点更新为node结点 } // 更新链表节点数 list-&gt;len++; return list; } /* * 创建一个包含值 value 的新节点，并将它插入到 old_node 的之前或之后 * * 如果 after 为 0 ，将新节点插入到 old_node 之前。 * 如果 after 为 1 ，将新节点插入到 old_node 之后。 * * T = O(1) */ list *listInsertNode(list *list, listNode *old_node, void *value, int after) { listNode *node; // 创建新节点 if ((node = zmalloc(sizeof(*node))) == NULL) return NULL; // 保存值 node-&gt;value = value; // 将新节点添加到给定节点之后 if (after) { node-&gt;prev = old_node; node-&gt;next = old_node-&gt;next; // 给定节点是原表尾节点 if (list-&gt;tail == old_node) { list-&gt;tail = node; } // 将新节点添加到给定节点之前 } else { node-&gt;next = old_node; node-&gt;prev = old_node-&gt;prev; // 给定节点是原表头节点 if (list-&gt;head == old_node) { list-&gt;head = node; } } // 更新新节点的前置指针 if (node-&gt;prev != NULL) { node-&gt;prev-&gt;next = node; } // 更新新节点的后置指针 if (node-&gt;next != NULL) { node-&gt;next-&gt;prev = node; } // 更新链表节点数 list-&gt;len++; return list; } /* * 取出链表的表尾节点，并将它移动到表头，成为新的表头节点。 * * T = O(1) */ void listRotate(list *list) { listNode *tail = list-&gt;tail;// 取出表尾节点 if (listLength(list) &lt;= 1) return; /* Detach current tail */ list-&gt;tail = tail-&gt;prev; //尾结点为先尾结点的前一个结点 list-&gt;tail-&gt;next = NULL; //新尾结点指向NULL /* Move it as head */ // 插入到表头 list-&gt;head-&gt;prev = tail; //旧头结点的前驱变为旧尾结点 tail-&gt;prev = NULL; //旧尾结点的前驱置为NULL tail-&gt;next = list-&gt;head; //旧尾结点的后驱置为头结点 list-&gt;head = tail; //新头结点置为旧尾结点 } /* * 从链表 list 中删除给定节点 node * * 对节点私有值(private value of the node)的释放工作由调用者进行。 * * T = O(1) */ void listDelNode(list *list, listNode *node) { // 调整前置节点的指针 if (node-&gt;prev) node-&gt;prev-&gt;next = node-&gt;next; else list-&gt;head = node-&gt;next; // 调整后置节点的指针 if (node-&gt;next) node-&gt;next-&gt;prev = node-&gt;prev; else list-&gt;tail = node-&gt;prev; // 释放值 if (list-&gt;free) list-&gt;free(node-&gt;value); // 释放节点 zfree(node); // 链表数减一 list-&gt;len--; } // 为给定链表创建一个迭代器，之后每次对这个迭代器调用 listNext 都返回被迭代到的链表节点 listIter *listGetIterator(list *list, int direction) { // 为迭代器分配内存 listIter *iter; if ((iter = zmalloc(sizeof(*iter))) == NULL) return NULL; // 根据迭代方向，设置迭代器的起始节点 if (direction == AL_START_HEAD) iter-&gt;next = list-&gt;head; else iter-&gt;next = list-&gt;tail; // 记录迭代方向 iter-&gt;direction = direction; return iter; } // 返回迭代器当前所指向的节点 listNode *listNext(listIter *iter) { listNode *current = iter-&gt;next; if (current != NULL) { // 根据方向选择下一个节点 if (iter-&gt;direction == AL_START_HEAD) // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;next; else // 保存下一个节点，防止当前节点被删除而造成指针丢失 iter-&gt;next = current-&gt;prev; } return current; } listNode *listSearchKey(list *list, void *key) { listIter *iter; listNode *node; // 迭代整个链表 iter = listGetIterator(list, AL_START_HEAD); while((node = listNext(iter)) != NULL) { // 对比 if (list-&gt;match) { if (list-&gt;match(node-&gt;value, key)) { listReleaseIterator(iter); // 找到 return node; } } else { if (key == node-&gt;value) { listReleaseIterator(iter); // 找到 return node; } } } listReleaseIterator(iter); // 未找到 return NULL; } 测试代码 Ref： 1.Redis设计与实现 2.Redis源码]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redis之简单字符串SDS]]></title>
    <url>%2F2018%2F08%2F08%2Fredis%E4%B9%8B%E7%AE%80%E5%8D%95%E5%AD%97%E7%AC%A6%E4%B8%B2SDS%2F</url>
    <content type="text"><![CDATA[redis并没有直接使用C语言传统的字符串表示，而是自己构建了一种名为简单动态字符串(simple dynamic string,SDS)的抽象类型，并将SDS用作Redis的默认字符串表示。struct sdshdr { // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间，char类型的数组，以空字符&#39;\0&#39;结尾，空字符不计算在SDS中的len属性里 char buf[]; }; SDS与C字符串的区别: 1.SDS中len属性记录了SDS的长度，所以获取一个SDS长度的复杂度仅为O(1)，而为了获取C字符串的长度必须对C字符串进行遍历，直到遇到代表字符串结尾的空字符串为止，复杂度为O(N)。 2.SDS的空间分配策略完全杜绝了发生缓冲溢出的可能性：当SDS API需要对SDS进行修改时，API会首先检查SDS的空间是否满足修改所需要的要求，如果不满足的话，API会自动将SDS的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用SDS既不需要手动修改SDS的空间大小，也不会出现缓冲区溢出问题。 3.减少修改字符串带来的内存重分配次数 3.1空间预分配 - 如果对SDS进行修改之后，SDS的长度将小于1MB，那么将程序分配两倍所需长度的空间。 - 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配所需长度加上SDS_MAX_PREALLOC(默认为1M)。 通过空间预分配策略，Redis可以减少连续执行字符串增长操作所需的内存重分配次数。3.2 惰性空间释放 - 惰性空间释放用于优化SDS的字符串缩短操作：当SDS的API需要缩短SDS保存的字符串时，程序并不会立即使用内存分配来回收缩短后多出来的字节，而是使用free属性将这些字节的数量记录起来，并等待将来使用。 4.二进制安全 C字符串中的字符必须符合某种编码(如ASCII),并且除了字符串的末的尾之外，字符串里不能包含空字符，否则会被认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存像图片、音频、视频、压缩文件这样的二进制数据。SDS的API都是二进制安全的，所有SDS API都会以处理二进制的方式处理SDS存放在buf数组里的数据，程序不会对其中的数据做任何限制、过滤、或者假设，数据在写入的时候是什么样的，它被读取时就是什么样的。 C字符串与SDS之间的区别 C字符串 SDS 获取字符串长度的复杂度为O(N) 获取字符串长度的复杂度为O(1) API不安全，可能会造成缓冲区溢出 API是安全的，不会造成缓冲区溢出 修改字符串长度N次必然需要执行N次内存分配 修改字符串长度N次最多需要执行N次内存分配 只能保存文本 可以保存文本或者二进制数据 可以使用所有库中的函数 可以使用部分中的函数 // 创建一个包含给定C字符串的SDS sds sdsnewlen(const void *init, size_t initlen) { struct sdshdr *sh; // 根据是否有初始化内容，选择适当的内存分配方式 // T = O(N) if (init) { // zmalloc 不初始化所分配的内存 sh = zmalloc(sizeof(struct sdshdr)+initlen+1); } else { // zcalloc 将分配的内存全部初始化为 0 sh = zcalloc(sizeof(struct sdshdr)+initlen+1); } // 内存分配失败，返回 if (sh == NULL) return NULL; // 设置初始化长度 sh-&gt;len = initlen; // 新 sds 不预留任何空间 sh-&gt;free = 0; // 如果有指定初始化内容，将它们复制到 sdshdr 的 buf 中 // T = O(N) if (initlen &amp;&amp; init) memcpy(sh-&gt;buf, init, initlen); // 以 \0 结尾 sh-&gt;buf[initlen] = &#39;\0&#39;; // 返回 buf 部分，而不是整个 sdshdr return (char*)sh-&gt;buf; } // 创建一个包含给定C字符串的SDS,在sdsnewlen基础上添加的对char *init的判断 sds sdsnew(const char *init) { size_t initlen = (init == NULL) ? 0 : strlen(init); return sdsnewlen(init, initlen); } /* * 返回 sds 实际保存的字符串的长度 * * T = O(1) */ // 因为创建SDS时返回的是指向buf部分的指针，所以通过指针运算取得指向sdshdr的指针 static inline size_t sdslen(const sds s) { struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;len; } /* * 返回 sds 可用空间的长度 * * T = O(1) */ static inline size_t sdsavail(const sds s) { struct sdshdr *sh = (void*)(s-(sizeof(struct sdshdr))); return sh-&gt;free; } // 创建并返回一个只保存了空字符串&quot;&quot;的sds sds sdsempty(void) { return sdsnewlen(&quot;&quot;,0); } // 复制给定sds的副本 sds sdsdup(const sds s) { return sdsnewlen(s, sdslen(s)); } // 释放sds void sdsfree(sds s) { if (s == NULL) return; zfree(s-sizeof(struct sdshdr)); } // 清空SDS保存的字符串为空字符串，并没有释放内存 void sdsclear(sds s) { // 取出 sdshdr struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr))); // 重新计算属性 sh-&gt;free += sh-&gt;len; sh-&gt;len = 0; // 将结束符放到最前面（相当于惰性地删除 buf 中的内容） sh-&gt;buf[0] = &#39;\0&#39;; } /* 扩展sds大小，先判断空余长度是否大于需要扩展的长度，如果是直接返回sds。然后再判断，如果对SDS进行修改之后，SDS的长度将小于1MB，那么将程序分配两倍所需长度的空间。 如果对SDS进行修改之后，SDS的长度将大于等于1MB，那么程序会分配所需长度加上SDS_MAX_PREALLOC(默认为1M) */ sds sdsMakeRoomFor(sds s, size_t addlen) { struct sdshdr *sh, *newsh; // 获取 s 目前的空余空间长度 size_t free = sdsavail(s); size_t len, newlen; // s 目前的空余空间已经足够，无须再进行扩展，直接返回 if (free &gt;= addlen) return s; // 获取 s 目前已占用空间的长度 len = sdslen(s); sh = (void*) (s-(sizeof(struct sdshdr))); // s 最少需要的长度 newlen = (len+addlen); // 根据新长度，为 s 分配新空间所需的大小 if (newlen &lt; SDS_MAX_PREALLOC) // 如果新长度小于 SDS_MAX_PREALLOC // 那么为它分配两倍于所需长度的空间 newlen *= 2; else // 否则，分配长度为目前长度加上 SDS_MAX_PREALLOC newlen += SDS_MAX_PREALLOC; // T = O(N) newsh = zrealloc(sh, sizeof(struct sdshdr)+newlen+1); // 内存不足，分配失败，返回 if (newsh == NULL) return NULL; // 更新 sds 的空余长度 newsh-&gt;free = newlen - len; // 返回 sds return newsh-&gt;buf; } // 回收sds中的空闲空间 sds sdsRemoveFreeSpace(sds s) { struct sdshdr *sh; sh = (void*) (s-(sizeof(struct sdshdr))); // 进行内存重分配，让 buf 的长度仅仅足够保存字符串内容 // T = O(N) sh = zrealloc(sh, sizeof(struct sdshdr)+sh-&gt;len+1); // 空余空间为 0 sh-&gt;free = 0; return sh-&gt;buf; } // 返回给定sds分配的内存字节数 size_t sdsAllocSize(sds s) { struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr))); return sizeof(*sh)+sh-&gt;len+sh-&gt;free+1; } // 增加sds的长度，缩减空余空间，并将 \0 放到新字符串的尾端 void sdsIncrLen(sds s, int incr) { struct sdshdr *sh = (void*) (s-(sizeof(struct sdshdr))); // 确保 sds 空间足够 assert(sh-&gt;free &gt;= incr); // 更新属性 sh-&gt;len += incr; sh-&gt;free -= incr; // 这个 assert 其实可以忽略 // 因为前一个 assert 已经确保 sh-&gt;free - incr &gt;= 0 了 assert(sh-&gt;free &gt;= 0); // 放置新的结尾符号 s[sh-&gt;len] = &#39;\0&#39;; } // 将长度为 len 的字符串 t 追加到 sds 的字符串末尾 sds sdscatlen(sds s, const void *t, size_t len) { struct sdshdr *sh; // 原有字符串长度 size_t curlen = sdslen(s); // 扩展 sds 空间 // T = O(N) s = sdsMakeRoomFor(s,len); // 内存不足？直接返回 if (s == NULL) return NULL; // 复制 t 中的内容到字符串后部 // T = O(N) sh = (void*) (s-(sizeof(struct sdshdr))); memcpy(s+curlen, t, len); // 更新属性 sh-&gt;len = curlen+len; sh-&gt;free = sh-&gt;free-len; // 添加新结尾符号 s[curlen+len] = &#39;\0&#39;; // 返回新 sds return s; } // 将给定字符串 t 追加到 sds 的末尾 sds sdscat(sds s, const char *t) { return sdscatlen(s, t, strlen(t)); } // 将另一个 sds 追加到一个 sds 的末尾 sds sdscatsds(sds s, const sds t) { return sdscatlen(s, t, sdslen(t)); } // 把数组转换为sds sds sdsjoin(char **argv, int argc, char *sep) { sds join = sdsempty(); int j; for (j = 0; j &lt; argc; j++) { join = sdscat(join, argv[j]); if (j != argc-1) join = sdscat(join,sep); } return join; } typedef int int_t; // declares int_t to be an alias for the type int typedef char char_t, *char_p, (*fp)(void); // declares char_t to be an alias for char // char_p to be an alias for char* // fp to be an alias for char(*)(void) 在计算机科学中，内联函数（有时称作在线函数或编译时期展开函数）是一种编程语言结构，用来建议编译器对一些特殊函数进行内联扩展（有时称作在线扩展）；也就是说建议编译器将指定的函数体插入并取代每一处调用该函数的地方（上下文），从而节省了每次调用函数带来的额外时间开支。但在选择使用内联函数时，必须在程序占用空间和程序执行效率之间进行权衡，因为过多的比较复杂的函数进行内联扩展将带来很大的存储资源开支。另外还需要特别注意的是对递归函数的内联扩展可能引起部分编译器的无穷编译。内联函数 static inline,使用static修饰符，函数仅在文件内部可见，不会污染命名空间 void *memset( void *dest, int ch, size_t count ); Copies the value ch (after conversion to unsigned char as if by (unsigned char)ch) into each of the first count characters of the object pointed to by dest. The behavior is undefined if access occurs beyond the end of the dest array. The behavior is undefined if dest is a null pointer Ref： 1.Redis设计与实现 2.Redis源码]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式开篇]]></title>
    <url>%2F2018%2F08%2F07%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%BC%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[oop特性1.封装，对外部隐藏内部的实现,对数据进行隐藏 2.多态性，根据类型的不一致，调用不同的方法 3.继承，可以继承其它类的功能 4.抽象-提供简易接口给客户端，客户端可以调用接口中的方法 5.混合，可以构建复杂的数据结构 # A类在B类下 class A(object): def a1(self): print(&quot;a1&quot;) class B(object): def b(self): print(&quot;b&quot;) A().a1() objectB = B() objectB.b() 1.打开/关闭原则：对扩展开放(在不改变原有代码的情况下添加功能)，对修改关闭 2.控制原理的反转：控制原理的反转表明高级模块不应该依赖于低级模块;它们都应该依赖于抽象。细节应该取决于抽象，而不是相反。这个原则表明，任何两个模块都不应该以一种紧密的方式相互依赖。 4.接口隔离原则：客户端不应该依赖它不需要的接口，一个类对另一个类的依赖应该建立在最小的接口 5.单一责任原则：一个类只负责一个功能 6.替代原则 —在扩展子类的时候，不需要重写父类的功能 设计模式分为三大类：1.创建型-它们的工作原理是如何创建对象-他们隔离了对象创建的细节-代码与要创建的对象类型无关 2.结构型-它们设计对象和类的结构以便获得更大的结果-它们关注简化结构和发现类和对象之间的关系-它们关注类的继承与混合 3.行为型-他们关注对象之间的交互和对象的责任-对象应该能够交互并且仍然可以松散耦合]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python property]]></title>
    <url>%2F2018%2F08%2F05%2Fpython-property%2F</url>
    <content type="text"><![CDATA[class property([fget[, fset[, fdel[, doc]]]]) # fget是用于获取属性值的函数 # fset是用于设置属性值的函数 # fdel是用于删除属性值的函数 # doc是文档字符串 下面是官方文档上的一个例子： In [2]: class C: def __init__(self): print(&quot;invoke __init__&quot;) self._x = None def getx(self): print(&quot;invoke getx&quot;) return self._x def setx(self, value): print(&quot;invoke setx&quot;) self._x = value def delx(self): print(&quot;invoke delx&quot;) del self._x x = property(getx, setx, delx, &quot;I&#39;m the &#39;x&#39; property.&quot;) 当访问c.x时会调用getx方法，当为c.x设置值时会调用setx方法，删除c.x时会调用delx方法。 在对实例属性的获取和设定上，我们希望增加一些额外的处理过程(比如类型检查或者验证)。要自定义对属性的访问，一种简单的方式是将其定义为property。比如下面的代码定义了一个property，增加了对属性的类型检查。 In [9]: class Person: def __init__(self, first_name): self.first_name = first_name print(&quot;invoke __init__&quot;) @property def first_name(self): print(&quot;invoke get first_name&quot;) return self._first_name @first_name.setter def first_name(self, value): print(&quot;invoke set first_name&quot;) if not isinstance(value, str): raise TypeError(&quot;Expected a string&quot;) self._first_name = value @first_name.deleter def first_name(self): raise AttributeError(&quot;Can&#39;t delete attribute&quot;) 你可能会问为什么在init方法中设定的是self.first_name而 不是self._first_name?在这个例子中，property的全部意义就在于我们设置属性时可以执行类型检查。因此，你也想让这种类型检查在初始化的时候也可以进行。所以，在init中设置self.first_name,实际上会调用到setter方法(因此就会跳过self.first_name而去访问self._first_name)。 property使用场景： 只有当确定需要在访问属性必完成一些额外的处理任务时，才应该使用property. Ref： 1.官方文档 2.python cookbook]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python描述器]]></title>
    <url>%2F2018%2F08%2F04%2Fpython%E6%8F%8F%E8%BF%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[通常，描述器是具有“绑定行为”的对象属性，其属性访问已被描述器协议中的方法覆盖。这些方法是get __（）， set _（）和\_delete __（）。如果为对象定义了任何这些方法，则称其为描述器. descr.__get__(self, obj, type=None) --&gt; value descr.__set__(self, obj, value) --&gt; None descr.__delete__(self, obj) --&gt; None 如果你想创建一个全新的实例属性，可以通过一个描述器类的形式来定义它的功 能。下面是一个例子： class Integer: def __init__(self, name): self.name = name def __get__(self, instance, cls): if instance is None: return self else: return instance.__dict__[self.name] def __set__(self, instance, value): if not isinstance(value, int): raise TypeError(&#39;Expected an int&#39;) instance.__dict__[self.name] = value def __delete__(self, instance): del instance.__dict__[self.name] class Point: x = Integer(&#39;x&#39;) y = Integer(&#39;y&#39;) def __init__(self, x, y): self.x = x self.y = y 当你这样做后，所有对描述器属性(比如x 或y) 的访问会被get () 、set () 和delete () 方法捕获到. 描述器只能在类 级别被定义，而不能为每个实例单独定义。 # 定义一个带__get__,__set__方法的描述符 In [5]: class D: def __init__(self): pass def __get__(self, instance, cls): print(&quot;D get...&quot;) def __set__(self, instance, val): self.value = val In [6]: class TD: x = D() def __init__(self,x): self.x = x In [11]: t = TD(1) In [12]: t.x D get... #这里调用了描述符的__get__方法 # 去掉描述中的__set__方法 In [5]: class D: def __init__(self): pass def __get__(self, instance, cls): print(&quot;D get...&quot;) In [6]: class TD: x = D() def __init__(self,x): self.x = x In [11]: t = TD(1) In [12]: t.x #去掉__set__方法后并没调用描述符中的__get__方法 1 一个类，如果只定义了 get() 方法，而没有定义 set(), delete() 方法，则认为是非数据描述符； 反之，则成为数据描述符。 而非数据描述符的优先级是低于实例属性的。 python中的属性访问优先级： 1.getattribute()方法无条件调用 2.数据描述符（由1触发调用，若改写了getattribute(),可能会导致无法调用描述符) 3.实例对象的字典（若与描述符对象同名，会被覆盖) 4.类的字典 5.非数据描述符 6.父类的字典 7.getattr()方法 使用一个描述器类定义一个延迟计算属性 class lazyproperty: def __init__(self, func): self.func = func def __get__(self, instance, cls): if instance is None: return self else: value = self.func(instance) setattr(instance, self.func.__name__, value) return value import math class Circle: def __init__(self, radius): self.radius = radius @lazyproperty def area(self): print(&#39;Computing area&#39;) return math.pi * self.radius ** 2 @lazyproperty def perimeter(self): print(&#39;Computing perimeter&#39;) return 2 * math.pi * self.radius 很多时候构建延迟计算属性的主要目的是为了提升性能，只有在真正需要的时候才进行计算。 上例中访问c.area只计算一次，当第一次计算后，c.area的值会存在实例的字典中，而只定义了一个get方法的非数据描述器的优先级是小于实例属性的,所以会直接访问实例属性。 vars(...) vars([object]) -&gt; dictionary Without arguments, equivalent to locals(). With an argument, equivalent to object.__dict__. Ref： 1.官方文档 2.python cookbook 3.https://www.cnblogs.com/Jimmy1988/p/6808237.html]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python GIL与线程调度]]></title>
    <url>%2F2018%2F08%2F03%2Fpython%20GIL%E4%B8%8E%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[GIL(Global Interpreter Lock)-全局解释锁，python使用GIL来互斥线程对python虚拟机的使用，同一时间只能并且仅有一个线程使用python虚拟机。 为什么要使用GIL 假设有两个线程A、B，在两个线程中，都同时保存着对内存中同一对象obj的引用，也就是说，这时obj-&gt;ob_refcnt的值为2。如果A销毁对obj的引用，显然，A将通过Py_DECREF调整obj的引用计数值。Py_DECREF的整个 动作可以分为两个部分： --obj-&gt;ob_refcnt; if(obj-&gt;ob_refcnt == 0) destory object and free memory 如果A在执行完第一个动作之后，obj-&gt;ob_refcnt的值变为1。若此时线程A被挂起，而唤醒了B线程。如果B同样也开始销毁对obj的引用。B完成第一动作之后，obj-&gt;ob_refcnt为0，若此时B没有被线程调度打断，而是顺利地完成了接下来的第二个动作，将对象销毁，内存释放。然后A又被重新唤醒，obj-&gt;ob_refcnt已经被B减少到0，而不是当初的1。此时A开始对已经销毁的对象进行对象销毁和内存释放动作。这将会发生意外。。 为了支持多线程机制，一个基本的要求就是需要实现不同线程对共享资源访问的互斥，所以引入了GIL。 python线程的调度 python会在两个情况下启动线程调度机制 1.线程执行完成，释放全局锁 2.当python执行了N条指令后会立即启动线程调度机制。这个值可以通过sys.getcheckinterval()获取。 In [3]: sys.getcheckinterval() /usr/local/bin/ipython:1: DeprecationWarning: sys.getcheckinterval() and sys.setcheckinterval() are deprecated. Use sys.getswitchinterval() instead. #!/usr/bin/python3 Out[3]: 100 从python3.2开始这个函数已经被废弃了，改用sys.getswitchinterval()，调度间隔时间改为了0.005秒。 In [4]: sys.getswitchinterval() Out[4]: 0.005 Ref： 1.python源码剖析—深度探索动态语言核心技术 2.官方文档]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单例模式(Singleton)-创建型]]></title>
    <url>%2F2018%2F08%2F02%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F(Singleton)-%E5%88%9B%E5%BB%BA%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[单例模式保证类有且只有一个实例，并提供一个访问他的全局访问点。 1. class Singleton(object): def __new__(cls): if not hasattr(cls, &#39;instance&#39;): cls.instance = super(Singleton, cls).__new__(cls) return cls.instance s = Singleton() print(&quot;Object created&quot;, s) s1 = Singleton() print(&quot;Object created&quot;, s1) 2. class Singleton: __instance = None def __init__(self): if not Singleton.__instance: print(&quot; __init__ method called..&quot;) else: print(&quot;Instance already created:&quot;, self.getInstance()) @classmethod def getInstance(cls): if not cls.__instance: cls.__instance = Singleton() return cls.__instance 3. 4.有时候我们并不关心生成的实例是否具有同一id，而只关心其状态和行为方式。我们可以允许许多个实例被创建，但所有的实例都共享状态和行为方式： class Borg: __shared_state = {&quot;1&quot;:&quot;2&quot;} def __init__(self): self.x = 1 self.__dict__ = self.__shared_state pass b = Borg() b1 = Borg() b.x = 4 print(&quot;Borg Object &#39;b&#39;: &quot;, b) print(&quot;Borg Object &#39;b1&#39;: &quot;, b1) print(&quot;Object State &#39;b&#39;:&quot;, b.__dict__) print(&quot;Object State &#39;b1&#39;:&quot;, b1.__dict__) 或者使用new方法： In [16]: class Borg(object): _shared_state = {} def __new__(cls, *args, **kwargs): obj = super(Borg, cls).__new__(cls, *args, **kwargs) obj.__dict__ = cls._shared_state return obj 4.元类实现 # 当我们调用Logger()初始化Logger时，将会调用类MetaSingleton中的__call__方法 In [1]: class MetaSingleton(type): _instances = {} def __call__(cls, *args, **kwargs): if cls not in cls._instances: cls._instances[cls] = super(MetaSingleton, cls).__call__(*args, **kwargs) return cls._instances[cls] class Logger(metaclass=MetaSingleton): pass In [2]: l1 = Logger() In [3]: l2 = Logger() In [4]: id(l1) Out[4]: 140568820684616 In [5]: id(l2) Out[5]: 140568820684616 线程安全的写法： In [9]: def synchronized(func): ...: func.__lock__ = threading.Lock() ...: ...: def synced_func(*args, **kws): ...: with func.__lock__: ...: return func(*args, **kws) ...: ...: return synced_func ...: ...: In [10]: def Singleton(cls): ...: instances = {} ...: ...: @synchronized ...: def get_instance(*args, **kw): ...: if cls not in instances: ...: instances[cls] = cls(*args, **kw) ...: return instances[cls] ...: ...: return get_instance ...: ...: In [11]: @Singleton ...: class test(): ...: a = 1 ...: In [12]: s = test() In [13]: s1 = test() In [14]: id(s) Out[14]: 140308601028336 In [15]: id(s1) Out[15]: 140308601028336 实用场景： 1.数据库连接 2.应用健康检测 3.许多需要创建一个对象的场景，如线程池，缓存，对话框，等等。 优点： 通过单例模式可以保证系统中一个类只有一个实例而且该实例易于外界访问，从而方便对实例个数的控制并节约系统资源。如果希望在系统中某个类的对象只能存在一个，单例模式是最好的解决方案。 缺点： 全局变量可能会在某一处被修改，一个同一对象存在多个引用，所有依赖于全局变量的类都会紧密耦合，因为一个类对全局数据的更改可能会无意中影响另一个类 Ref： 1.Learning Python Design Patterns 2.https://www.cnblogs.com/baiyb/p/8506438.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[二叉树]]></title>
    <url>%2F2018%2F07%2F31%2F%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[代码实现]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[反转链表]]></title>
    <url>%2F2018%2F07%2F31%2F%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[class Node: def __init__(self, data=None, next=None): self.data = data self.next = next link = Node(1, Node(2, Node(3, Node(4, Node(5, Node(6, Node(7, Node(8, Node(9))))))))) def rev(link): p = link q = link.next p.next = None while q: r = q.next q.next = p p = q q = r return p root = rev(link) while root: print(root.data) root = root.next]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python functools wraps]]></title>
    <url>%2F2018%2F07%2F25%2Fpython-functools-wraps%2F</url>
    <content type="text"><![CDATA[把被封装函数的name、module、doc和dict复制到封装函数中，这样在未来排错或者函数自省的进修能够获得正确的源函数的对应属性，所以使用wraps。 In [1]: import functools # 不加wraps In [2]: def deco(f): ...: def wrapper(*args, **kwargs): ...: return f(*args, **kwargs) ...: return wrapper ...: In [3]: @deco ...: def func(): ...: return 1 ...: In [4]: func.__name__ Out[4]: &#39;wrapper&#39; # 加wraps In [5]: def deco2(f): ...: @functools.wraps(f) ...: def wrapper(*args, **kwargs): ...: return f(*args, **kwargs) ...: return wrapper ...: In [6]: @deco2 ...: def func(): ...: return 1 ...: In [7]: func.__name__ Out[7]: &#39;func&#39; Ref： 1.python web开发实战]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket Characters类型]]></title>
    <url>%2F2018%2F07%2F20%2Fracket%20Characters%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[&gt; (char? #\a) ;判断是否是字符 #t &gt; (char-&gt;integer #\λ) ;返回字符的整数 955 &gt; (char-&gt;integer #\A) 65 &gt; (integer-&gt;char 65) ;将整数转换为整数 #\A &gt; (char=? #\a #\a #\a) ;判断多个字符是否相等 #t &gt; (char&lt;? #\A #\a) ;根据(char-&gt;integer v)返回的整数值进行比较 #t &gt; (char&lt;? #\A #\λ) #t &gt; (char&lt;=? #\a #\b #\b) #t &gt; (char&gt;? #\a #\A) #t &gt; (char&gt;=? #\a #\A) #t ;生成与Unicode定义的1对1映射一致的字符。如果字符没有大写映射，则char-upcase会生成char &gt; (char-upcase #\a) #\A &gt; (char-upcase #\λ) #\Λ &gt; (char-downcase #\A) #\a &gt; (char-downcase #\Λ) #\λ ;和char-upcase类似，但对对应于Unicode case-folding mapping &gt; (char-foldcase #\A) #\a &gt; (char-foldcase #\Σ) #\σ &gt; (char-ci&lt;? #\A #\a) #f &gt; (char-ci&lt;? #\a #\b) #t &gt; (char-ci&lt;? #\a #\b #\c) #t Ref： 1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket number类型]]></title>
    <url>%2F2018%2F07%2F16%2Fracket%20number%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[&gt; (number? 1) ;判断是否是number类型 #t &gt; (complex? 1+2i) ;判断是否是complex类型 #t &gt; (complex? 1) ;所有number都是complex类型，1==1+0i #t &gt; (real? 1) ;判断是否是实数 #t &gt; (real? 1+i) #f &gt; (rational? 0.3) ;判断是否是有理数 #t &gt; (integer? 1) ;判断是否是整数 #t &gt; (integer? 1.0) #t &gt; (integer? 2.3) #f &gt; (exact? 1) ;判断是否是精确值 #t &gt; (exact? 1.0) #f &gt; (inexact? 1) ;判断是否是非精确数 #f &gt; (inexact? 1.0) #t &gt; (exact-integer? 1) ;相当于 (and (integer? v) (exact? v)) #t &gt; (exact-integer? 4.0) #f &gt; (negative? -10) ;判断是否是负数，返回(&gt; x 0) #t &gt; (negative? -0.0) #f &gt; (positive? 10) ;判断是否是正数，返回(&gt; x 0) #t &gt; (positive? 0.0) #f &gt; (even? 2) ;判断是否是偶数，0属于偶数 #t &gt; (even? 0) #t &gt; (odd? 11) ;判断是否是奇数 #t &gt; (exact-nonnegative-integer? 0) ;相当于(and (exact-integer? v) (not (negative? v))) #t &gt; (exact-positive-integer? 1) ;相关于(and (exact-integer? v) (positive? v)) #t &gt; (inexact-real? 1) ;相当于(and (real? v) (inexact? v)) #f &gt; (flonum? 0) ;判断是否是浮点数，等同于(double-flonum? v) #f &gt; (flonum? 0.0) #t &gt; (single-flonum? 1.0) ;判断是否是单精度浮点数 #f &gt; (zero? 0) ;判断是否等于0 #t &gt; (zero? -0.0) #t &gt; (truncate 2.5) ;对数进行截断 2.0 &gt; (truncate -2.5) -2.0 &gt; (numerator 3) ;返回数的分子 3 &gt; (numerator 14/3) 14 &gt; (denominator 5) ;返回数的分母 1 &gt; (denominator 17/4) 4 &gt; (expt 2 3) ;等于2**3 8 Ref： 1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[凸函数]]></title>
    <url>%2F2018%2F07%2F12%2F%E5%87%B8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[定义：函数f: \mathcal R^{n} \to \mathcal R是凸的，如果dom f是凸集，且对于任意x, y \in dom f和任意0 \le \theta \le 1，有 f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)图象代码 当\theta = 0时，f(y) = f(y)成立，当\theta = 1时，f(x) = f(x)成立, 当\theta = \frac{1}{2}时，f(\frac{x+y}{2}) \le \frac{f(x)+f(y)}{2} 从几何意义上看，上述不等式意味着点(x, f(x))和(y, f(y))之间的线段，即从x到y的弦，在函数f的图像上方(如上图)。 Ref：1.Convex Optimization (Stephen Boyd)2.支持向量机：理论，算法与拓展(田英杰)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket list类型]]></title>
    <url>%2F2018%2F07%2F11%2Fracket%20list%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[创建一个list,并赋值给变量lst&gt; (define lst (list 1 2 3 4)) &gt; lst &#39;(1 2 3 4) list内置方法&gt; (length lst) ;列表长度 4 &gt; (list-ref lst 0) ;根据下标取值 1 &gt; (append lst (list 5 6 7)) ;连接两个列表 &#39;(1 2 3 4 5 6 7) &gt; (reverse lst) ;反转列表 &#39;(4 3 2 1) &gt; (member 5 (list 1 2 3)) ;判断元素是否存在于列表中 #f &gt; (map sqrt (list 1 4 9 16)) ;依次对列表中的每个元素求算术平方根 &#39;(1 2 3 4) &gt; (andmap string? (list &quot;a&quot; &quot;b&quot; 6)) ;判断列表中元素是否全部为string, #f &gt; (ormap number? (list &quot;a&quot; &quot;b&quot; 6)) ;判断列表中元素是否存在number &gt; (filter string? (list &quot;a&quot; &quot;b&quot; 6)) ;对列表元素进行过滤 &#39;(&quot;a&quot; &quot;b&quot;) &gt; (first (list 1 2 3)) ;返回列表中的第一个元素 1 &gt; (rest (list 1 2 3)) ;返回除第一个元素的剩余元素 &#39;(2 3) Ref： 1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket条件语句]]></title>
    <url>%2F2018%2F07%2F11%2Fracket%E6%9D%A1%E4%BB%B6%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[( if ‹expr› ‹expr› ‹expr› )if语句，若2&gt;3输出”bigger”,否则输出”smaller”&gt; (if (&gt; 2 3) &quot;bigger&quot; &quot;smaller&quot;) ( cond {[ ‹expr› ‹expr› ]} )(相当于C中的switch,else相当于c中的default)&gt; (define (reply-more s) (cond [(equal? &quot;hello&quot; (substring s 0 5)) (string-append &quot;this is &quot; s)] [(equal? &quot;goodbye&quot; (substring s 0 7)) (string-append &quot;this is &quot; s)] [else &quot;hehe&quot;])) &gt; (reply-more &quot;hello&quot;) &quot;this is hello&quot; &gt; (reply-more &quot;dfdfllfd&quot;) &quot;hehe&quot; ( and ‹expr›* )(当所有表达式为真时，返回真)&gt; (define (is-digit v) (if (and (number? v) (&gt; v 0) (&lt; v 10)) &quot;v is a number and value is between 1-9&quot; &quot;v is not a number&quot;)) &gt; (is-digit 5) &quot;v is a number and value is between 1-9&quot; ( or ‹expr›* )(当任一表达式为真时，返回真)&gt; (define (str-num v) (if (or (string? v) (number? v)) &quot;v is string or number&quot; &quot;v is not string or number&quot;)) Ref： 1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python注释样本]]></title>
    <url>%2F2018%2F07%2F10%2Fpython%E6%B3%A8%E9%87%8A%E6%A0%B7%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[12345678910111213141516def select_proxy(url, proxies): &quot;&quot;&quot;Select a proxy for the url, if applicable. :param url: The url being for the request #参数 :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs :rtype: dict #返回值 &quot;&quot;&quot;def twoSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot;]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[requests中的公共函数utils.py]]></title>
    <url>%2F2018%2F07%2F10%2Frequests%E4%B8%AD%E7%9A%84%E5%85%AC%E5%85%B1%E5%87%BD%E6%95%B0utils.py%2F</url>
    <content type="text"><![CDATA[# 将字典转换为元素为(key, val)的列表 In [1]: def dict_to_sequnce(d): if hasattr(d, &#39;items&#39;): d = d.items() return d In [2]: d = {&#39;a&#39;: 1, &#39;b&#39;: 2} In [3]: dict_to_sequnce(d) Out[3]: dict_items([(&#39;a&#39;, 1), (&#39;b&#39;, 2)]) # 获取各种对象的长度，如str,BytesIO,文件 # 通过hasattr(o, &#39;getvalue&#39;)判断对象是是否是BytesIO,StringIO # 通过hasattr(o, &#39;fileno&#39;)判断是否是文件对象 # 文件和标准输入都有tell方法，而标准输入调用tell()方法则会产生异常，当遇到类似情况会把对象长度置为0,交给请求来获取它的长度。 def super_len(o): total_length = 0 current_position = 0 if hasattr(o, &#39;__len__&#39;): total_length = len(o) elif hasattr(o, &#39;len&#39;): total_length = o.len elif hasattr(o, &#39;getvalue&#39;): # e.g. BytesIO, cStringIO.StringIO total_length = len(o.getvalue()) elif hasattr(o, &#39;fileno&#39;): try: fileno = o.fileno() except io.UnsupportedOperation: pass else: total_length = os.fstat(fileno).st_size # Having used fstat to determine the file length, we need to # confirm that this file was opened up in binary mode. if &#39;b&#39; not in o.mode: warnings.warn(( &quot;Requests has determined the content-length for this &quot; &quot;request using the binary size of the file: however, the &quot; &quot;file has been opened in text mode (i.e. without the &#39;b&#39; &quot; &quot;flag in the mode). This may lead to an incorrect &quot; &quot;content-length. In Requests 3.0, support will be removed &quot; &quot;for files in text mode.&quot;), FileModeWarning ) if hasattr(o, &#39;tell&#39;): try: current_position = o.tell() except (OSError, IOError): # This can happen in some weird situations, such as when the file # is actually a special file descriptor like stdin. In this # instance, we don&#39;t know what the length is, so set it to zero and # let requests chunk it instead. current_position = total_length return max(0, total_length - current_position) # 如果提供的url中没有scheme,则将new_scheme作为url的scheme,如果有则不改变 def prepend_scheme_if_needed(url, new_scheme): scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme) if not netloc: netloc, path = path, netloc #应对netloc为空的特殊情况 return urlunparse((scheme, netloc, path, params, query, fragment)) # 去掉url中的用户名和密码部分 # 如http://root:root@www.baidu.com/,返回为http://www.baidu.com/ def urldefragauth(url): scheme, netloc, path, params, query, fragment = urlparse(url) if not netloc: netloc, path = path, netloc netloc = netloc.rsplit(&#39;@&#39;, 1)[-1] return urlunparse((scheme, netloc, path, params, query, &#39;&#39;)) _CLEAN_HEADER_REGEX_BYTE = re.compile(b&#39;^\\S[^\\r\\n]*$|^$&#39;) #\S匹配任意非空格字符,-- _CLEAN_HEADER_REGEX_STR = re.compile(r&#39;^\S[^\r\n]*$|^$&#39;) # 匹配以非空开头，非\r\n结尾或者空白字符串 # 检验头的有效性防止header injection def check_header_validity(header): name, value = header if isinstance(value, bytes): pat = _CLEAN_HEADER_REGEX_BYTE else: pat = _CLEAN_HEADER_REGEX_STR try: if not pat.match(value): raise InvalidHeader(&quot;Invalid return character or leading space in header: %s&quot; % name) except TypeError: raise InvalidHeader(&quot;Header value %s must be of type str or bytes, &quot; &quot;not %s&quot; % (value, type(value))) def get_auth_from_url(url): parsed = urlparse(url) try: auth = (unquote(parsed.username), unquote(parsed.password)) except (AttributeError, TypeError): auth = (&#39;&#39;, &#39;&#39;) return auth def is_ipv4_address(string_ip): try: socket.inet_aton(string_ip) except socket.error: return False return True def is_valid_cidr(string_network): &quot;&quot;&quot; 判断是否是类似192.168.2.1/16的地址 &quot;&quot;&quot; if string_network.count(&#39;/&#39;) == 1: try: mask = int(string_network.split(&#39;/&#39;)[1]) except ValueError: return False if mask &lt; 1 or mask &gt; 32: return False try: socket.inet_aton(string_network.split(&#39;/&#39;)[0]) except socket.error: return False else: return False return True # native string 指类型为str的字符串，python2与python3中str是不一样的(http://img.hysyeah.top/2017/10/02/python%E7%BC%96%E7%A0%81/) # Http请求/响应头和元数据要求数据为str类型。 def to_native_string(string, encoding=&#39;ascii&#39;): if isinstance(string, builtin_str): out = string else: if is_py2: out = string.encode(encoding) else: out = string.decode(encoding) return out # 将(key,val)为元素的列表转换为OrderedDict def from_key_val_list(value): if value is None: return None if isinstance(value, (str, bytes, bool, int)): raise ValueError(&#39;cannot encode objects that are not 2-tuples&#39;) return OrderedDict(value) # 将dict转换为list def to_key_val_list(value): if value is None: return None if isinstance(value, (str, bytes, bool, int)): raise ValueError(&#39;cannot encode objects that are not 2-tuples&#39;) if isinstance(value, collections.Mapping): value = value.items() return list(value) Ref：1.header injection2.requests/utils.py]]></content>
      <categories>
        <category>requests</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib改变坐标轴位置]]></title>
    <url>%2F2018%2F06%2F29%2Fmatplotlib%E6%94%B9%E5%8F%98%E5%9D%90%E6%A0%87%E8%BD%B4%E4%BD%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[ax.spines[‘top’].set_color(‘none’)隐藏上方的直线 通过set_position方法设置坐标轴的位置默认的设置为(‘outward’,0)。(‘axes’,0.0~1.0)，若数值=0.5表示把坐标轴放在整个坐标长度的中间位置(根据比例) (‘data’,xx)，根据实际数值指定坐标轴位置 下面将x,y轴移动至中间，画函数y=x的图像# -*- coding: utf-8 -*- import numpy as np import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;left&#39;].set_position((&#39;axes&#39;, 0.5)) ax.spines[&#39;bottom&#39;].set_position((&#39;axes&#39;, 0.5)) x = np.linspace(-5, 5, 1000) ax.plot(x, x) ax.text(-4, 4, &#39;y=x&#39;, color=&#39;r&#39;) ax.set_xlabel(&#39;x&#39;, horizontalalignment=&#39;right&#39;, x=1.0) ax.set_ylabel(&#39;y&#39;, horizontalalignment=&#39;right&#39;, y=1.0, rotation=0) plt.show() 代码 Ref： 1.matplotlib文档]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2018%2F06%2F28%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[logistic分布设X是连续随机变量，X服从logistic分布是指X具有下列分布函数和密度函数：$ F(x)=P(X \le x) = \frac{1}{1+e^{-(x - \mu)/\gamma}} $$ f(x) = F’(x) = \frac{e^{-(x-\mu)/\gamma} }{\gamma(1+e^{-(x-\mu)/\gamma})^2} $ 式中，$ \mu $ 为位置参数，$ \gamma \gt 0 $ 为形状参数。当$ \mu $不等于0时，相当于图像向左或向右移动$ \mu $个单位，$ \gamma $越大，图像越平坦。 图像代码 logistic回归模型二项logistic回归模型是如下的条件概率分布:$ P(Y=1|x) = \frac{\exp(w \cdot x + b)} {1+\exp(w \cdot x + b)} $$ P(Y=0|x) = \frac{1} {1+\exp(w \cdot x + b)} $这里，$ x \in \mathcal R^n $是输入，$ Y \in {0,1} $是输出，$ w\in \mathcal R^n $和$ b\in \mathcal R $是参数，w称为权值向量，b称为偏置，$ w \cdot x $为w和x的内积。 逻辑回归其实就是回归加上阶梯函数，将输出值映射到一定的范围内。 模型参数估计 # 构建数据集 def loadDataSet2(): dataMat = []; labelMat = [] fr = open(&#39;./dataset/testSet5.txt&#39;) for line in fr.readlines(): lineArr = line.strip().split() dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat,labelMat # 梯度下降(每次都要对所有训练数据进行计算，不适合数据量大的场景) def gradAscent(dataMatIn, classLabels): dataMatrix = np.mat(dataMatIn) labelMat = np.mat(classLabels).transpose() m, n = dataMatrix.shape alpha = 0.001 max_iter = 500 weights = np.ones((n, 1)) for k in range(max_iter): h = sigmoid(dataMatrix*weights) error = (labelMat -h) weights = weights + alpha * dataMatrix.transpose() * error return weights # 随机梯度下降每次迭代只使用一条训练数据进行计算，减少计了计算量 def stocGradAscent0(dataMatrix, classLabels): m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) for i in range(m): h = sigmoid(sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights # 改良版随机梯度下降， def stocGradAscent1(dataMatrix, classLabels, numIter=150): m, n = dataMatrix.shape weights = np.ones(n) dataIndex = range(m) for j in range(numIter): for i in range(m): alpha = 4/(1.0+j+i)+0.01 randIndex = int(np.random.uniform(0, len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha*error*dataMatrix[randIndex] return weights # 分类 def classifyVec(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1 return 0 # 训练，测试 def colicTest(): frTrain = open(&#39;dataset/horseColicTraining.txt&#39;) frTest = open(&#39;dataset/horseColicTest.txt&#39;) trainingSet, trainingLabels = [], [] for line in frTrain.readlines(): currLine = line.strip().split(&#39;\t&#39;) lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(np.array(trainingSet), trainingLabels, 500) errorCount, numTestVec = 0, 0 for line in frTest.readlines(): numTestVec += 1 currLine = line.strip().split(&#39;\t&#39;) lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVec(np.array(lineArr), trainWeights))!=int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print(&quot;the error rate of this test is: %f&quot; % errorRate) 完整代码 Ref：1.统计学习方法2.机器学习实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[牛顿法]]></title>
    <url>%2F2018%2F06%2F28%2F%E7%89%9B%E9%A1%BF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[图片来源 曲线函数为$ f(x)=(x-1)**2-2 $,起始点为x=4,进行4次迭代后x值为2.4142137800471977，接近真实值$ 1+ \sqrt{2} $ 实现代码 Ref：1.普林斯顿微积分读本2.https://www.cnblogs.com/shixiangwan/p/7532830.html]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[将sympy.plotting转换为matplotlib.pyplot]]></title>
    <url>%2F2018%2F06%2F27%2F%E5%B0%86sympy.plotting%E8%BD%AC%E6%8D%A2%E4%B8%BAmatplotlib.pyplot%2F</url>
    <content type="text"><![CDATA[# -*- coding: utf-8 -*- from sympy import symbols from sympy.plotting import plot, plot_implicit import matplotlib.pyplot as plt def move_sympyplot_to_axes(p, ax): backend = p.backend(p) backend.ax = ax backend.process_series() backend.ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) backend.ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) backend.ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) plt.close(backend.fig) x = symbols(&#39;x&#39;) p1 = plot((x**2,(x,-2,2)),(x,(x,-4,4)),legend=True, show=False) p1[1].line_color = &#39;r&#39; p2 = plot((1/x,(x,-4,4)),ylim=(-2,2),legend=True, show=False) fig,(ax,ax2) = plt.subplots(ncols=2) move_sympyplot_to_axes(p1,ax) move_sympyplot_to_axes(p2, ax2) plt.show()]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最大似然估计]]></title>
    <url>%2F2018%2F06%2F26%2F%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[若总体X属于离散型，其分布律$ P{X=x} = p(x;\theta),\theta \in \Theta $的形式为已知，$ \theta $为待估参数，$ \Theta $为$ \theta $可能取值的范围。设$ X{1},X{2},\cdot \cdot \cdot, X{n} $是来自X的样本，则$ X{1},X{2},\cdot \cdot \cdot, X{n} $的联合分布律为$ \prod{i=1}^n p(x{i};\theta) $ 又设$ x{1},x{2},\cdot \cdot \cdot, x{n} $是相应于样本$ X{1},X{2},\cdot \cdot \cdot, X{n} $的一个样本值。易知样本$ X{1},X{2},\cdot \cdot \cdot, X{n} $取到观察值$ x{1},x{2},\cdot \cdot \cdot, x{n} $的概率，亦即事件$ { X{1} = x{1}, X{2}=x{2},\cdot \cdot \cdot, X{n}=x{n}} $发生的概率为$ L(\theta)=L(x{1},x{2},\cdot \cdot \cdot,x{n};\theta) = \prod{i=1}^np(x{i};\theta), \theta \in \Theta $这一概率随$ \theta $的取值而变化，它是$ \theta $的函数，$ L(\theta) $称为样本的似然函数。最大似然估计法就是固定样本观察值$ x{1},x{2},\cdot \cdot \cdot, x{n} $,在$ \theta $的取值范围$ \Theta $内挑选使似然函数$ L(x{1},x{2},\cdot \cdot \cdot,x{n};\hat \theta)=\max{\theta \in \Theta}L(x{1},x{2},\cdot \cdot \cdot,x{n};\theta) $ 这样得到的$ \hat \theta $与样本值$ x{1},x{2},\cdot \cdot \cdot, x{n} $有关，常记为$ \hat \theta (x{1},x{2},\cdot \cdot \cdot, x{n}) $,称为参数$ \theta $的最大似然估计值，而相应的统计量$ \hat \theta (X{1},X{2},\cdot \cdot \cdot, X{n}) $称为参数$ \theta $的最大似然估计量。 若总体X属于连续型，其概率密度$ f(x;\theta),\theta \in \Theta $的形式为已知，$ \theta $为待估参数，$ \Theta $为$ \theta $可能取值的范围。设$ X{1},X{2},\cdot \cdot \cdot, X{n} $是来自X的样本，则$ X{1},X{2},\cdot \cdot \cdot, X{n} $的联合密度为$ \prod{i=1}^n f(x{i};\theta) $ 又设$ x{1},x{2},\cdot \cdot \cdot, x{n} $是相应于样本$ X{1},X{2},\cdot \cdot \cdot, X{n} $的一个样本值，则随机点($ X{1},X{2},\cdot \cdot \cdot, X{n} $)落在点$ x{1},x{2},\cdot \cdot \cdot, x{n} $的邻域(边长分别为$ dx{1},dx{2},\cdot \cdot \cdot, dx{n} $的n维立方体)内的概率近似的为$ \prod{i=1}^n f(x{i};\theta)dx{i} $其值随$ \theta $的取值而变化，与离散型的情况一样，我们取$ \theta $估计值$ \hat \theta $使概率取到最大值，但因子$ \prod{i=1}^{n}dx{i} $不随$ \theta $而变，故只需考虑函数$ L(\theta)=L(x{1},x{2},\cdot \cdot \cdot,x{n};\theta) = \prod{i=1}^nf(x{i};\theta) $的最大值。若$ L(x{1},x{2},\cdot \cdot \cdot,x{n};\hat \theta)=\max{\theta \in \Theta}L(x{1},x{2},\cdot \cdot \cdot,x{n};\theta) $，则称$ \hat \theta (x{1},x{2},\cdot \cdot \cdot, x{n}) $为$ \theta $的最大似然估计值，称$ \hat \theta (X{1},X{2},\cdot \cdot \cdot, X{n}) $称为参数$ \theta $的最大似然估计量 例：设$ X \sim b(1,p). X{1},X{2},\cdot \cdot \cdot,X_{n} $是来自X的一个样本，试求参数p的最大似然估计值。 最大似然估计就是通过已知的样本点来求得求似然函数取值最大的参数$ \theta $,通常采用牛顿法和梯度下降法求解。 Ref： 1.概率论与数理统计]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scipy相关函数]]></title>
    <url>%2F2018%2F06%2F25%2Fscipy-e7-9b-b8-e5-85-b3-e5-87-bd-e6-95-b0%2F</url>
    <content type="text"><![CDATA[scipy.special.expit # expit(x) = 1/(1+exp(-x)) scipy.misc.derivative import scipy.integrate as integrate r = integrate.quad(lambda x: 2*x, 0,2) #函数f(x) = 2*x在区间[0,2]内的积分]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[支持向量机--线性可分支持向量机]]></title>
    <url>%2F2018%2F06%2F23%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA--%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[支持向量机(support vector machines, SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题，支持向量机的学习算法是求解凸二次规划的最优化算法。 支持向量机学习方法包含由构建由简至繁的模型：线性可分支持向量机，线性支持向量机及非线性支持向量机。 给定线性可分线性训练数据集，通过间隔最大化或等价的求解相应的凸二次规划问题学习得到的分离超平面为 w^{*} \cdot x + b^{*} = 0 以及相应的分类决策函数f(x) = sign(w^{*} \cdot x + b^{*})称为线性可分支持向量机。 (函数间隔) 对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点$ (x{i},y{i}) $的函数间隔为 $ \hat{\gamma}{i} = y{i}(w \cdot x{i} + b) $ 定义超平面(w, b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点$ (x{i},y{i}) $的函数间隔之最小值，即 $ \hat{\gamma} = \min{i=1,\cdot \cdot \cdot,N} \hat{\gamma}_{i} $ (几何间隔) 对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点$ (x{i},y{i}) $的几何间隔为 $ \gamma{i} = y{i}\bigg(\frac{w}{||w||} \cdot x{i} + \frac{b}{||w||}\bigg) $ 定义超平面(w, b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点$ (x{i},y{i}) $的几何间隔之最小值，即 $ \gamma = \min{i=1,\cdot \cdot \cdot,N} \gamma_{i} $ 函数间隔和几何间隔有如下关系： $ \gamma{i} = \frac{\hat{\gamma}{i}}{||w||} $$ \gamma = \frac{\hat{\gamma}}{||w||} $ 间隔最大化 支持向量机的基本思想是求解能够正确划分训练数据集并且几何间隔最在的分离超平面. 对线性可分的训练数据集而言，线性可分分离超平面有无穷多个(等价于感知机),但是几何 间隔最大化的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化(与训练数据集近似线性可分时的软间隔最大化相对应) 凸优化问题是指约束最优化问题： $ \min{w} f(w) $ s.t. $ g{i}(w) \le 0, i=1,2,\cdot \cdot \cdot,k $$ h{i}(w) = 0, i=1,2,\cdot \cdot \cdot,l $ 其中，目标函数f(w)和约束函数$ g{i}(w) $都是$ R^{n} $上的连续可微的凸函数，约束函数$ h{i}(w) $是$ R^{n} $上的仿射函数。当目标函数f(w)是二次函数且约束函数$ g{i}(w) $是仿射函数时，上述凸最优化问题成为凸二次规划问题。 线性可分支持向量机学习算法—-最大间隔法输入：线性可分训练数据集$ T = {(x{1},y{1}),(x{2},y{2}),\cdot \cdot \cdot, (x{N},y{N})} $,其中，$ x{i} \in \mathcal=R^{n},y{i}={-1,+1}, i=1,2,\cdot \cdot \cdot,N $;输出：最大间隔分离超平面和分类决策函数(1)构造并求解约束最优化问题：$ \min{w,b}^{} \frac{||w||^2}{2} $s.t.$ y{i}(w \cdot x_{i}+b)-1 \ge 0, i=1,2,\cdot \cdot \cdot,N $ 求得最优解w^{*},b^{*} (2)由此得到的分离超平面： w^* \cdot x + b^* = 0分类决策函数： f(x) = sign(w^* \cdot x + b^*) (定理) (最大间隔分离超平面的存在唯一性)若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。 例：已知一个如下图所示的训练数据集，其正例点是$ x{1}=(3,3)^T,x{2}=(4,3)^T $,负实例点是$ x{3}=(1, 1)^T $,试求最大间隔分离超平面。 图片代码 按照最大间隔法，根据训练数据集构造约束最优化问题：$ \min{w,b} \frac{1}{2}(w{1}^2+w{2}^2) $s.t.$ 3w{1} + 3w{2}+b \ge 1 $$ 4w{1} + 3w{2}+b \ge 1 $$ -w{1} - w{2}-b \ge 1 $ import cvxpy as cvx w1 = cvx.Variable() w2 = cvx.Variable() b = cvx.Variable() #约束条件 constraints = [3*w1+3*w2+b&gt;=1,4*w1+3*w2+b&gt;=1,-w1-w2-b&gt;=1] #目标函数 obj = cvx.Minimize((w1**2+w2**2)/2) prob = cvx.Problem(obj, constraints) prob.solve() print(&quot;w1=&quot;,w1.value) print(&quot;w2=&quot;,w2.value) print(&quot;b=&quot;,b.value) 求解得最优化解为$ w{1}=w{2}=\frac{1}{2}, b=-2 $,最大间隔分离超平面为$ \frac{1}{2}x^{(1)} +\frac{1}{2}x^{(1)}-1=0 $，其中 $ x{1}=(3,3)^T,x{3}=(1,1)^T $为支持向量。 学习的对偶算法未完待续 Ref：1.统计学习方法2.cvxpy]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯法]]></title>
    <url>%2F2018%2F06%2F23%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.条件概率(定义): 设A，B两 个事件，且P(A)&gt;0,称 $ P(B|A) = \frac{P(AB)}{P(A)} $为事件A发生的条件下事件B发生的概率。 2.全概率公式： 设试验E的样本空间为S， A为E的事件，$ B{1},B{2}, \cdot \cdot \cdot,B{n} $为S的一个划分，且$ P(B{i})&gt;0 (i=1,2,\cdot \cdot \cdot,n) $,则 $ P(A) = P(A|B{1})P(B{1})+P(A|B{2})P(B{2})+\cdot \cdot \cdot + P(A|B{n})P(B{n}) $，称为全概率公式。 3.贝叶斯公式：设试验E的样本空间为S。A为E的事件，$ B{1},B{2}, \cdot \cdot \cdot,B{n} $为S的一个划分,且$ P(A)&gt;0,P(B{i})&gt;0 (i=1,2,\cdot \cdot \cdot,n) $,则 $ P(B{i}|A) = \frac{P(A|B{i})P(B{i})}{\sum{j=1}^nP(A|B{j})P(B{j})},i=1,2, \cdot \cdot \cdot, n. $称为贝叶斯公式。 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入x,利用贝叶斯定理求出后验概率大的输出y。朴素贝叶斯法实现简单，学习与预测的效率都很高，是种常用的方法。 朴素贝叶斯算法输入：训练数据$ T = {(x{1},y{1}),(x{2},y{2}),\cdot \cdot \cdot,(x{N},y{N})} $,其中$ x{i} = (x{i}^{(1)},x{i}^{(2)},\cdot \cdot \cdot, x{i}^{(n)})^T $,$ x{i}^{(j)} $是第i个样本的第j个特征，$ x{i}^{(j)} \in {a{j1},a{j2},\cdot \cdot \cdot,a{js_j}} $,$ a\{jl} $是第j个特征可能取的第l个值，$ j = 1,2,\cdot \cdot \cdot,n, l= 1, 2,\cdot \cdot \cdot, Sj, y{i} \in {c{1},c{2}, \cdot \cdot \cdot ,c{k}} $;实例x;输出：实例x的分类。（1）计算先验概率及条件概率$ P(Y=c{k})=\frac{\sum{i=1}^{N}I(y{i}=c{k})}{N}, k=1,2,\cdot \cdot \cdot,K $ $ P(X^{(j)}=a{jl}|Y=c{k})=\frac{\sum{i=1}^{n}I(x{i}^{(j)}=a{jl},y{i}=c{k})}{\sum{i=1}^{N}I(y{i}=c{k})} $ $ j=1,2,\cdot \cdot,\cdot,n;l=1,2,\cdot \cdot \cdot,S{j};k=1,2,\cdot \cdot \cdot, K $ (2)对于给定的实例$ x=(x^{(1)},x^{(2)},\cdot \cdot \cdot,x^{(n)})^T $,计算$ P(Y=c{k})\prod{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c{k}),k=1,2,\cdot \cdot \cdot, K $ (3)确定实例x的类$ y = arg \max{c{k}}P(Y=c{k})\prod{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c{k}) $ 实现代码 # 对数据集进行去重 def createVecabList(dataSet): vocabSet = set([]) for doc in dataSet: vocabSet = vocabSet | set(doc) return list(vocabSet) # 将数据集中出现的word，映射到去重后的数据集中，如果出现则为1，否则为0 def setOfWords2Vec(vocabList, inputSet): retVec = [0] * len(vocabList) for word in inputSet: if word in vocabList: retVec[vocabList.index(word)] = 1 else: print(&quot;the word: %s is not in my vocabulary!&quot; % word) return retVec # 计算p(wi|c0)，p(wi|c1)，pAbusive def trainNB0(trainMatrix, trainCategory): numTrainDocs = len(trainMatrix) print(numTrainDocs) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) p0Num = np.zeros(numWords) p1Num = np.zeros(numWords) p0Denom, p1Denom = 0.0, 0.0 for i in range(numTrainDocs): if trainCategory[i] == 1: p1Num += trainMatrix[i] print(trainMatrix[i]) p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) print(p1Denom) p1Vect = p1Num/p1Denom p0Vect = p0Num/p0Denom return p0Vect, p1Vect, pAbusive def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1): p1 = sum(vec2Classify * p1Vec) + log(pClass1) p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 def testingNB(): listPosts, listClasses = loadDataSet() myVocabList = createVecabList(listPosts) trainMat = [] for p in listPosts: trainMat.append(setOfWords2Vec(myVocabList, p)) p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses)) testEntry = [&#39;love&#39;, &#39;my&#39;, &#39;dalmation&#39;] thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,&#39;classified as: &#39;,classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = [&#39;stupid&#39;, &#39;garbage&#39;] thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry)) print(testEntry,&#39;classified as: &#39;,classifyNB(thisDoc,p0V,p1V,pAb)) 完整代码 Ref：1.机器学习实战2.统计学习方法]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[决策树-ID3算法]]></title>
    <url>%2F2018%2F06%2F23%2F%E5%86%B3%E7%AD%96%E6%A0%91-ID3%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[决策树分类决策树模型是一 种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。 决策树学习常用的算法有ID3,C4.5,CART。 特征选择特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。 信息增益在信息论与概率统计中，熵(entropy)表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：$ P(X=x{i}) = p{i}, i=1,2, \cdot \cdot \cdot, n $ 则随机变量X的熵定义为$ H(X)=-\sum{i=1}^{n}p{i} \log p_{i} $ ,通常式中的对数以2为底或以e为底。熵越大，随机变量的不确定性就越大。 (信息增益：定义) 特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$ g(D,A)= H(D)-H(D|A) $熵H(Y)与条件熵H(Y|X)之差称为互信息。 from scipy.stats import entropy from collections import Counter # 计算熵 def calcEnt(dataSet): num = len(dataSet) labelCount = {} for featVec in dataSet: label = featVec[-1] if label not in labelCount: labelCount[label] = 0 labelCount[label] += 1 Ent = 0.0 for k in labelCount: prob = float(labelCount[k])/num Ent -= prob * log(prob, 2) return Ent def calcEnt2(dataSet): dataSet = np.array(dataSet) num = len(dataSet) labelCount = Counter(dataSet[:,-1]) #使用Counter简化代码 prod = [float(labelCount[k])/num for k in labelCount] Ent = entropy(prod, base=2) # scipy中提供计算熵的函数 return Ent # 对数据集进行切割 def splitData(dataSet, axis, value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet # 选择信息增益最大的特征 def chooseBestFeature(dataSet): numFeatures = len(dataSet[0]) - 1 # 特征的数量 hd = calcEnt2(dataSet) bestInfoGain, bestFeature = 0.0, -1 for i in range(numFeatures): featList = [elt[i] for elt in dataSet] #获取特征列表 uniqueVals = set(featList) #对特征值进行去重 newEntropy = 0.0 for v in uniqueVals: # 切割数据计算熵 subDataSet = splitData(dataSet, i, v) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob * calcEnt2(subDataSet) infoGain = hd - newEntropy # 计算信息增益 print(infoGain) if infoGain &gt; bestInfoGain: bestInfoGain = infoGain bestFeature = i return bestFeature # 返回出现次数最多的那项 def majorityCnt(classList): classCount = {} for vote in classList: if vote not in classCount: classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] # 使用字典构造决策树 def createTree(dataSet, labels): classList = [elt[-1] for elt in dataSet] #获取分类 # 如果只有一分类，则只有一个根节点 if classList.count(classList[0]) == len(classList): return classList[0] # 当数据为一维时，返回出现次数最多的项 if len(dataSet[0]) == 1: print(&#39;dataSet&#39;, dataSet) return majorityCnt(classList) bestFeat = chooseBestFeature(dataSet) bestFeatLabel = labels[bestFeat] myTree = {bestFeatLabel: {}} del(labels[bestFeat]) featValues = [elt[bestFeat] for elt in dataSet] uniqueVals = set(featValues) print(&#39;labels=&#39;, labels) for v in uniqueVals: subLabels = labels[:] print(&#39;subLabels&#39;,subLabels) # 使用递归法构建决策树 myTree[bestFeatLabel][v] = createTree(splitData(dataSet, bestFeat, v), subLabels) return myTree # 使用构建的决策树进行分类 def classify(inputTree, featLabels, testVec): firstStr = list(inputTree.keys())[0] secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) print(&#39;featIndex&#39;, featIndex) print(&#39;secondDict&#39;, secondDict) for k in secondDict: if testVec[featIndex] == k: if isinstance(secondDict[k], dict): classLabel = classify(secondDict[k], featLabels, testVec) else: classLabel = secondDict[k] return classLabel Ref：1.机器学习实战2.统计学习方法—李航]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[矩阵相关概念]]></title>
    <url>%2F2018%2F06%2F20%2F%E7%9F%A9%E9%98%B5%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[单位矩阵在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1，这种矩阵被称为单位矩阵。它是个方阵，从左上角到右下角的对角线（称为主对角线）上的元素均为1。除此以外全都为0。 逆矩阵一个n阶 方阵 A称为可逆的，或非奇异的，如果存在一个n阶方阵B，使得 AB = BA = E 并称B是A的一个逆矩阵。E为单位矩阵。A的逆矩阵记作$ A^{-1} $ 。当|A|=0时，A称为奇异矩阵，否则称为非奇异矩阵。 可逆矩阵又称满秩矩阵，不可逆矩阵(奇异矩阵)又称降秩矩阵。 np.linalg.inv(A) #求A的逆矩阵，若矩阵A可逆，则|A|!=0 正定矩阵 对称阵A为正定矩阵充分必要条件是，A的特征值全部&gt;0。 对称阵A为半正定矩阵充分必要条件是，A的特征值全部&gt;=0。 矩阵的转置把矩阵A的行换成同序数的列得到一个新矩阵，叫做A的转置矩阵，记住$ A^T $ 。 矩阵的转置也是一种运算。 $ (A^{T})^{T} = A $$ (A+B)^{T} = A^{T} + B^{T} $$ (\lambda A)^{T} = \lambda A^{T} $$ (AB)^{T} = B^T A^T $ 对称矩阵设A为n阶方阵，如果满足$ A^T = A $ ,那么A称为对称矩阵，简称对称阵。 共轭矩阵当$ A = (a{ij}) $ 为复矩阵时，用$ \bar {a{ij}} $ 表示$ a{ij} $ 的共轭复数，记$ \bar A = (\bar {a{ij}}) $ ,$ \bar A $ 称为$ A $ 的共轭矩阵。 共轭矩阵满足下述运算规律(设A，B为复矩阵，$ \lambda $ 为复数，且运算都是可行的)；$ \bar {A+B} = \bar A + \bar B $$ \bar {\lambda A} = \bar \lambda \bar A $$ \bar {AB} = \bar A \bar B $ a.H为矩阵a的共轭转置(先求矩阵a的共轭矩阵，然后再进行转置) 方阵行列式由n阶方阵A的元素所构成的行列式(各元素的位置不变),称为方阵A的行列式，记作|A|或det A。 正交矩阵如果n阶矩阵A满足$ A^T A = E $ 即$ A^{-1} = A^T $ ,那么称A为正交矩阵，简称正交阵. 特征向量设A是n阶矩阵，如果数$ \lambda $ 和n维非零列向量x使关系式$ Ax=\lambda x $ 成立，那么，这样的数$ \lambda $ 称为矩阵A的特征值，非零向量x称为A的对应于特征值$ \lambda $ 的特征向量。 4和2是特征值，后面是特征值所对应的特征向量。 矩阵的迹矩阵的迹为矩阵对角线各元素之和。 克拉默法则 两个矩阵行数和列数相同，则称为同型矩阵。元素都为零的矩阵称为零矩阵 若一n行n列的复数矩阵$ U $ 满足：$ U^H U= UU^H = E{n} $其中，$ U^H $ 为$ U $ 的共轭转置，$ E{n} $ 为n阶单位矩阵，则$ U $ 称为酉矩阵(幺正矩阵)。 一个简单的充分必要判断准则是：$ U^{-1} = U^H $ ,即酉矩阵的共轭转置和它的逆矩阵相等。 奇异值分解假设M是一个$ m $ x$ n $ ,如果存在一个分解使得$ M=U \sum V^ $其中U是$ m $ x$ m $ 阶酉矩阵；$ \sum $ 是半正定$ m $ x$ n $ 阶对角矩阵；而$ V^ $ 为共轭转置，是$ n $ x$ n $ 阶酉矩阵。这样的分解就称作M的奇异值分解。$ \sum $ 的对角线上的元素为M的奇异值。 对角矩阵对角矩阵(diagonal matrix)是一个主对角线之外的元素皆为0的矩阵，常写为diag（a1，a2,…,an)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[将图像转换为excel表格]]></title>
    <url>%2F2018%2F06%2F15%2F%E5%B0%86%E5%9B%BE%E5%83%8F%E8%BD%AC%E6%8D%A2%E4%B8%BAexcel%E8%A1%A8%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[原理：将图像中的每一个点映射到excel中的每一个单元格 安装：PIL, openpyxl 步骤： 1.使用PIL读取图像 2.使用openpyxl将一个个像素点写入到excel文件 中 # -*- coding: utf-8 -*- # 将图像转换为excel文件，一个像素点对应一个单元格 import re import openpyxl from PIL import Image from openpyxl.styles import PatternFill # 因为通过PIL获取到的像素点是元组，所以将元组转换为FFEEFFDD格式的字符串 def tuple2str(t): lst = [&#39;{0:#0{1}x}&#39;.format(elt, 4) for elt in t] s = &#39;&#39;.join(lst) return re.sub(&#39;0x&#39;, &#39;&#39;, s).upper() # 设置填充的颜色 def fill_color(s): color = PatternFill(fgColor=s, bgColor=&#39;00000000&#39;,fill_type=&#39;solid&#39;) return color wb = openpyxl.Workbook() ws = wb.active im = Image.open(&#39;t.jpg&#39;) # 循环将像素点写入excel for i in range(0,im.size[-1]): for j in range(0,im.size[0]): pixel = im.getpixel((j,i)) pixel_str = tuple2str(pixel) print(i,j,pixel_str) color = fill_color(pixel_str) d = ws.cell(row=i+1, column=j+1) d.fill = color for col in ws.columns: column = col[0].column ws.column_dimensions[column].width=1 for row in ws.rows: row = row[0].row ws.row_dimensions[row].height=1 wb.save(&#39;out.xlsx&#39;) 完整代码 输入图像为： 输出excel: Ref： 一则新闻]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k近邻法-k_Nearest Neighbors]]></title>
    <url>%2F2018%2F06%2F10%2Fk%E8%BF%91%E9%82%BB%E6%B3%95-k_Nearest%20Neighbors%2F</url>
    <content type="text"><![CDATA[k近邻法(knn)就一种基本分类与回归方法。 from scipy.spatial import minkowski_distance def classify0(inX, dataSet, labels, k): # dataSetSize = dataSet.shape[0] # diffMat = np.tile(inX, (dataSetSize, 1)) - dataSet # sqDiffMat = diffMat **2 # sqDistances = sqDiffMat.sum(axis=1) # distances = sqDistances**0.5 # print(&#39;distances&#39;, distances) distances = minkowski_distance(inX, dataSet) #计算inX与与dataSet各个点的距离，返回个array sortedDistIndicies = distances.argsort() #返回排序后的元素下标 classCount = {} for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] #获得对应的label classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 # 对classCount根据键值进行排序 sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] # 将txt转换为矩阵 def file2matrix2(filename): data = np.genfromtxt(filename, dtype=&#39;|U8&#39;) returnMat = data[:, 0:-1].astype(&#39;float64&#39;) return returnMat, data[:,-1] # 数据进行规格化处理 def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = np.zeros(dataSet.shape) m = dataSet.shape[0] normDataSet = dataSet - np.tile(minVals, (m, 1)) normDataSet = normDataSet/np.tile(ranges, (m,1)) return normDataSet, ranges, minVals # 这里使用了scikit-learn中的API,结果与autoNorm是一样的 def autoNorm2(dataSet): from sklearn.preprocessing import normalize minVals = dataSet.min(0) maxVals = dataSet.max(0) normDataSet = normalize(dataSet, norm=&#39;max&#39;,axis=0) return normDataSet, maxVals-minVals, minVals # 将txt文件转换为向量 def img2vec(filename): rVec = np.zeros((1,1024)) f = open(filename) for i in range(32): line = f.readline() for j in range(32): rVec[0, 32*i+j] = int(line[j]) return rVec # 对手写数字进行测试 def handwritingClassTest(): hwLabels = [] trainFileList = os.listdir(&#39;digits/trainingDigits/&#39;) m = len(trainFileList) trainingMat = np.zeros((m, 1024)) for i in range(m): filename = trainFileList[i] className = int(filename.split(&#39;_&#39;)[0]) hwLabels.append(className) trainingMat[i,:] = img2vec(&#39;digits/trainingDigits/%s&#39; % filename) testFileList = os.listdir(&#39;digits/testDigits&#39;) accuracyCount = 0.0 mTest = len(testFileList) for i in range(mTest): filename = testFileList[i] className = int(filename.split(&#39;_&#39;)[0]) vecTest = img2vec(&#39;digits/testDigits/%s&#39; % filename) classifierResult = classify0(vecTest, trainingMat, hwLabels, 1) if classifierResult == className: accuracyCount += 1 print(&quot;the total accuracy rate is: %f&quot; % (accuracyCount/mTest)) 完整代码 在正式进行数据挖掘之前，尤其是使用基于对象距离的 挖掘算法时，如：神经网络、最近邻分类等，必须 进行数据规格化。也就是将其缩至特定的范围之内，如：[0,10]。如：对于一 个顾客信息数据库中的年龄属性或工资属性，由于工资属性的取值比年龄属性的 取值要大许多，如果不进行规格化处理，基于工资属性的距离计算值显然将远超 过基于年龄属性的距离计算值，这就意味着工资属性的作用在整个数据对象的距 离计算中被错误地放大了。 规格化就是将一个属性取值范围投射到一个特定范围之内，以消除数值型属 性因大小不一而造成挖掘结果的偏差。规划化处理常常用于神经网络、基于距离 计算的最近邻分类和聚类挖掘的数据预处理。 。对于神经网络，采用规格化后的数据不仅有助于确保学习结果的正确性，而且也会帮助提高学习的速度。对于基于距离计算的挖掘，规格化方法可以帮助消除因属性取值范围不同而影响挖掘结果 的公正性。 三种规格化方法： 1.最大最小规格化方法 该方法对初始数据进行一种线性转换。设$ min{A} $和$ max{A} $为属性A的最小和最大值。最大最小规格化方法将属性A的一个值映射为$ v^{‘} $且有$ v^{‘} \in [newmin{A},newmax{A}] $,具体映射公式如下： $ v^{‘} = \frac{v-min{A}}{max{A}-min{A}} (new_max{A}-new_min_{A})+new_min_{A} $ 2.零均值规格化方法。该方法是根据属性A的均值和偏差来对A进行规格化。属性A的v值可以通过以下计算公式获得其映射值$ v^{‘} $ $ v^{‘} = \frac{v-\bar A}{\sigma{A}} $其中的$ \bar A $和$ \sigma{A} $分别为属性A的均值和方差。这种规格化方法常用于属性A最大值与最小值未知，或使用最大最小规格化方法时会出现异常数据的情况。 3.十基数变换规格化方法。 该方法通过移动属性A值的小数位置来达到规格化的目的。把移动的小数位取决于属性a绝对值的最大值。属性A的v值可以通过以下计算公式获得其映射值$ v^{‘} $。 $ v^{‘} = \frac{v}{10^{j}} $ 其中j为使$ \max(|v^{‘}|) \lt 1 $成立的最小值。 Ref：1.统计学习方法-李航2.数据挖掘导论3.机器学习实战]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[感知机算法的对偶形式]]></title>
    <url>%2F2018%2F06%2F08%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E5%81%B6%E5%BD%A2%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[class PerceptionPair: #初始化一些参数学习率，gram矩阵等等 7 def __init__(self, dataSet, target, learningrate=1): 8 self.lr = learningrate 9 self.a = np.zeros(len(dataSet), np.float) 10 self.b = 0.0 11 self.gram = np.matmul(np.array(dataSet), np.array(dataSet).T) 12 self.target = target 13 #计算判断是否误分类的值 14 def calc(self,i): 15 res = np.dot(self.a*self.target, self.gram[i]) 16 res = (res+self.b)*target[i] 17 return res 18 19 def train(self, inputs, target): 20 flag = False 21 m = len(inputs) 22 res = 0.0 23 for i in range(m): # 进行权值更新 24 if self.calc(i) &lt;= 0: 25 self.a[i] += self.lr 26 self.b += target[i] 27 print(self.a, self.b) 完整代码 Ref：1.统计学习方法-李航]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[感知机算法的原始形式]]></title>
    <url>%2F2018%2F06%2F08%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E5%A7%8B%E5%BD%A2%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[使用例2.1的数据点作为输入。 class Perception: def __init__(self, learningrate): self.lr = learningrate # 学习率 self.wh = np.array([0.0, 0.0]) #初始化wh = [0.0, 0.0] self.b = 0 def train(self,inputs, target): inputs = np.asarray(inputs) if self.check(inputs,target): # 如果被误分类，则更新wh,b self.wh += self.lr * np.dot(inputs, target) self.b += target * self.lr print(self.wh, self.b) #判断是否被正确分类 def check(self, inputs, target): flag = False res = 0.0 res += (np.dot(inputs, self.wh)+self.b)*target if res &lt;=0: flag = True return flag 例2.1的迭代过程为 程序输出迭代过程为： 完整代码 scikit-learn实现from sklearn.linear_model import Perceptron # max_iter为最大迭代次数，eta0为学习率 perceptron = Perceptron(max_iter=1000, eta0=1) # coef_init设置w初始向量，intercept_init设置初始参数b p_fit = perceptron.fit(x, y, coef_init=np.array([[0.0, 0.0]]).reshape(-1,1),intercept_init=np.array([0]),sample_weight=np.array([1,1,1])) # 对未知数据进行预测 p_fit.predict(np.array([4,4]).reshape(1,-1)) Ref：1.统计学习方法-李航2.scikit-learn官方文档]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[感知机-简介]]></title>
    <url>%2F2018%2F06%2F08%2F%E6%84%9F%E7%9F%A5%E6%9C%BA-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[定义(感知机) 假设输入空间(特征空间)是$ \chi \in \mathbf {R}^{n} $,输出空间是$ \mathcal{Y} ={+1,-1} $.输入$ x \in \chi $表示实例的特征向量，对应于输入空间的点，输出$ y \in \mathcal{Y} $表示实例的类别.由输入空间到输出空间的如下函数 $ f(x) = sign(w\cdot x + b) $ 称为感知机.其中，w和b为感知机模型参数，$ w \in \mathbf {R}^{n} $叫作权值或权值向量，$ b \in \mathbf{R} $叫作偏置，$ w \cdot x $表示w和x的内积。sign是符号函数，即 sign(x)= \begin{cases} +1,& {x \geq 0}\ \\ -1,& {x \le 0} \end{cases} 感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$ {f|f(x)=w \cdot x + b} $ 感知机学习策略 定义(数据集的线性可分性) 给定一个数据集 $ T = {(x{1},y{1}),(x{2},y{2}),\cdot \cdot \cdot,(x{N},y{N})} $,其中，$ x{i} \in \chi=\mathcal R^{n} $,$ y{i} \in \chi ={+1,-1} i=1,2,\cdot \cdot \cdot,N $,如果存在某个超平面S $ w \cdot x +b = 0 $ 能够将数据集的正实例点和负实例点完全正确的划分到超平面的 两侧，即对所有$ y{i}=+1 $的实例i,有$ w \cdot x{i} &gt; 0 $,对所有$ y{i}=-1 $的实例i，有$ w \cdot x{i} \lt 0 $,则称数据集T为线性可分数据集；否则，称数据集T线性不可分。 假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。感知机采用误分类点到超平面S的总距离作为损失函数。 输入空间中任一点到超平面S的距离： ||w||是w的L2范数。 Ref：1.统计学习方法-李航]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zipfile乱码问题]]></title>
    <url>%2F2018%2F05%2F31%2Fzipfile%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[python3.5使用zipfile.ZipFile(“xx.zip”)读取压缩文件为中文的文件时出现乱码 zipfile检测文件编码，如果是’utf-8’编码则解码为’utf-8’,否则解码为’cp437’,修改zipfile.py将’cp437’改为’gbk’即可。 import zipfile import os os.path.dirname(zipfile.__file__) #查看zipfile.py文件所在路径 if flags &amp; 0x800: # UTF-8 file names extension filename = filename.decode(&#39;utf-8&#39;) else: # Historical ZIP filename encoding filename = filename.decode(&#39;cp437&#39;) #改为filename = filename.decode(&#39;gbk&#39;) if zinfo.flag_bits &amp; 0x800: # UTF-8 filename fname_str = fname.decode(&quot;utf-8&quot;) else: fname_str = fname.decode(&quot;cp437&quot;) #改为fname_str = fname.decode(&quot;gbk&quot;) Ref 1.https://blog.csdn.net/tian544556/article/details/78635840]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[连接docker创建的mysql容器时出错]]></title>
    <url>%2F2018%2F05%2F29%2F%E8%BF%9E%E6%8E%A5docker%E5%88%9B%E5%BB%BA%E7%9A%84mysql%E5%AE%B9%E5%99%A8%E6%97%B6%E5%87%BA%E9%94%99%2F</url>
    <content type="text"><![CDATA[连接mysql容器时报错如下 ERROR 2059 (HY000): Authentication plugin ‘caching_sha2_password’ cannot be loaded: /usr/lib/mysql/plugin/caching_sha2_password.so: cannot open shared object file: No such file or directory 原因是docker pull下来的是最新的mysql镜像，移除镜像，重新拉取5.7版的镜像即可 docker pull mysql:5.7]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[scipy entropy求熵]]></title>
    <url>%2F2018%2F05%2F23%2Fscipy%20entropy%E6%B1%82%E7%86%B5%2F</url>
    <content type="text"><![CDATA[熵(entropy)是表示随机变量不确定性的度量,熵越大,随机变量的不确定性就越大。 设X是一个取有限个值的离散随机变量，其概率分布为 [latex]P(X=x{i}) = p{i}, i=1,2,\cdot \cdot \cdot,n[/latex] 则随机变量X的熵定义为 [latex]H(X) = -\sum{i=1}^n p{i}\log {p_{i}}[/latex] 对数以2或e为底 scipy.stats.entropy(pk, qk=None,base=None) 计算给定概率值的分布的熵 In [213]: from scipy import stats In [215]: stats.entropy([9/15,6/15],base=2) Out[215]: 0.9709505944546688 Ref：1.scipy文档2.统计学习方法]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas数据类型DataFrame及基本操作]]></title>
    <url>%2F2018%2F05%2F23%2Fpandas%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8BDataFrame%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[import pandas as pd df = pd.DataFrame() df[:n] #取前n行 df.loc[行标签,列标签] df.loc[&#39;a&#39;:&#39;b&#39;]#选取ab两行数据,ab为行名 df.loc[:,&#39;one&#39;]#选取one列的数据 df.iloc(n) #取第n行 df.iloc[:,0] #取第0列 df.iloc[:,[0,1]] #取第0列和第1列 df2 = df.fillna(pd.NaT) df.to_excel(&#39;foo.xlsx&#39;, sheet_name=&#39;Sheet1&#39;) #写入Excel df.dropna(how=&#39;any&#39;) #当有一项为NaN时，丢弃整行或整列 df.dropna(how=&#39;all&#39;) #当所有项都为NaN时，丢弃整行或整列 df.describe() #展示df的相关信息 # 创建DataFrame In [63]: import pandas as pd In [64]: d ={&#39;A&#39;:[1,2,3],&#39;B&#39;:[4,5,6]} In [65]: df = pd.DataFrame(df,index=[&#39;x&#39;,&#39;y&#39;,&#39;z&#39;]) In [67]: df Out[67]: A B x 1 4 y 2 5 z 3 6]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy相关函数]]></title>
    <url>%2F2018%2F05%2F21%2Fnumpy%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[import numpy as np import scipy as sp a = np.ones((2, 3),dtype=int) a.sum(axis=0)#按列相加 a.sum(axis=1)#按行相加 a.ravel()#转为一维 b = a.reshape #返回一个新的修改后的数组,若修改b中的值,a中相应的值也会改变，若要真正的拷贝改为b = a.reshape().copy() a.resize#直接修改数组本身,返回值为None a.ndim #返回矩阵的维度 a.shape #返回矩阵的大小(行数, 列数) np.argmax(a) #返回最大值的下标 np.isnan(a) #返回和a一样大小的矩阵，若为nan则为True,否则为False a[~np.isnan(a)] #返回过滤nan元素后的矩阵 np.nan != np.nan is true sp.sum(np.isnan(a)) # 返回元素值为nan的个数 np.linspace(start,stop,num) #返回在区间start--stop中的num个点 np.logspace(start, stop, num=50, endpoint=True, base=10.0) #返回在区间base**start--base**stop中的num个点 a[:,0] #取a的第0例 d = a[:,0:-1] #除去最后一列 d.min(0) #取每列的最小值 d.min(1) #取每行的最大值 d[i,:] #取第i行 d[100:1000,:] #取[100,1000)行 np.eye #对角矩阵,只有对角线上有非0元素的矩阵称为对角矩阵 np.identity #同上 np.linalg.inv(a) #求矩阵a的逆矩阵 np.trace #矩阵的迹(即对角线元素之和) numpy.matrix.getH #共轭转置 np.ones #返回一个矩阵元素全为1 numpy.linalg.det #计算矩阵的行列式 np.random.uniform(low=0.0, high=1.0, size=None) #从均匀分布中取样,size为返回array元素的个数 np.linalg.svd #奇异值分解 np.mat #将输入转换为矩阵对象 np.transpose #矩阵转置,np.transpose(a)相当于a.T np.sign(x) #T函数返回 -1 if x &lt; 0, 0 if x==0, 1 if x &gt; 0. nan if x is nan. np.matmul # 两个矩阵之间的积 numpy.random.normal #生成高斯矩阵 numpy.random.choice #从序列中随机选取 numpy.asarray #将python list转换为numpy array np.unique(a) #返回去重后的array rng = np.random.RandomState(0) X = rng.rand(10, 2000) In:np.linspace(0,10,9) #在区间[0,10]中,返回含有9个元素的等差数组 Out:array([ 0. , 1.25, 2.5 , 3.75, 5. , 6.25, 7.5 , 8.75, 10. ]) In:np.arange(0,10,9) #返回在区间[0,10],步长为9的数组 Out: array([0, 9]) In [43]: b = np.array([[1,2,3],[4,5,6]]) In [44]: b.reshape(3,-1) #-1会被自动识别为2 Out[44]: array([[1, 2], [3, 4], [5, 6]]) In [45]: b.reshape(-1,3) #-1会被自动识别为2 Out[45]: array([[1, 2, 3], [4, 5, 6]]) np.var #方差 In [126]: a = np.array([[1, 2], [3, 4]]) In [127]: np.var(a) #求展开后所有数的方差 Out[127]: 1.25 In [128]: np.var(a, axis=0) #按列计算方差 Out[128]: array([1., 1.]) In [129]: np.var(a, axis=1) #按行计算方差 Out[129]: array([0.25, 0.25]) np.cov #协方差 np.tile(A, reps) 根据reps重复A生成一个矩阵,reps为(行数,列数),可把A看作一个元素 In [165]: a = np.array([1,2,3]) In [166]: d = np.tile(a,(3,2)) In [167]: d Out[167]: array([[1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]]) # 求矩阵a的秩 In [3]: a Out[3]: array([[ 2, -1, 0, 3, -2], [ 0, 3, 1, -2, 5], [ 0, 0, 0, 4, -3], [ 0, 0, 0, 0, 0]]) In [4]: np.linalg.matrix_rank(a) Out[4]: 3 # nonzero 返回非零元素的下标 In [1]: a = np.array([0,2,0]) In [2]: np.nonzero(a) Out[2]: (array([1]),) In [3]: a[np.nonzero(a)] # 取数组a的非零元素 Out[3]: array([2]) In [4]: x = np.array([[1, 0, 0], [0, 2, 0], [1, 1, 0]]) # 二维数组 In [5]: x Out[5]: array([[1, 0, 0], [0, 2, 0], [1, 1, 0]]) In [6]: np.nonzero(x) Out[6]: (array([0, 1, 2, 2]), array([0, 1, 0, 1])) # np.multiply(x, y) == x * y In [120]: x = np.array([1, 2, 3]) In [121]: y = np.array([4, 5, 6]) In [122]: x *y Out[122]: array([ 4, 10, 18]) In [123]: np.multiply(x, y) Out[123]: array([ 4, 10, 18]) # np.mean 求均值 &gt;&gt;&gt; a = np.array([[1, 2], [3, 4]]) #求所有数的均值 &gt;&gt;&gt; np.mean(a) 2.5 &gt;&gt;&gt; np.mean(a, axis=0) # 对列求均值 array([ 2., 3.]) &gt;&gt;&gt; np.mean(a, axis=1) # 对行求均值 array([ 1.5, 3.5]) In [132]: a = np.array([[1, 2], [3, 4]]) In [132]: ma = np.mat(a) In [133]: ma Out[133]: matrix([[1, 2], [3, 4]]) In [134]: ma.A # 返回array形式 Out[134]: array([[1, 2], [3, 4]])]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[皮尔逊相关系数]]></title>
    <url>%2F2018%2F05%2F20%2F%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[皮尔逊相关系数 两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差和标准差的商上式定义了总体相关系数，常用希腊小写字母 作为代表符号。估算样本的协方差和标准差，可得到皮尔逊相关系数，常用英文小写字母 代表 $ \rho{X,Y}=\frac{cov(X,Y)}{\sigma{X}\sigma{Y}}=\frac{E[(X-\mu{x})(Y - \mu{Y})]}{\sigma{X}\sigma{Y}} $ $ r = \frac{\sum{i=1}^n(X{i}-\overline X)(Y{i}-\overline Y)}{\sqrt{\sum{i=1}^n(X{i}-\overline X)^2}\sqrt{\sum{i=1}^n(Y{i}-\overline Y)^2}} $ from scipy import stats import numpy as np a = np.array([0, 0, 0, 1, 1, 1, 1]) b = np.arange(7) stats.pearsonr(a,b) (0.8660254037844386, 0.011724811003954654)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在客户机添加路由访问docker集群服务]]></title>
    <url>%2F2018%2F05%2F17%2F%E5%9C%A8%E5%AE%A2%E6%88%B7%E6%9C%BA%E6%B7%BB%E5%8A%A0%E8%B7%AF%E7%94%B1%E8%AE%BF%E9%97%AEdocker%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[在局域网(网段为10.68.2.0/24)中通过docker搭建了spark服务，docker容器的网段为192.168.2.0/24。 在相同网段的客户机中要访问docker容器中的服务，可以在客户机添加路由 # linux环境下 10.68.2.xx:为网关，也就是搭建服务电脑的ip sudo route add -net 192.168.2.0 netmask 255.255.255.0 gw 10.68.2.xx # windows环境下 route add 192.168.0.0 mask 255.255.0.0 10.68.2.xxx]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy中矩阵元素使用分数表示]]></title>
    <url>%2F2018%2F05%2F17%2Fnumpy%E4%B8%AD%E7%9F%A9%E9%98%B5%E5%85%83%E7%B4%A0%E4%BD%BF%E7%94%A8%E5%88%86%E6%95%B0%E8%A1%A8%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[In [1]: from sympy import Rational In [2]: a = Rational(&#39;1/3&#39;) In [3]: p = np.array([[0,1,0,0,0],[a,a,a,0,0],[0,a,a,a,0],[0,0,a,a,a],[0,0,0,1,0]]) In [4]: p Out[4]: array([[0, 1, 0, 0, 0], [1/3, 1/3, 1/3, 0, 0], [0, 1/3, 1/3, 1/3, 0], [0, 0, 1/3, 1/3, 1/3], [0, 0, 0, 1, 0]], dtype=object) # n个矩阵进行点乘 In [70]: def matric_dots(a,n): i = 1 c = a while i&lt;n: i += 1 c = c.dot(a) return c In [75]: matric_dots(p,3) == p.dot(p).dot(p) Out[75]: array([[ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True]])]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib subplot]]></title>
    <url>%2F2018%2F05%2F16%2Fmatplotlib-subplot%2F</url>
    <content type="text"><![CDATA[这两种方式是等价的]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python求明氏距离]]></title>
    <url>%2F2018%2F05%2F16%2Fpython%E6%B1%82%E6%98%8E%E6%B0%8F%E8%B7%9D%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[何为明式距离$ P = (x{1},x{2},\cdot \cdot \cdot,x{n}) and Q=(y{1},y{2},\cdot \cdot \cdot,y{n}) \in R^n $ $ D(X,Y) = \bigg( \sum{i=1}^n |x{i} - y_{i}|^p\bigg)^{1/p} $ 假设P = (0,4),Q=(3,0),求P,Q两点的明氏距离。 import numpy as np from scipy.spatial import minkowski_distance a = np.array([[0,4]]) a = np.array([[3,0]]) dist = minkowski_distance(a,b,1) #array([7]) dist = minkowski_distance(a,b,2) #array([5.]) dist = minkowski_distance(a,b,3) #array([4.49794145]) $ \lim{p \to \infty} \bigg( \sum{i=1}^n |x{i}-y{i}|^p\bigg)^{1/p} = \max{i=1}^n |x{i}-y_{i}| $ ,当p—&gt;oo时，P,Q两点的距离等于4 from sympy import symbols, limit p = symbols(&#39;p&#39;) f = (3**p + 4**p)**(1/p) limit(f,p,oo) # 4]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python对函数求导]]></title>
    <url>%2F2018%2F05%2F16%2Fpython%E5%AF%B9%E5%87%BD%E6%95%B0%E6%B1%82%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[假设函数[latex]f(x) = x^3+2x[/latex],求[latex]f’(x)[/latex]在x=1的值. 1.通过numpy多项式求导 import numpy as np y = np.poly1d([1,0,2,0]) print(y) # 3*x**2 + 2*x dy = y.deriv() dy(1) # 5 2.通过sympy对函数求导,通过命令pip3 install sympy安装 from sympy import symbols from sympy import diff as diff from npmath import diff as diff2 x = symbols(&#39;x&#39;) f = x**3 + 2*x f1 = diff(f) print(f1) # 3*x**2 + 2 diff2(lambda x: x**3+2*x,1) # 5 Ref：1.sympy文档2.mpmath文档]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[latex公式]]></title>
    <url>%2F2018%2F05%2F11%2Flatex%E5%85%AC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[常见特殊符号 语法 公式 语法 公式 \theta $ \theta $ \alpha $ \alpha $ \beta $ \beta $ \gamma $ \gamma $ \delta $ \delta $ \epsilon $ \epsilon $ \zeta $ \zeta $ \eta $ \eta $ \rho $ \rho $ \lambda $ \lambda $ \mu $ \mu $ \sigma $ \sigma $ \phi $ \phi $ \chi $ \chi $ \psi $ \psi $ \omega $ \omega $ \Theta $ \Theta $ \Phi $ \Phi $ \sim $ \sim $ \mathcal{P} $ \mathcal{P} $ 上下标 语法 公式 语法 公式 x^2 $ x^2 $ x_2 $ x_2 $ x_{i,j} $ x_{i,j} $ x_2^3 $ x_2^3 $ {}_1^2!X_3^4 $ {}_1^2!X_3^4 $ x^{2+2} $ x^{2+2} $ \hat{\gamma} $ \hat{\gamma} $ 导数，积分 语法 公式 语法 公式 x’ $ x’ $ x^\prime $ x^\prime $ \dot{x} $ \dot{x} $ \ddot{x} $ \ddot{y} $ \vec{c} $ \vec{c} $ \overleftarrow{a b} $ \overleftarrow{a } $ \sum_{k=1}^N k^2 $ \sum_{k=1}^N k^2 $ \prod_{i=1}^N x_i $ \prod_{i=1}^N x_i $ \lim_{n \to \infty}x_n $ \lim_{n \to \infty}x_n $ \int_{-N}^{N} e^x\, dx $ \int_{-N}^{N} e^x\, dx $ 分数 语法 公式 语法 公式 |\frac{1}{2}| $ \frac{1}{2} $ | 其它]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python排列组合，范数, 中位数相关函数]]></title>
    <url>%2F2018%2F05%2F06%2Fpython%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88%EF%BC%8C%E8%8C%83%E6%95%B0%2C%20%E4%B8%AD%E4%BD%8D%E6%95%B0%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[import numpy as np a = np.array([1, 2, 3]) np.linalg.norm(a, 1) #1-范数 np.linalg.norm(a, 2) #2-范数 x = np.array([1, 2]) y = np.array([1, 4]) dist = np.linalg.norm(x-y, 2) # x,y之间的欧里几德距离 [latex]C_{5}^2[/latex] from scipy.special import comb comb(5, 2) # output: 10 [latex]A_{5}^2[/latex] from scipy.special import perm perm(5, 2) # output: 20 中位数 x = np.array([1, 2, 5, 4, 9, 0, 6]) np.median(x) # 4 阶乘[latex]k![/latex] from scipy.special import factorial factorial(6) # out :array(720.0)]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[第1章 统计学习方法概论]]></title>
    <url>%2F2018%2F05%2F06%2F%E7%AC%AC1%E7%AB%A0%20%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[统计学习(statistical learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。统计学习的主要特点： 1.统计学习以计算机及网络为平台，是建立大计算机及网络之上的 2.统计学习以数据为研究对象，是数据驱动的学科 3.统计学习的目的是对数据进行预测与分析 4.统计学习以方法为中心，统计学习方法构建模型并应用模型进行预测与分析 5.统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科、并且在发展中逐步形成独自的理论体系与方法论。 统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。 统计学习的方法是基于数据构建统计模型从而对数据进行预测与分析。统计学习由监督学习(supervised learning)、非监督学习(unsupervised learning)、半监督学习(semi-supervised learning)和强化学习(reinforcement learning)等组成。 统计学习方法的步骤如下： 1.得到一个有限的训练数据集合 2.确定包含所有可能的模型的假设空间，即学习模型的集合 3.确定模型选择的准则，即学习的策略 4.实现求解最优模型的算法，即学习的算法 5.通过学习方法选择最优模型 6.利用学习的最优模型对新数据进行预测或分析 模型就是使用什么方法对数据进行预测与分析，就是所要学习的条件概率分布或决策函数，比如是线性回归，还是用多项式回归 策略就是你你通过什么方法判断哪个模型是最优的， 算法就是求解策略的最优解的算法，如梯度下降，最小二乘法 根据输入、输出变量的不同类型，对预测任务给予不同的名称：输入变量与输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题。 在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。 假设空间用$ \mathcal{F} $表示。假设空间可以定义为决策函数的集合 $ \mathcal{F}= { {\mathcal{f}|Y = f(X)}} $ 其中，X和Y是定义在输入空间$ \chi $和输出空间$ \mathcal{Y} $上的变量。这时$ \mathcal{F} $通常是由一个参数向量决定的函数族：$ \mathcal{F} = { {f|Y=f_{\theta}}(X),\theta\in R^{n}} $ 参数向量$ \theta $取值于n维欧式空间$ R^{n} $,称为参数空间。 假设空间也可以定义为条件概率的集合$ \mathcal{F}={ {P|P(Y|X)}} $ 策略： 统计学习的目标在于从假设空间中选取最优模型。损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。 统计学习常用损失函数：(1) 0-1损失函数$ L(Y,f(X))= \begin{cases} 0,&amp; {Y\neq f(X)}\ 1,&amp; {Y\neq f(X)} \end{cases} $(2) 平方损失函数$ L(Y, f(X)) = (Y - f(X))^2 $ (3) 绝对损失函数$ L(Y, f(X)) = |(Y - f(X))| $ (4) 对数损失函数$ L(Y, P(Y|X)) = -log P(Y|X) $ 如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高。这种现象称为过拟合。过拟合是指学习时选择的模型所包含的参数过多，以致于出现这一模型对已知数据预测得很好，但对未知的数据预测得很差的现象。 模型选择的典型方法是正则化。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项。正则化项一般是模型复杂度的单调函数，模型越复杂，正则化的值就越大。正则化项一般具有如下形式： $ min{f\in\mathcal{F}} \frac{1}{N}\sum{i=1}^NL(y{(i)}, f(x{i})) + \lambda J(f) $ 其中，第一项是经验风险，第二项是正则化项，$ \lambda \geq 0 $为调整两者之间关系的系数。 交叉验证1.简单交叉验证 简单交叉验证方法是：首先随机地将已给数据分为两部分，一部分作为训练，另一部分作为测试集。 2.S折交叉验证 首先随机地将已给数据切分为S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。 3.留一交叉验证 S折交叉验证的特殊情形是S=N，称为留一交叉验证，往往在数据缺乏的情况下使用，N是给定的数据集容量。 性能指标TP-将正类预测为正类数 FN-将正类预测为负类数 FP-将负类数预测为正类数 TN-将负数预测为负类数 准确率 $ A = \frac{TP+TN}{TP+FN+FP+TN} $ 精确率 $ P = \frac{TP}{TP+FP} $ 召回率 $ R = \frac{TP}{TP+FN} $ $ F{1} $值 $ \frac{2}{F{1}}= \frac{1}{P}+\frac{1}{R} $ $ F_{1} = \frac{2TP}{2TP+FP+FN} $ 泛化能力泛化能力是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。如果学到的模型是$ \hat f $,那么利用这个模型 对未知数据预测的误差就是泛化误差$ R{exp}(\hat f) = E[L(Y, \hat f(X))] = \int{\chi \times \mathcal{Y}}L(y,\hat f(x))P(x,y)dxdy $ 泛化误差上界：对二类分类问题，当假设空间是有限个函数集合$ \mathcal{F}={f{1},f{2},\cdot \cdot \cdot, f_{d}} $时，对于任意一个函数$ f \in \mathcal{F} $,至少以概率$ 1-\delta $,以下不等式成立$ R(f) \leq \hat R(f) + \varepsilon(d, N, \delta) $,其中$ \varepsilon(d, N, \delta) = \sqrt{\frac{1}{2N}\bigg(\log d + \log \frac{1}{\delta}\bigg)} $左端R(f)就泛化误差，右端即为泛化误差上界。第二项$ \varepsilon(d, N, \delta) $是N的单调递减函数，当N趋于无穷时趋于0 监督学习方法又可以分为生成方法和判别方法。生成方法由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型判别方法由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型典型的生成模型包括：朴素贝叶斯法和隐马尔可夫模型。典型的判别模型包括：K近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[通过梯度下降法求最小值]]></title>
    <url>%2F2018%2F04%2F24%2F%E9%80%9A%E8%BF%87%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%B1%82%E6%9C%80%E5%B0%8F%E5%80%BC%2F</url>
    <content type="text"><![CDATA[1.梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。—来源于百度百科 2.给定初始值和步长，通过当前值与上一值的比较，不断进行迭代，直到符合设定的条件后停止。 3.下面是对f(x) = x * x，通过梯度下降法求最小值 import matplotlib.pyplot as plt f = lambda x: x*x fig, ax = plt.subplots() def gradient_x(step,init_val): f_change, f_current = f(init_val), f(init_val) count = 0 x = init_val while f_change &gt; 0.0001: x = x - 2 * step * x f_change = f_current - f(x) f_current = f(x) count += 1 plt.plot(x,f(x), &quot;*-&quot;) print(&quot;after%diteration func found min value %d,x value is %d&quot;,count,f(x),x) plt.show() if __name__ == &#39;__main__&#39;: gradient_x(0.1, 2)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python使用最小二乘法求拟合曲线]]></title>
    <url>%2F2018%2F04%2F23%2Fpython%E4%BD%BF%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%B1%82%E6%8B%9F%E5%90%88%E6%9B%B2%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[1.最小二乘法 2.假设4个数据点为：(1, 6),(2, 5),(3, 7),(4, 10),求最佳匹配直线。 import numpy as np x = [1, 2, 3, 4] y = [6, 5, 7, 10] r = np.polyfit(x, y, deg=1) # 对(x,y)拟合多项式p(x) = p[0] * x**deg + ... + p[deg] f = np.poly1d(r) #f为拟合函数 求出的拟合直线为$ y = r[0]x+r[1] $, $y = 1.4x+3.5 $3.画出拟合直线和数据点 import matplotlib.pyplot as plt fig = plt.subplots() plt.plot(1,6,&#39;*-&#39;) plt.plot(2,5,&#39;*-&#39;) plt.plot(3,7,&#39;*-&#39;) plt.plot(4,10,&#39;*-&#39;) f = lambda x: 1.4*x + 3.5 x = [1, 2, 3, 4] y = [f(e) for e in x] plt.plot(x,y) plt.show() 使用scikit-learn# -*- coding: utf-8 -*- import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression x = np.array([ 6, 8, 10, 14, 18]) y = np.array([ 7. , 9. , 13. , 17.5, 18. ]) plt.plot(x, y, &#39;k.&#39;) plt.axis([0, 25, 0, 25]) model = LinearRegression() model.fit(x.reshape(-1,1),y.reshape(-1,1)) e = [item[0] for item in model.predict(x.reshape(-1,1))] X = list(zip(x,x)) Y = list(zip(y,e)) for i in range(len(X)): plt.plot(X[i],Y[i],&#39;r&#39;) plt.title(&quot;Linear regression use least square method&quot;) plt.xlabel(r&#39;$x$&#39;, horizontalalignment=&#39;right&#39;, x=1.0) plt.ylabel(r&#39;$y$&#39;, horizontalalignment=&#39;right&#39;, y=1.0, rotation=0) plt.plot(x, e) plt.grid(True) plt.show() 代码 选择最小二乘的原因：1.平方能将正残差与负残差都变成正数2.平方相当于给残差赋予了一个权重，越大的残差(绝对值)被赋予的权重越大。但是并不是所有情况下大的残差都应该被赋予大的权重，因为这样拟合方程就很容易受到异常值的影响3.在残差服从均值为0，方差为$ \sigma ^2 $ (未知，但为常数)的正态分布，且在残差与x独立的假设下，参数的最小二乘估计结果与极大似然估计量相同。4.最小二乘估计的计算简单 在数理统计中,残差是指实际观察值与估计值(拟合值)之间的差。 Ref：1.Think Stats Probability and Statistics for Programmers2.numpy3.scikit-learn]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python标准库之OrderedDict]]></title>
    <url>%2F2018%2F04%2F02%2Fpython%E6%A0%87%E5%87%86%E5%BA%93%E4%B9%8BOrderedDict%2F</url>
    <content type="text"><![CDATA[OrderedDict继承于dict,当插入key时顺序就已经确定了。如插入一个key存在的项，key的顺序不会改变。如果先删除再插入，则会移动到右边。In [5]: d = OrderedDict({&#39;banana&#39;: 3, &#39;apple&#39;: 4, &#39;pear&#39;: 1, &#39;orange&#39;: 2}) In [6]: d Out[6]: OrderedDict([(&#39;pear&#39;, 1), (&#39;banana&#39;, 3), (&#39;apple&#39;, 4), (&#39;orange&#39;, 2)]) In [7]: d.update({&#39;d&#39;:1}) In [8]: d Out[8]: OrderedDict([(&#39;pear&#39;, 1), (&#39;banana&#39;, 3), (&#39;apple&#39;, 4), (&#39;orange&#39;, 2), (&#39;d&#39;, 1)]) In [12]: d.update({&quot;pear&quot;:99}) # 顺序没有改变 In [13]: d Out[13]: OrderedDict([(&#39;pear&#39;, 99), (&#39;banana&#39;, 3), (&#39;apple&#39;, 4), (&#39;orange&#39;, 2), (&#39;d&#39;, 1)]) In [15]: del d[&#39;pear&#39;] In [16]: d Out[16]: OrderedDict([(&#39;banana&#39;, 3), (&#39;apple&#39;, 4), (&#39;orange&#39;, 2), (&#39;d&#39;, 1)]) In [17]: d.update({&#39;pear&#39;: 99}) # 移动到右边 In [18]: d Out[18]: OrderedDict([(&#39;banana&#39;, 3), (&#39;apple&#39;, 4), (&#39;orange&#39;, 2), (&#39;d&#39;, 1), (&#39;pear&#39;, 99)]) In [22]: d.move_to_end(&#39;apple&#39;) In [22]: &quot; &quot;.join(d.keys()) Out[22]: &#39;banana orange d pear apple&#39; # 当last=False时，将移动到最左边 In [23]: d.move_to_end(&#39;apple&#39;, last=False) In [24]: &quot; &quot;.join(d.keys()) Out[24]: &#39;apple banana orange d pear&#39;]]></content>
      <categories>
        <category>python标准库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[执行go get出现 go: GOPATH entry is relative错误]]></title>
    <url>%2F2018%2F04%2F01%2F%E6%89%A7%E8%A1%8Cgo%20get%E5%87%BA%E7%8E%B0%20go%20GOPATH%20entry%20is%20relative%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[将~/.zshrc文件中 export GOPATH=&quot;/home/hys/mycode/go:/home/hys/mycode/go/gopl:$GOPATH&quot; 改为 export GOPATH=&quot;/home/hys/mycode/go:/home/hys/mycode/go/gopl&quot;]]></content>
      <categories>
        <category>GO</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python标准库之ast]]></title>
    <url>%2F2018%2F04%2F01%2Fpython%E6%A0%87%E5%87%86%E5%BA%93%E4%B9%8Bast%2F</url>
    <content type="text"><![CDATA[ast-Abstract Syntax Trees用于帮助python应用处理抽象语法树# ast.literal_eval()将包含在字符串中的python结构转换为python结构，这些结构包含：strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None In [33]: s = &#39;[1, 2, 3]&#39; In [34]: lst = ast.literal_eval(s) In [35]: lst Out[35]: [1, 2, 3] In [39]: s2 = &quot;{&#39;a&#39;: 1, &#39;b&#39;: 2}&quot; In [40]: s2 Out[40]: &quot;{&#39;a&#39;: 1, &#39;b&#39;: 2}&quot; In [41]: ast.literal_eval(s2) Out[41]: {&#39;a&#39;: 1, &#39;b&#39;: 2}]]></content>
      <categories>
        <category>python标准库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[request中之自定义的结构体]]></title>
    <url>%2F2018%2F04%2F01%2Frequest%E4%B8%AD%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%2F</url>
    <content type="text"><![CDATA[# 通过LookupDict构建个可以根据多个value查找key的字典 class LookupDict(dict): &quot;&quot;&quot;Dictionary lookup object.&quot;&quot;&quot; def __init__(self, name=None): self.name = name super(LookupDict, self).__init__() def __repr__(self): return &#39;&lt;lookup \&#39;%s\&#39;&gt;&#39; % (self.name) def __getitem__(self, key): # We allow fall-through here, so values default to None return self.__dict__.get(key, None) def get(self, key, default=None): return self.__dict__.get(key, default) In [2]: _codes = { ...: ...: # Informational. ...: 100: (&#39;continue&#39;,), ...: 101: (&#39;switching_protocols&#39;,), ...: 102: (&#39;processing&#39;,), ...: 103: (&#39;checkpoint&#39;,), ...: 122: (&#39;uri_too_long&#39;, &#39;request_uri_too_long&#39;) ...: } In [3]: codes = LookupDict(name=&#39;status_codes&#39;) In [4]: for code, titles in _codes.items(): ...: for title in titles: ...: setattr(codes, title, code) ...: if not title.startswith(&#39;\\&#39;): ...: setattr(codes, title.upper(), code) ...: In [5]: codes Out[5]: &lt;lookup &#39;status_codes&#39;&gt; In [6]: codes.processing Out[6]: 102 In [7]: codes.checkpoint Out[7]: 103 # codes的__dict__如下 In [8]: codes.__dict__ Out[8]: {&#39;CHECKPOINT&#39;: 103, &#39;CONTINUE&#39;: 100, &#39;PROCESSING&#39;: 102, &#39;REQUEST_URI_TOO_LONG&#39;: 122, &#39;SWITCHING_PROTOCOLS&#39;: 101, &#39;URI_TOO_LONG&#39;: 122, &#39;checkpoint&#39;: 103, &#39;continue&#39;: 100, &#39;name&#39;: &#39;status_codes&#39;, &#39;processing&#39;: 102, &#39;request_uri_too_long&#39;: 122, &#39;switching_protocols&#39;: 101, &#39;uri_too_long&#39;: 122} # CaseInsensitiveDict定义了大小不敏感的字典 class CaseInsensitiveDict(collections.MutableMapping): &quot;&quot;&quot; cid = CaseInsensitiveDict() cid[&#39;Accept&#39;] = &#39;application/json&#39; cid[&#39;aCCEPT&#39;] == &#39;application/json&#39; # True list(cid) == [&#39;Accept&#39;] # True&quot;&quot;&quot; def __init__(self, data=None, **kwargs): self._store = OrderedDict() if data is None: data = {} self.update(data, **kwargs) def __setitem__(self, key, value): # Use the lowercased key for lookups, but store the actual # key alongside the value. self._store[key.lower()] = (key, value) def __getitem__(self, key): return self._store[key.lower()][1] def __delitem__(self, key): del self._store[key.lower()] def __iter__(self): return (casedkey for casedkey, mappedvalue in self._store.values()) def __len__(self): return len(self._store) def lower_items(self): &quot;&quot;&quot;Like iteritems(), but with all lowercase keys.&quot;&quot;&quot; return ( (lowerkey, keyval[1]) for (lowerkey, keyval) in self._store.items() ) def __eq__(self, other): if isinstance(other, collections.Mapping): other = CaseInsensitiveDict(other) else: return NotImplemented # Compare insensitively return dict(self.lower_items()) == dict(other.lower_items()) # Copy is required def copy(self): return CaseInsensitiveDict(self._store.values()) def __repr__(self): return str(dict(self.items())) MutableMapping的定义是实现__getitem__,__setitem__,__delitem__,__iter__,__len__方法的对象。 CaseInsensitiveDict通过将key转换为小写进行查找，在字典值中以元组的形式存储了原始键和值。 lower_items()返回小写的key,和原始的value。 In [13]: cid = CaseInsensitiveDict() In [16]: cid Out[16]: {&#39;Accept&#39;: &#39;application/json&#39;} In [19]: cid.__dict__ Out[19]: {&#39;_store&#39;: OrderedDict([(&#39;accept&#39;, (&#39;aCCEPT&#39;, &#39;application/json&#39;))])} In [30]: list(cid.lower_items()) Out[30]: [(&#39;accept&#39;, &#39;application/json&#39;), (&#39;ab&#39;, &#39;Cd&#39;)] In [31]: cid._store.items() Out[31]: odict_items([(&#39;accept&#39;, (&#39;aCCEPT&#39;, &#39;application/json&#39;)), (&#39;ab&#39;, (&#39;Ab&#39;, &#39;Cd&#39;))])]]></content>
      <categories>
        <category>requests</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python中的native string]]></title>
    <url>%2F2018%2F03%2F26%2Fpython%E4%B8%AD%E7%9A%84native%20string%2F</url>
    <content type="text"><![CDATA[# 将给定的字符串对象转换为native string def to_native_string(string, encoding=&#39;ascii&#39;): if isinstance(string, builtin_str): out = string else: if is_py2: out = string.encode(encoding) else: out = string.decode(encoding) return out 1.”Native” strings(指那些类型名为str的字符串类型)，用于请求/响应头和元数据。 2.”Bytestring”(在python3中类型名为bytes,python2中为str),用于请求/响应中的body。 Ref：1.pep33332.requests源码3.http://img.hysyeah.top/2017/10/02/python%E7%BC%96%E7%A0%81/]]></content>
      <categories>
        <category>requests</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu上搭建git仓库]]></title>
    <url>%2F2018%2F03%2F21%2Fbuntu%E4%B8%8A%E6%90%AD%E5%BB%BAgit%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1.安装git ,git-core,openssh-server 2.创建用户git, 3.客户端生成公钥，ssh-keygen -t rsa -C &quot;xxx@126.com&quot; 5.将客户端公钥添加到服务端的~/.ssh/authorized_keys文件下 6.git –bare init /home/git/myRep.git #在服务端建一个空的仓库 7.在客户端输入命令拉取代码 git clone git@gitServerIP:/home/git/myRep.git 8.在myRep.git下的hooks 新建post-receive, 添加git --work-tree=/home/wwwroot/hehe checkout -f 设置工作目录(代码存放的目录)，每次提交代码后将会触发post-receive下的脚本, 目标目录必须先创建,注意文件夹git用户写入权限]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker使容器和宿主机使用同一网段]]></title>
    <url>%2F2018%2F03%2F16%2Fdocker%E4%BD%BF%E5%AE%B9%E5%99%A8%E5%92%8C%E5%AE%BF%E4%B8%BB%E6%9C%BA%E4%BD%BF%E7%94%A8%E5%90%8C%E4%B8%80%E7%BD%91%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[1.通过ifconfig查询物理网卡信息，名称为eno1 2.route -n,查询网关为10.68.2.1 3.使用docker命令创建网络docker network create -d macvlan \ --subnet 10.68.2.0/24 --gateway 10.68.2.1 \ -o parent=eno1 -o macvlan_mode=bridge macnet # parent指定物理网卡名称, # 创建的网络名称为macnet 4.创建docker-compose.yml文件version: &quot;2&quot; services: ubuntu-master: image: ubuntu:16.04 networks: default: ipv4_address: 10.68.2.133 container_name: ubuntu-master privileged: true tty: true networks: default: external: name: macnet 5.启动容器，查看IP]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker容器启动时设置容器内服务自启动]]></title>
    <url>%2F2018%2F02%2F24%2Fdocker%E5%AE%B9%E5%99%A8%E5%90%AF%E5%8A%A8%E6%97%B6%E8%AE%BE%E7%BD%AE%E5%AE%B9%E5%99%A8%E5%86%85%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[1.编写shell脚本，auto_start.sh #！/bin/sh /etc/init.d/ssh start #启动服务 /bin/bash #一定要加上这句，否则容器会自动退出 2.修改docker-compose.yml volumes: - ./auto_start.sh:/root/auto_start.sh command:[&quot;/root/auto_start.sh&quot;] 3.启动容器，便会自动启动auto_start.sh中的服务。 可能出现的问题： 1.exec: \“/root/auto_start.sh\“: permission denied”: unknown’，提示没有权限 解决方法: 修改宿主机上的auto_start.sh文件权限，chmod a+x auto_start.sh,重新启动服务。]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用docker搭建hadoop,spark集群]]></title>
    <url>%2F2018%2F02%2F12%2F%E4%BD%BF%E7%94%A8docker%E6%90%AD%E5%BB%BAhadoop%2Cspark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[环境：ubuntu 16.04 64位安装所需文件： Java SDK 1.8, scala-2.11.11,hadoop-2.7.4,spark-2.2.0-bin-hadoop2.7 1.docker pull ubuntu2.启动镜像，docker run -idt [image_id] /bin/bash3.进入容器，安装必要的工具 apt-get install iputils-ping #ping命令 apt-get install net-tools #ifconfig 4.将安装包复制到容器内部，查看容器主机之间文件复制5.解压各个安装包，设置环境变量 ~/.bashrc docker commit -m “xx” -a “xx” spark_base #生成一个新的镜像 6.新建项目，修改各种配置文件7.项目目录 # ssh文件夹下面是通过ssh-keygen -t rsa 命令生成的~/.ssh目录下的所有文件（这样不用每次重新启动新的容器后，都要重新配置ssh) ├── master │ ├── hadoop │ ├── spark │ └── ssh ├── slave1 │ ├── hadoop │ ├── spark │ └── ssh └── slave2 | └── ssh |__ docker-compose.yml 8.创建网络 docker-compose up -d #启动容器，一个namenode,二个datanode9.进入master容器，/opt/hadoop-2.7.4/sbin/start-all.sh #启动集群 10. 进入master容器，输入jps，有如下进程 3191 NameNode 3817 Jps 3433 ResourceManager 进入slave1容器，输入jps，有如下进程 962 NodeManager 1235 Jps 854 DataNode 11.输入hdfs dfsadmin -report可查看节点信息 12.测试集群是否正常工作。 vim a.txt #创建本地文件 hdfs dfs -put a.txt / #上传到hdfs文件系统 cd /opt/hadoop-2.7.4/share/hadoop/mapreduce hadoop jar hadoop-mapreduce-examples-2.7.4.jar wordcount /a.txt /out #执行count计算 hdfs dfs -text /out/part-r-00000 #查看执行结果 若以上都没有出错，说明集群正常 Ref：1.http://blog.csdn.net/xu470438000/article/details/50512442]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark-shell使用mysql存储元数据]]></title>
    <url>%2F2018%2F02%2F07%2Fspark-shell%E4%BD%BF%E7%94%A8mysql%E5%AD%98%E5%82%A8%E5%85%83%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1.hive安装(依赖于hadoop) - 下载hive - 解压到/opt目录下 - 添加环境变量到~/.bashrc export HIVE_HOME=/opt/apache-hive-2.2.0-bin export PATH=$HIVE_HOME/bin:$PATH 2.hive-env.sh添加 HADOOP_HOME=/opt/hadoop-2.7.4 3.hive-site.xml添加[server端，hive目录下] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://10.68.2.55:3307/hivedb?createDatabaseIfNotExist=true&amp;characterEncoding=utf8&amp;useSSL=false&lt;/value&gt; #mysql连接信息 &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hello&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;hdfs://10.68.2.55:9000/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 4.hive-site.xml[client端，spark目录下] &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://10.68.2.55:8000&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;spark.sql.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; #hdfs dfs -mkdir -p /usr/hive/warehouse &lt;/property&gt; &lt;/configuration&gt; 5.下载mysql-connector-java-5.1.44-bin.jar，放到/opt/apache-hive-2.2.0-bin/lib目录下 6. schematool -dbType mysql -initSchema #初始化 hive --service metastore -p &lt;port&gt; #启动服务 启动spark-shell将会将元数据存储到mysql]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hadoop,spark踩过的坑]]></title>
    <url>%2F2018%2F02%2F07%2Fhadoop%2Cspark%E8%B8%A9%E8%BF%87%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[1.WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOMEhdfs dfs -mkdir /hadoop/spark_jars hdfs dfs -put /opt/spark-2.2.0-bin-hadoop2.7/jars/spark-* /hadoop/spark_jars #将spark-*.jar文件复制到/hadoop/spark_jars 在spark-defaults.conf中添加 spark.yarn.jars hdfs://192.168.2.10:9000/hadoop/spark_jars/* 2. NodeManager from 464aa87ad374 doesn’t satisfy minimum allocations, Sending SHUTDOWN signal to the NodeManager.# 在yarn-site.xml添加如下 &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;3072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 3.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).18/02/07 08:10:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 18/02/07 08:10:57 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Container marked as failed: container_1517990956375_0001_01_000003 on host: slave2. Exit status: 1. Diagnostics: Exception from container-launch. Container id: container_1517990956375_0001_01_000003 Exit code: 1 Stack trace: ExitCodeException exitCode=1: at org.apache.hadoop.util.Shell.runCommand(Shell.java:585) at org.apache.hadoop.util.Shell.run(Shell.java:482) at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:776) at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Container exited with a non-zero exit code 1 # 修改spark-defaults.conf添加如下，值根据实际情况修改 spark.executor.memory 2g spark.driver.memory 2g 4.Caused by: java.io.InvalidClassException: org.apache.spark.sql.execution.FileSourceScanExec; local class incompatible: stream classdesc serialVersionUID = 4243567174184146251, local class serialVersionUID = -7006716103980652543使用pyspark对进行groupby后DataFrame进行show()操作时出现的错误 df2 = df.groupby(df.date).agg({&#39;event_type&#39;: &#39;count&#39;}) df2 = df.show() 解决方法：将客户端的pyspark版本改成与服务端一致 5.Cannot run program “python”: error=2, No such file or directory执行spark任务时，出现如上提示 解决方法：因为用的是python3,可能spark却去找python，找不到所以报错。因时间问题，只能暴力解决，在各个节点执行下面命令 ln -s /usr/bin/python3 /usr/bin/python 6.standard_init_linux.go:195: exec user process caused “exec format error”启动容器时报如上错误，原因不小心将docker-compose.yml中command中执行的脚本文件中第一行写成了#/bin/sh,改为#!/bin/sh后解决。 Ref：1.https://issues.apache.org/jira/browse/SPARK-12759 2.http://blog.csdn.net/u013641234/article/details/51123648]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令2]]></title>
    <url>%2F2018%2F02%2F07%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A42%2F</url>
    <content type="text"><![CDATA[# 容器和主机之间的文件复制 1.从主机复制到容器 sudo docker cp host_path containerid:container_path 2.从容器复制到主机 sudo docker cp containerid:container_path host_path # 免sudo sudo groupadd docker sudo usermod -aG docker $USER sudo service docker restart sudo chmod a+rw /var/run/docker.sock newgrp docker ➜ ~ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hyspark latest 652a7a9d9fbe 12 days ago 1.66GB d.img.hysyeah.top:5000/busybox latest f9b6f7f7b9d3 3 weeks ago 1.14MB mysql latest f008d8ff927d 3 weeks ago 409MB docker save -o mysql.tar mysql #将镜像导出为mysql.tar文件 docker load -i mysql.tar #导入镜像]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[启动docker hadoop集群失败]]></title>
    <url>%2F2018%2F02%2F01%2F%E5%90%AF%E5%8A%A8docker%20hadoop%E9%9B%86%E7%BE%A4%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[启动hadoop发生失败，查看日志文件 docker hadoop Does not contain a valid host:port authority 从google得知，网络信息不能带下划线 解决方法1.docker network ls发现docker网络中NAME:dockerpro_sparknet中含有下划线 NETWORK ID NAME DRIVER SCOPE 321147ce790d bridge bridge local 0fd542a85347 dockerpro_hsnet bridge local fa6a55dd5ed3 dockerpro_sparknet bridge local 2.docker network create -d bridge —subnet 192.168.2.0/24 sparknet3.在docker-compose.yml中引用已创建的网络networks: default: external: name: sparknet version: &#39;2&#39; services: master: image: hyspark networks: default: ipv4_address: 192.168.2.10 4.重新启动hadoop集群 这问题够坑的。。]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker-compose up -d启动失败不能创建网络]]></title>
    <url>%2F2018%2F02%2F01%2Fdocker-compose%20up%20-d%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%E4%B8%8D%E8%83%BD%E5%88%9B%E5%BB%BA%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[当修改docker-compose.yml文件后台，使用命令docker-coompose up -d 启动服务 出现如下错误 Creating network &quot;master_sparknet&quot; with driver &quot;bridge&quot; ERROR: cannot create network ddc16ca081602776df238912eeb042380f5cb15b88c108799b66d074742c704a (br-ddc16ca08160): conflicts with network fa6a55dd5ed30ce92342c4d3218558139293fd1800f5893c83b9f6a75aeaf277 (br-fa6a55dd5ed3): networks have overlapping IPv4 提示不能创建网络，两个网络发生冲突 解决方法1.docker network ls #展示docker容器网络 2.找到冲突的网络 docker network rm fa6a55dd5ed3 3.docker-compose up -d #重新启动服务]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[搭建安全Docker Registry]]></title>
    <url>%2F2018%2F01%2F23%2F%E6%90%AD%E5%BB%BA%E5%AE%89%E5%85%A8Docker%20Registry%2F</url>
    <content type="text"><![CDATA[1.安装registry docker pull registry 2.制作证书 openssl req -newkey rsa:2048 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt 3.docker run -d -p 5000:5000 --restart=always --name registry \ -v /data/registry:/var/lib/registry \ -v /certs:/certs \ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \ .4.在客户端安装证书 sudo mkdir -p /etc/docker/certs.d/d.img.hysyeah.top:5000 sudo cp domain.crt /etc/docker/certs.d/d.img.hysyeah.top:5000/ca.crt sudo service docker restart 5.docker pull busybox 6.docker tag busybox d.img.hysyeah.top:5000/busybox 7.docker push d.img.hysyeah.top:5000/busybox]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose管理mysql容器]]></title>
    <url>%2F2018%2F01%2F22%2FDocker%20Compose%E7%AE%A1%E7%90%86mysql%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Dockerfile用于管理一个单独的应用容器，Docker Compose用于管理多个容器。 1.sudo pip install docker-compose 2.docker pull mysql 3.mkidr mysql vim docker-compose.yml docker-compose.yml(包括version,services,networks三大部分)version: &#39;2&#39; services: mysql: image: mysql #指定镜像为mysql networks: mysqlnet: ipv4_address: xxx.xxx.xxx.xxx #设置ip地址 volumes: - /etc/mysql/:/etc/mysql #将宿主机的/etc/mysql/映射到容器中，即容器上使用宿主中的/etc/mysql/ restart: always environment: #设置环境变量 MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: test MYSQL_USER: hys MYSQL_PASSWORD: hello expose: - &quot;3307&quot; ports: - &quot;3307:3306&quot; networks: mysqlnet: driver: bridge ipam: driver: default config: - subnet: xxx.xxx.xxx.0/26 docker-compose up -d #启动后，可访问mysql服务]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot添加中文显示]]></title>
    <url>%2F2018%2F01%2F14%2Fmatplotlib.pyplot%E6%B7%BB%E5%8A%A0%E4%B8%AD%E6%96%87%E6%98%BE%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[1.从C:\Windows\Fonts复制微软雅黑字体到/usr/local/lib/python3.5/dist-packages/matplotlib/mpl-data/fonts/ttf 然后将名称修改为 sudo mv MSYHBD.TTF MSYHBD.ttf sudo mv MSYH.TTF MSYH.ttf 2.获取配置文件信息，修改文件 In [1]: import matplotlib In [2]: matplotlib.matplotlib_fname() Out[2]: &#39;/usr/local/lib/python3.5/dist-packages/matplotlib/mpl-data/matplotlibrc&#39; font.family : Microsoft YaHei font.sans-serif : DejaVu Sans, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif, Microsoft YaHei 3.删除~/.cache/matplotlib下文件fontList.py3k.cache,重启终端。]]></content>
      <categories>
        <category>pyplot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[x86相关寄存器]]></title>
    <url>%2F2018%2F01%2F06%2Fx86%E7%9B%B8%E5%85%B3%E5%AF%84%E5%AD%98%E5%99%A8%2F</url>
    <content type="text"><![CDATA[IP-Instruction Pointer--指令指针寄存器 CS-Code Segment--段寄存器 DS-Data Segment--数据段寄存器 %rax-%eax-%ax-%al---返回值 %rbx-%ebx-%bx-%bl---被调用者保存 %rcx-%ecx-%cx-%cl---第4个参数 %rdx-%edx-%dx-%dl---第3个参数 %rsi-%esi-%si-%sil---第2个参数 %rdi-%edi-%di-%dil---第1个参数 %rbp-%ebp-%bp-%bpl---基址寄存器(stack pointer),一般在函数中用来保存进入函数时的sp的栈顶基址(被调用者保存) %rsp-%esp-%sp-%spl---栈指针 %r8-%r8d-%r8w-%r8b---第5个参数 %r9-%r9d-%r9w-%r9b---第6个参数 %r10-%r10d-%r10w-%r10b---调用者保存 %r11-%r11d-%r11w-%r11b---调用者保存 %r12-%r12d-%r12w-%r12b---被调用者保存 %r13-%r13d-%r13w-%r13b---被调用者保存 %r14-%r14d-%r14w-%r14b---被调用者保存 %r15-%r15d-%r15w-%r15b---被调用者保存]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[elf可重定向目标文件格式]]></title>
    <url>%2F2018%2F01%2F01%2Felf%E5%8F%AF%E9%87%8D%E5%AE%9A%E5%90%91%E7%9B%AE%E6%A0%87%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[ELF头 .text .rodata .data .bss .symtab .rel.text .rel.data .debug .line .strtab 节头部表 典型的ELF可重定位目标文件 .text:已编译程序的机器代码 .rodata:只读数据，一般是程序里面的只读变量（如const修饰的变量）和字符串常量。 .data:已初始化的全局和静态C变量。局部C变量运行被保存在栈中，既不出现在.data节中，也不出现在.bss中 .COMMON 未初始化的全局变量 .bss:未初始化的静态C变量，以及所有被初始化为0的全局或静态变量。在目标文件中这个节不占实际的空间，它仅仅是一个占位符。目标文件格式区分已初始化和未初始化变量是为了空间效率：在目标文件中，未初始化变量不需要占据任何实际的磁盘空间。运行时，在内存中分配这些变量，初始值为0。 .symtab:一个符号表，它存放在程序中定义和引用的函数和全局变量的信息。 .rel.text:一个.text节中位置的列表，当链接器把这个目标文件和其他文件组合时，需要修改这些位置。 .rel.data:被模块引用或定义的所有全局变量的重定位作息。一般来说，任何已初始化的全局变量，如果它的初始值是一个全局变量地址或者外部定义函数的地址，都需要被修改。 .debug:一个调试符号表，其条目是程序中定义的局部变量和类型定义，程序中定义和引用的全局变量，以及原始的C源文件。只有以-g选项调用编译器驱动程序时，才会得到这张表。 .line:原始C源程序中的行号和.text节中的机器指令这间的映射。只有以-g选项调用编译器驱动程序时，才会得到这张表。 .strtab:一个字符串表，其内容包括.symtab和.debug节中的符号表，以及节头部中的节名字。字符串就是以null结尾的字符串的序列。 .comment:存放的是编译器版本信息。 .dynamic:动态链接作息 .hash:符号哈希表 .note:额外的编译器信息 .plt .got:动态链接的跳转表和全局入口表 .init .fini:程序初始化与终结代码段]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gcc常用选项]]></title>
    <url>%2F2017%2F12%2F24%2Fgcc%E5%B8%B8%E7%94%A8%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[/*对源文件hello.c进行预处理,并生成一个ASCII码的中间文件hello.i*/ gcc -E hello.c -o hello.i /*等同于cpp hello.c hello.i*/ /*将hello.c或hello.i翻译成汇编语言文件hello.s*/ gcc -S hello.c -o hello.s /*gcc -S hello.i -o hello.s*/ /*将hello.s翻译成可重定向文件hello.o*/ gcc hello.s -o hello.o /*编译和汇编(不进行链接)生成hello.o*/ gcc -c hello.c /*生成可执行文件hello*/ gcc hello.c -o hello /*-fpic(Position-Independent Code)器生成位置无关的代码,共享库的编译必须总是使用这个选项;-shared指示编译器创建一个共享的目标文件*/ gcc -shared -fpic -o lib.so a.c b.c]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装readelf]]></title>
    <url>%2F2017%2F12%2F19%2Fubuntu%E5%AE%89%E8%A3%85readelf%2F</url>
    <content type="text"><![CDATA[sudo apt-get install binutils Usage: readelf &lt;option(s)&gt; elf-file(s) Display information about the contents of ELF format files Options are: -a --all Equivalent to: -h -l -S -s -r -d -V -A -I -h --file-header Display the ELF file header -l --program-headers Display the program headers --segments An alias for --program-headers -S --section-headers Display the sections&#39; header --sections An alias for --section-headers -g --section-groups Display the section groups -t --section-details Display the section details -e --headers Equivalent to: -h -l -S -s --syms Display the symbol table --symbols An alias for --syms --dyn-syms Display the dynamic symbol table -n --notes Display the core notes (if present) -r --relocs Display the relocations (if present) -u --unwind Display the unwind info (if present) -d --dynamic Display the dynamic section (if present) -V --version-info Display the version sections (if present) -A --arch-specific Display architecture specific information (if any) -c --archive-index Display the symbol/file index in an archive -D --use-dynamic Use the dynamic section info when displaying symbols -x --hex-dump=&lt;number|name&gt; Dump the contents of section &lt;number|name&gt; as bytes -p --string-dump=&lt;number|name&gt; Dump the contents of section &lt;number|name&gt; as strings -R --relocated-dump=&lt;number|name&gt; Dump the contents of section &lt;number|name&gt; as relocated bytes -z --decompress Decompress section before dumping it -w[lLiaprmfFsoRt] or --debug-dump[=rawline,=decodedline,=info,=abbrev,=pubnames,=aranges,=macro,=frames, =frames-interp,=str,=loc,=Ranges,=pubtypes, =gdb_index,=trace_info,=trace_abbrev,=trace_aranges, =addr,=cu_index] Display the contents of DWARF2 debug sections --dwarf-depth=N Do not display DIEs at depth N or greater --dwarf-start=N Display DIEs starting with N, at the same depth or deeper -I --histogram Display histogram of bucket list lengths -W --wide Allow output width to exceed 80 characters @&lt;file&gt; Read options from &lt;file&gt; -H --help Display this information -v --version Display the version number of readelf]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[django models migrate]]></title>
    <url>%2F2017%2F12%2F17%2Fdjango-models-migrate%2F</url>
    <content type="text"><![CDATA[修改models.py后，python manage.py makemigrations未发现改变 原因是settings.py中INSTALLED_APPS未添加app名称]]></content>
      <categories>
        <category>Django</category>
        <category>Python Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python c PyLongObject定义及实现2]]></title>
    <url>%2F2017%2F12%2F17%2Fpython%20c%20%20PyLongObject%E5%AE%9A%E4%B9%89%E5%8F%8A%E5%AE%9E%E7%8E%B02%2F</url>
    <content type="text"><![CDATA[/*根据长整型生成python PyLongObject对象*/ PyObject* PyLong_FromLong(long v) 通过这个函数我们来了解python是怎么构造一个PyLongObject对象。 PyLong_FromLong函数源码 /*在64位系统中PyLong_SHIFT=30,*/ #define PyLong_BASE ((digit)1 &lt;&lt; PyLong_SHIFT) #define PyLong_MASK ((digit)(PyLong_BASE - 1)) PyObject * PyLong_FromLong(long ival) { PyLongObject *v; unsigned long abs_ival; unsigned long t; /* unsigned so &gt;&gt; doesn&#39;t propagate sign bit */ int ndigits = 0; int negative = 0; if (ival &lt; 0) { /* if LONG_MIN == -LONG_MAX-1 (true on most platforms) then ANSI C says that the result of -ival is undefined when ival == LONG_MIN. Hence the following workaround. */ abs_ival = (unsigned long)(-1-ival) + 1; negative = 1; } else { abs_ival = (unsigned long)ival; } /* Count the number of Python digits. We used to pick 5 (&quot;big enough for anything&quot;), but that&#39;s a waste of time and space given that 5*15 = 75 bits are rarely needed. */ t = abs_ival; while (t) { ++ndigits; t &gt;&gt;= PyLong_SHIFT; } v = _PyLong_New(ndigits);//为PyLongObject对象分配内存 if (v != NULL) { digit *p = v-&gt;ob_digit; v-&gt;ob_size = negative ? -ndigits : ndigits; t = abs_ival; while (t) { *p++ = (digit)(t &amp; PyLong_MASK); t &gt;&gt;= PyLong_SHIFT; } } return (PyObject *)v; } 转换成可以单独执行的C代码 #include &lt;stdio.h&gt; #include &lt;math.h&gt; #define base ((unsigned int)1 &lt;&lt;30) #define mask ((unsigned int)(base-1)) int main(int argc, char *argv[]){ unsigned int abs_ival; unsigned int t; int ndigits = 0; int i; abs_ival = (1&lt;&lt;32) - 1 ; t = abs_ival; while(t){ ++ndigits; t &gt;&gt;=30; } unsigned int ob_digit[ndigits]; unsigned int *p = ob_digit; t = abs_ival; while(t){ *p++ = (unsigned int)(t &amp; mask); t &gt;&gt;= 30; } for(i=0;i&lt;ndigits;i++) printf(&quot;%u\t&quot;,ob_digit[i]); printf(&quot;\n&quot;); abs_ival = 0; //根据数组ob_digit[]中的值，打印出原始值 for(i=0;i&lt;ndigits;i++) abs_ival += ob_digit[i] * pow(2,30*i);//or:abs_ival += ob_digit[i]*(1&lt;&lt;(30*i)); printf(&quot;%u\n&quot;,abs_ival); return 0; } gcc longo.c -lm //不加-lm则会报错 out: 1073741823 3 4294967295 base = 0x40000000 mask = 0x3fffffff abs_ival = 0xffffffff ob_digit中元素的最大值为(2**30)-1 ？PyLong_SHIFT为什么是30。 Ref：python2.7源码]]></content>
      <categories>
        <category>python源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python c PyLongObject定义及实现1]]></title>
    <url>%2F2017%2F12%2F12%2Fpython%20c%20%20PyLongObject%E5%AE%9A%E4%B9%89%E5%8F%8A%E5%AE%9E%E7%8E%B01%2F</url>
    <content type="text"><![CDATA[typedef PY_UINT32_T digit; #define PyObject_VAR_HEAD \ PyObject_HEAD \ Py_ssize_t ob_size; struct _longobject { PyObject_VAR_HEAD digit ob_digit[1]; }; typedef struct _longobject PyLongObject; PyLongObject对象除了引用计数ob_refcnt,*ob_type指针，与PyIntObject不同的是，通过动态改变ob_digit数组的大小，以实现任意精度的长整形。 PyLongObject对象的绝对值表示为 SUM(for i=0 through abs(ob_size)-1) ob_digit[i] * 2**(SHIFT*i) 当数值为0L时，ob_size = 0，不会为ob_digit数组分配内存。 当数值为负数时，ob_size &lt; 0; 当创建一个PyLongObject时，内存分配函数会为ob_digit数组分配足够的空间，所以ob_digit[0] … ob_digit[abs(ob_size)-1]是有效的。 为结构体的成员数组动态分配大小]]></content>
      <categories>
        <category>python源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为结构体的成员数组动态分配大小]]></title>
    <url>%2F2017%2F12%2F12%2F%E4%B8%BA%E7%BB%93%E6%9E%84%E4%BD%93%E7%9A%84%E6%88%90%E5%91%98%E6%95%B0%E7%BB%84%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[struct _longobject { Py_ssize_t ob_refcnt; struct _typeobject *ob_type; Py_ssize_t ob_size; digit ob_digit[1]; }PyLongObject; PyLongObject *p; /*为结构体动态分配内存*/ p = (PyLongObject*)malloc(sizeof(PyLongObject)+sizeof(digit)*abs(ob_size)) for(int i = 0; i &lt; ob_size; i++){ p-&gt;ob_digit[i] = 1; } 这就是PyLongObject对象的内存分配，只不过python源码由内存管理模块进行内存的分配，不那么好理解。 example#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; typedef struct T { int a; int b; char c[0]; }T;//大小为8字节，在32位操作系统 int main(int argc, char *argv[]){ T *t = (T*)malloc(sizeof(T)+sizeof(char)*6); printf(&quot;%lu,%lu,%lu\n&quot;,sizeof(T),sizeof(int),sizeof(t-&gt;c)); t-&gt;a = 1; t-&gt;b = 1; strcpy(t-&gt;c, &quot;hello&quot;); printf(&quot;%s\n&quot;,t-&gt;c); return 0; } 输出： 8,4,0 hello Ref：1.https://wiki.sei.cmu.edu/confluence/display/c/MEM33-C.++Allocate+and+copy+structures+containing+a+flexible+array+member+dynamically]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[c语言移位操作]]></title>
    <url>%2F2017%2F12%2F10%2Fc%E8%AF%AD%E8%A8%80%E7%A7%BB%E4%BD%8D%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[对于一个位表示为[ [latex]x{w-1},x{w-2},…,x_1,x_0[/latex] ]的操作数x,C表达式x&lt;&lt; k会生成一个值，其位表示为[ [latex]x{w-k-1},x{w-k-2},…,x_1,x_0,0,…,0[/latex] ]也就是说，x向左移动k位，丢弃最高的k位，并在右端补k个0。 逻辑右移与算术右移逻辑右移在左端补k个0，得到的结果是[ [latex]0,…,0,x{w-1},x{w-2},…,xk[/latex] ]算术右移是在左端补k个最高有效位的值，得到的结果是[ [latex]x{w-1},…,x{w-1},x{w-1},x_{w-2}…,x_k[/latex] ] 操作 值1 值2 参数x 0110 0011 1001 0101 x &lt;&lt; 4 0011 0000 0101 0000 x &gt;&gt; 4(逻辑右移) 0000 0110 0000 1001 x &gt;&gt; 4(算术右移) 0000 0110 1111 1001 几乎所有的编译器都对有符号数(负数)使用算术右移，可Ref]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python c相关结构体]]></title>
    <url>%2F2017%2F12%2F09%2Fpython%20c%E7%9B%B8%E5%85%B3%E7%BB%93%E6%9E%84%E4%BD%93%2F</url>
    <content type="text"><![CDATA[/*object.h*/ #define PyObject_HEAD \ Py_ssize_t ob_refcnt; \ struct _typeobject *ob_type; typedef struct _object { PyObject_HEAD } PyObject; typedef struct { Py_ssize_t ob_refcnt; struct _typeobject *ob_type; Py_ssize_t ob_size; //容纳元素的个数 } PyVarObject; /*intobject.h*/ typedef struct { PyObject_HEAD long ob_ival; } PyIntObject; /*listobject.h*/ typedef struct { PyObject_VAR_HEAD /* Vector of pointers to list elements. list[0] is ob_item[0], etc. */ PyObject **ob_item; /* ob_item contains space for &#39;allocated&#39; elements. The number * currently in use is ob_size. * Invariants: * 0 &lt;= ob_size &lt;= allocated * len(list) == ob_size * ob_item == NULL implies ob_size == allocated == 0 * list.sort() temporarily sets allocated to -1 to detect mutations. * * Items must normally not be NULL, except during construction when * the list is not yet visible outside the function that builds it. */ Py_ssize_t allocated; } PyListObject; /*iterobject.c*/ typedef struct { PyObject_HEAD long it_index; PyObject *it_seq; /* Set to NULL when iterator is exhausted */ } seqiterobject; /*funcobject.h*/ typedef struct { PyObject_HEAD PyObject *func_code; /* A code object */ PyObject *func_globals; /* A dictionary (other mappings won&#39;t do) */ PyObject *func_defaults; /* NULL or a tuple */ PyObject *func_closure; /* NULL or a tuple of cell objects */ PyObject *func_doc; /* The __doc__ attribute, can be anything */ PyObject *func_name; /* The __name__ attribute, a string object */ PyObject *func_dict; /* The __dict__ attribute, a dict or NULL */ PyObject *func_weakreflist; /* List of weak references */ PyObject *func_module; /* The __module__ attribute, can be anything */ /* Invariant: * func_closure contains the bindings for func_code-&gt;co_freevars, so * PyTuple_Size(func_closure) == PyCode_GetNumFree(func_code) * (func_closure may be NULL if PyCode_GetNumFree(func_code) == 0). */ } PyFunctionObject; /*genobject.h*/ typedef struct { PyObject_HEAD /* The gi_ prefix is intended to remind of generator-iterator. */ /* Note: gi_frame can be NULL if the generator is &quot;finished&quot; */ struct _frame *gi_frame; /* True if generator is being executed. */ int gi_running; /* The code object backing the generator */ PyObject *gi_code; /* List of weak reference. */ PyObject *gi_weakreflist; } PyGenObject; /*fileobject.h*/ typedef struct { PyObject_HEAD FILE *f_fp; PyObject *f_name; PyObject *f_mode; int (*f_close)(FILE *); int f_softspace; /* Flag used by &#39;print&#39; command */ int f_binary; /* Flag which indicates whether the file is open in binary (1) or text (0) mode */ char* f_buf; /* Allocated readahead buffer */ char* f_bufend; /* Points after last occupied position */ char* f_bufptr; /* Current buffer position */ char *f_setbuf; /* Buffer for setbuf(3) and setvbuf(3) */ int f_univ_newline; /* Handle any newline convention */ int f_newlinetypes; /* Types of newlines seen */ int f_skipnextlf; /* Skip next \n */ PyObject *f_encoding; PyObject *f_errors; PyObject *weakreflist; /* List of weak references */ int unlocked_count; /* Num. currently running sections of code using f_fp with the GIL released. */ int readable; int writable; } PyFileObject; /*floatobject.h*/ typedef struct { PyObject_HEAD double ob_fval; } PyFloatObject; /*frameobject.h*/ typedef struct { int b_type; /* what kind of block this is */ int b_handler; /* where to jump to find handler */ int b_level; /* value stack level to pop to */ } PyTryBlock; typedef struct _frame { PyObject_VAR_HEAD struct _frame *f_back; /* previous frame, or NULL */ PyCodeObject *f_code; /* code segment */ PyObject *f_builtins; /* builtin symbol table (PyDictObject) */ PyObject *f_globals; /* global symbol table (PyDictObject) */ PyObject *f_locals; /* local symbol table (any mapping) */ PyObject **f_valuestack; /* points after the last local */ /* Next free slot in f_valuestack. Frame creation sets to f_valuestack. Frame evaluation usually NULLs it, but a frame that yields sets it to the current stack top. */ PyObject **f_stacktop; PyObject *f_trace; /* Trace function */ /* If an exception is raised in this frame, the next three are used to * record the exception info (if any) originally in the thread state. See * comments before set_exc_info() -- it&#39;s not obvious. * Invariant: if _type is NULL, then so are _value and _traceback. * Desired invariant: all three are NULL, or all three are non-NULL. That * one isn&#39;t currently true, but &quot;should be&quot;. */ PyObject *f_exc_type, *f_exc_value, *f_exc_traceback; PyThreadState *f_tstate; int f_lasti; /* Last instruction if called */ /* Call PyFrame_GetLineNumber() instead of reading this field directly. As of 2.3 f_lineno is only valid when tracing is active (i.e. when f_trace is set). At other times we use PyCode_Addr2Line to calculate the line from the current bytecode index. */ int f_lineno; /* Current line number */ int f_iblock; /* index in f_blockstack */ PyTryBlock f_blockstack[CO_MAXBLOCKS]; /* for try and loop blocks */ PyObject *f_localsplus[1]; /* locals+stack, dynamically sized */ } PyFrameObject; /*classobject.h*/ typedef struct { PyObject_HEAD PyObject *cl_bases; /* A tuple of class objects */ PyObject *cl_dict; /* A dictionary */ PyObject *cl_name; /* A string */ /* The following three are functions or NULL */ PyObject *cl_getattr; PyObject *cl_setattr; PyObject *cl_delattr; PyObject *cl_weakreflist; /* List of weak references */ } PyClassObject; typedef struct { PyObject_HEAD PyClassObject *in_class; /* The class object */ PyObject *in_dict; /* A dictionary */ PyObject *in_weakreflist; /* List of weak references */ } PyInstanceObject; typedef struct { PyObject_HEAD PyObject *im_func; /* The callable object implementing the method */ PyObject *im_self; /* The instance it is bound to, or NULL */ PyObject *im_class; /* The class that asked for the method */ PyObject *im_weakreflist; /* List of weak references */ } PyMethodObject; /*dictobject.h*/ typedef struct { /* Cached hash code of me_key. Note that hash codes are C longs. * We have to use Py_ssize_t instead because dict_popitem() abuses * me_hash to hold a search finger. */ Py_ssize_t me_hash; PyObject *me_key; PyObject *me_value; } PyDictEntry; typedef struct _dictobject PyDictObject; struct _dictobject { PyObject_HEAD Py_ssize_t ma_fill; /* # Active + # Dummy */ Py_ssize_t ma_used; /* # Active */ /* The table contains ma_mask + 1 slots, and that&#39;s a power of 2. * We store the mask instead of the size because the mask is more * frequently needed. */ Py_ssize_t ma_mask; /* ma_table points to ma_smalltable for small tables, else to * additional malloc&#39;ed memory. ma_table is never NULL! This rule * saves repeated runtime null-tests in the workhorse getitem and * setitem calls. */ PyDictEntry *ma_table; PyDictEntry *(*ma_lookup)(PyDictObject *mp, PyObject *key, long hash); PyDictEntry ma_smalltable[PyDict_MINSIZE]; }; /*funcobject.h*/ typedef struct { PyObject_HEAD PyMethodDef *m_ml; /* Description of the C function to call */ PyObject *m_self; /* Passed as &#39;self&#39; arg to the C func, can be NULL */ PyObject *m_module; /* The __module__ attribute, can be anything */ } PyCFunctionObject; /*code.h*/ /* Bytecode object */ typedef struct { PyObject_HEAD int co_argcount; /* #arguments, except *args */ int co_nlocals; /* #local variables */ int co_stacksize; /* #entries needed for evaluation stack */ int co_flags; /* CO_..., see below */ PyObject *co_code; /* instruction opcodes */ PyObject *co_consts; /* list (constants used) */ PyObject *co_names; /* list of strings (names used) */ PyObject *co_varnames; /* tuple of strings (local variable names) */ PyObject *co_freevars; /* tuple of strings (free variable names) */ PyObject *co_cellvars; /* tuple of strings (cell variable names) */ /* The rest doesn&#39;t count for hash/cmp */ PyObject *co_filename; /* string (where it was loaded from) */ PyObject *co_name; /* string (name, for reference) */ int co_firstlineno; /* first source line number */ PyObject *co_lnotab; /* string (encoding addr&lt;-&gt;lineno mapping) See Objects/lnotab_notes.txt for details. */ void *co_zombieframe; /* for optimization only (see frameobject.c) */ PyObject *co_weakreflist; /* to support weakrefs to code objects */ } PyCodeObject; /*object.h*/ typedef struct _typeobject { PyObject_VAR_HEAD const char *tp_name; /* For printing, in format &quot;&lt;module&gt;.&lt;name&gt;&quot; */ Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */ /* Methods to implement standard operations */ destructor tp_dealloc; printfunc tp_print; getattrfunc tp_getattr; setattrfunc tp_setattr; cmpfunc tp_compare; reprfunc tp_repr; /* Method suites for standard classes */ PyNumberMethods *tp_as_number; PySequenceMethods *tp_as_sequence; PyMappingMethods *tp_as_mapping; /* More standard operations (here for binary compatibility) */ hashfunc tp_hash; ternaryfunc tp_call; reprfunc tp_str; getattrofunc tp_getattro; setattrofunc tp_setattro; /* Functions to access object as input/output buffer */ PyBufferProcs *tp_as_buffer; /* Flags to define presence of optional/expanded features */ long tp_flags; const char *tp_doc; /* Documentation string */ /* Assigned meaning in release 2.0 */ /* call function for all accessible objects */ traverseproc tp_traverse; /* delete references to contained objects */ inquiry tp_clear; /* Assigned meaning in release 2.1 */ /* rich comparisons */ richcmpfunc tp_richcompare; /* weak reference enabler */ Py_ssize_t tp_weaklistoffset; /* Added in release 2.2 */ /* Iterators */ getiterfunc tp_iter; iternextfunc tp_iternext; /* Attribute descriptor and subclassing stuff */ struct PyMethodDef *tp_methods; struct PyMemberDef *tp_members; struct PyGetSetDef *tp_getset; struct _typeobject *tp_base; PyObject *tp_dict; descrgetfunc tp_descr_get; descrsetfunc tp_descr_set; Py_ssize_t tp_dictoffset; initproc tp_init; allocfunc tp_alloc; newfunc tp_new; freefunc tp_free; /* Low-level free-memory routine */ inquiry tp_is_gc; /* For PyObject_IS_GC */ PyObject *tp_bases; PyObject *tp_mro; /* method resolution order */ PyObject *tp_cache; PyObject *tp_subclasses; PyObject *tp_weaklist; destructor tp_del; /* Type attribute cache version tag. Added in version 2.6 */ unsigned int tp_version_tag; #ifdef COUNT_ALLOCS /* these must be last and never explicitly initialized */ Py_ssize_t tp_allocs; Py_ssize_t tp_frees; Py_ssize_t tp_maxalloc; struct _typeobject *tp_prev; struct _typeobject *tp_next; #endif } PyTypeObject; /*longintrepr.h*/ /* Long integer representation. The absolute value of a number is equal to SUM(for i=0 through abs(ob_size)-1) ob_digit[i] * 2**(SHIFT*i) Negative numbers are represented with ob_size &lt; 0; zero is represented by ob_size == 0. In a normalized number, ob_digit[abs(ob_size)-1] (the most significant digit) is never zero. Also, in all cases, for all valid i, 0 &lt;= ob_digit[i] &lt;= MASK. The allocation function takes care of allocating extra memory so that ob_digit[0] ... ob_digit[abs(ob_size)-1] are actually available. CAUTION: Generic code manipulating subtypes of PyVarObject has to aware that longs abuse ob_size&#39;s sign bit. */ struct _longobject { PyObject_VAR_HEAD digit ob_digit[1]; }; typedef struct _longobject PyLongObject; /* Revealed in longintrepr.h */]]></content>
      <categories>
        <category>python源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[c语言typedef]]></title>
    <url>%2F2017%2F12%2F09%2Fc%E8%AF%AD%E8%A8%80typedef%2F</url>
    <content type="text"><![CDATA[/*定义identifier为宏，指令为identifier后的一系列指令（以前认为只能是一句）*/ #define identifier replacement-list(optional) #define PyObject_HEAD \ Py_ssize_t ob_refcnt; \ struct _typeobject *ob_type; typedef struct _object { PyObject_HEAD } PyObject; //相当于： typedef struct _object { Py_ssize_t ob_refcnt; struct _typeobject *ob_type; } PyObject; #include &lt;stdio.h&gt; #define T int b;int c; typedef struct gg { T } gg; int main(int argc, char *argv[]){ gg g = {1, 2}; printf(&quot;%d\n&quot;, g.b); printf(&quot;%d\n&quot;, g.c); return 0; } 输出： 1 2]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python intobject大小]]></title>
    <url>%2F2017%2F12%2F09%2Fpython%20intobject%E5%A4%A7%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[# 64位系统 #python3 python2 In [4]: sys.getsizeof(1) In [4]: sys.getsizeof(1) Out[4]: 28 Out[4]: 24 In [6]: sys.getsizeof(0) In [6]: sys.getsizeof(0) Out[6]: 24 Out[6]: 24 在python2 int对象为24个字节, #define PyObject_HEAD \ Py_ssize_t ob_refcnt; \ struct _typeobject *ob_type; typedef struct { PyObject_HEAD long ob_ival; } PyIntObject; python3中已不区分int,long对象，而是将python2中的long对象改名为int对象 typedef unsigned short digit; struct _longobject { PyObject_VAR_HEAD digit ob_digit[1]; }; python3中sys.getsizeof(0) == 24 /*zero is represented by ob_size == 0.当int值为0时，ob_size=0，所以不会分配 ob_digit[1],所以大小为24Bytes.*/ Ref：python源码]]></content>
      <categories>
        <category>python源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[apue第一章:UNIX系统概览-笔记-函数]]></title>
    <url>%2F2017%2F12%2F03%2Fapue%E7%AC%AC%E4%B8%80%E7%AB%A0%20UNIX%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%A7%88-%E7%AC%94%E8%AE%B0-%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[opendir, fdopendidr ---打开目录 #include &lt;sys/types.h&gt; #include &lt;dirent.h&gt; DIR *opendir(const char *name); DIR *fdopendir(int fd); struct dirent *readdir(DIR *dirp); struct __dirstream { void *__fd; char *__data; int __entry_data; char *__ptr; int __entry_ptr; size_t __allocation; size_t __size; __libc_lock_define (, __lock) }; typedef struct __dirstream DIR; struct dirent { ino_t d_ino; /* inode number */ off_t d_off; /* not an offset; see NOTES */ unsigned short d_reclen; /* length of this record */ unsigned char d_type; /* type of file; not supported by all filesystem types */ char d_name[256]; /* filename */ }; read write #include &lt;unistd.h&gt; /*从文件描述符fd中读取count字节到buf中*/ ssize_t read(int fd, void *buf, size_t count); ssize_t write(int fd, const void *buf, size_t count); #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; /*返回进程ID*/ pid_t getpid(void); /*返回进程的父亲进程ID*/ pid_t getppid(void); fork execlp #include &lt;sys/types.h&gt; #include &lt;unistd.h&gt; pid_t fork(void); #include &lt;string.h&gt; /*返回指向错误信息字符串的指针*/ char *strerror(int errnum); #include &lt;stdio.h&gt; /*根据errno的值返回错误信息*/ void perror(const char *msg); getuid getgid #include &lt;unistd.h&gt; uid_t getuid(void); gid_t getgid(void); waitpid #include &lt;sys/wait.h&gt; pid_t waitpid(pid_t pid, int *stat_loc, int options);]]></content>
      <categories>
        <category>apue</category>
      </categories>
      <tags>
        <tag>apue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apue第一章:UNIX系统概览-笔记]]></title>
    <url>%2F2017%2F12%2F03%2Fpue%E7%AC%AC%E4%B8%80%E7%AB%A0%20UNIX%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%A7%88-%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.unix架构 狭义上说，操作系统被定义为可以利用,控制硬件资源和为程序提供运行环境。广义上说，操作系统就是通过内核和其实相关软件使电脑可以被使用和拥有其属性。 2.用户登录 当用户进行登录操作时，系统会从密码文件中查找用户名，通常这个文件为/etc/passwd,后面的操作系统把加密后的密码存放在/etc/shadow中,/etc/shadow介绍。# /etc/passwd hys:x:1000:1000:hys,,,:/home/hys:/bin/zsh 用户名 加密后密码 用户ID 组ID 注释字段 home目录 shell hys x 1000 1000 hys,,, /home/hys/ /bin/zsh # 打印出系统中的所有用户 cat /etc/passwd | awk -F : &#39;{print $1}&#39; 3.以’/‘开头的路径叫绝对路径，反之都是相对路径。 4.输入，输出 文件描述符，通常是一个非负数，内核用于标识被特定进程访问的文件。无论何时运行一个程序，shell都会打开三个描述符:标准输入，标准输出，标准错误。无缓冲I/O，不使用缓冲的输入/输出，如系统调用:open,read,write,lseek,close。 5.程序和进程 程序是指存储在磁盘中的可执行文件。程序的执行实例称为进程，进程ID用于标识进程的唯一非负数。线程，通常一个进程只有一个线程，多线程可以提高对多核处理器的利用。在同一进程中的所有线程共享相同的地址空间，文件描述符，栈和进程相关属性。 图片来源：&lt;&lt;程序员的自我修养&gt;&gt; 6.用户标识 User ID非负数，系统用于标识用户Group ID，组ID，用于用户分组# /etc/group adm:x:4:syslog,hys 组名 口令 组ID 所属组用户(用,隔开) adm x 4 syslog,hys 7.信号 信号的三种处理方式：1.忽略，2.默认动作，3.提供自定义动作8.时间值 Clock time，程序运行所消耗的时间User CPU time,进程获得CPU资源后，在用户态的执行时间System CPU time,进程获得CPU资源后，在内核态的执行时间 9.系统调用与库函数 系统调用 库函数 数量有限 数量较多 直接调用内核服务 不能直接调用内核服务 精简高效，只提供最底层服务 对用户友好，属于用户层函数 c函数 c函数 不可替换 库函数可替换，而系统调用不可]]></content>
      <categories>
        <category>apue</category>
      </categories>
      <tags>
        <tag>apue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python C API 迭代器]]></title>
    <url>%2F2017%2F12%2F02%2Fpython%20C%20API%20%E8%BF%AD%E4%BB%A3%E5%99%A8%2F</url>
    <content type="text"><![CDATA[#include &lt;stdio.h&gt; #include &quot;python3.5m/Python.h&quot; int main(int argc, char* argv[]){ long v; Py_Initialize(); PyObject *t, *item; t = PyList_New(3); PyList_SetItem(t, 0, PyLong_FromLong(1L)); PyList_SetItem(t, 1, PyLong_FromLong(2L)); PyList_SetItem(t, 2, PyLong_FromLong(3L)); PyObject *iterator = PyObject_GetIter(t); if (iterator == NULL){ printf(&quot;error\n&quot;); } while (item = PyIter_Next(iterator)){ v = PyLong_AsLong(item); printf(&quot;%ld\n&quot;, v); Py_DECREF(item); } Py_DECREF(iterator); Py_Finalize(); return 0; } output: 1 2 3 /*返回迭代器对象*/ PyObject* PyObject_GetIter(PyObject *o) /*获取迭代器o的下 一个值*/ PyObject* PyIter_Next(PyObject *o) /*将python对象转换为c long*/ long PyLong_AsLong(PyObject *pylong)]]></content>
      <categories>
        <category>python源码</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[c调用python3 C API]]></title>
    <url>%2F2017%2F11%2F30%2Fc%E8%B0%83%E7%94%A8python3%20C%20API%2F</url>
    <content type="text"><![CDATA[/*创建一个长度为3的列表，并对其进行赋值，并打印*/ /*hello.c*/ #include &lt;stdio.h&gt; #include &quot;python3.5m/Python.h&quot; int main(int argc, char *argv[]) { int i=0; long value; Py_Initialize(); PyObject *t, *item; t = PyList_New(3); PyList_SetItem(t, 0, PyLong_FromLong(1L)); PyList_SetItem(t, 1, PyLong_FromLong(2L)); PyList_SetItem(t, 2, PyLong_FromLong(3L)); for(i=0; i&lt;3;i++) { item = PyList_GetItem(t, i); value = PyLong_AsLong(item); printf(&quot;%ld\n&quot;, value); } Py_Finalize(); return 0; } void Py_Initialize()//初始化python解析器，必须在调用Python/C API之前调用 PyObject* PyList_New(Py_ssize_t len)//创建一个长度为len的列表，Return:List Reference OR NULL(创建失败) /*设置list索引为index的值为item,Return: 0 OR -1*/ int PyList_SetItem(PyObject *list, Py_ssize_t index, PyObject *item) /*获取list索引为index的值,Return: Borrowed reference OR NULL and set an IndexError exception.*/ PyObject* PyList_GetItem(PyObject *list, Py_ssize_t index) /*返回C类型的变量*/ long PyLong_AsLong(PyObject *obj) /*释放资源*/ void Py_Finalize() /*编译运行,ubuntu 16.04,gcc5.4.0,python3.5*/ /*-L 指定库的路径，-l 指定需连接的库名 如果文件为libpython3.5.so,刚库名为python3.5*/ gcc hello.c -L/usr/lib/python3.5/config-3.5m-x86_64-linux-gnu -lpython3.5 out: 1 2 3 若/usr/include/python3.5m下无Python.h文件，请RefPython.h Ref： 1.官方文档]]></content>
      <categories>
        <category>C语言</category>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu未发现Python.h文件]]></title>
    <url>%2F2017%2F11%2F30%2Fubuntu%E6%9C%AA%E5%8F%91%E7%8E%B0Python.h%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[/usr/include/python3.5m下没有Python.h头文件sudo apt-get install python3-dev]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib.pyplot相关函数]]></title>
    <url>%2F2017%2F11%2F28%2Fmatplotlib.pyplot%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[import matplotlib.pyplot as plt plt.title(s, *args, **kwargs) #标题 plt.xlabel(s, *args, **kwargs) #x轴标题 plt.ylabel(s, *args, **kwargs) #y轴标题 plt.autoscale() #自动适应 plt.xlim(0, 100) #设置x轴坐标 范围 plt.legend() #左上角或右上角上的说明 plt.grid(True,linestyle=&#39;-&#39;, color=&#39;0.75&#39;) #网格 plt.xticks(locs, [label]) #Set locations and labels plt.axis([xmin, xmax, ymin, ymax]) #设置x,y轴的最小最大坐标值 matplotlib.pyplot.subplot(*args, **kwargs) 返回一个子图坐标系 subplot(nrows, ncols, plot_number) subplot(211) subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw) 创建个图表和一系列子图 return fig ax:坐标对象 plt.imshow(X, cmap=&#39;Greys&#39;) #展示关于X的图像，X可以是矩阵或PIL Image plt.matshow() #将矩阵以图像的形式展示出来]]></content>
      <categories>
        <category>pyplot</category>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pyplot.pie]]></title>
    <url>%2F2017%2F11%2F27%2Fpyplot-pie%2F</url>
    <content type="text"><![CDATA[matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=None, radius=None, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, hold=None, data=None) 根据数组x绘制饼图 explode:数组扇形离圆点的距离(爆炸出来的那部分) labels:标题 colors:颜色 autopct:饼图扇区面积所显示的文字 shadow:阴影 startangle: title:图标题 import matplotlib.pyplot as plt # Pie chart, where the slices will be ordered and plotted counter-clockwise: labels = &#39;Frogs&#39;, &#39;Hogs&#39;, &#39;Dogs&#39;, &#39;Logs&#39; sizes = [15, 30, 45, 10] explode = (0, 0.1, 0, 0) # only &quot;explode&quot; the 2nd slice (i.e. &#39;Hogs&#39;) fig1, ax1 = plt.subplots() ax1.pie(sizes, explode=explode, labels=labels, autopct=&#39;%1.1f%%&#39;, shadow=True, startangle=90) #autopct 扇形区域显示的文字格式 ax1.axis(&quot;equal&quot;) # Equal aspect ratio ensures that pie is drawn as a circle. plt.show() #通过两个饼图绘制双层饼图 import matplotlib.pyplot as plt vals1 = [1] vals2 = [1, 1, 2] vals3 = [1] vals4 = [1] fig, ax = plt.subplots() labels = &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39; ax.pie(vals1, radius=1.2) ax.pie(vals2, radius=1.0,startangle=90) ax.pie(vals3, radius=0.8) ax.pie(vals4, radius=0.4) ax.set(aspect=&quot;equal&quot;) plt.show() Ref： 1.https://zhuanlan.zhihu.com/p/27442584 2.http://matplotlib.org/api/pyplot_api.html]]></content>
      <categories>
        <category>pyplot</category>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pyplot.[xcorr,acorr]]]></title>
    <url>%2F2017%2F11%2F26%2Fpyplot-xcorracorr%2F</url>
    <content type="text"><![CDATA[自相关（英语：Autocorrelation），也叫序列相关[1]，是一个信号于其自身在不同时间点的互相关。非正式地来说，它就是两次观察之间的相似度对它们之间的时间差的函数。它是找出重复模式（如被噪声掩盖的周期信号），或识别隐含在信号谐波频率中消失的基频的数学工具。它常用于信号处理中，用来分析函数或一系列值，如时域信号。 在统计学中，互相关有时用来表示两个随机矢量X和Y之间的协方差cov（X, Y），以与矢量X的“协方差”概念相区分，矢量X的“协方差”是X的各标量成分之间的协方差矩阵。 在信号处理领域中，互相关（有时也称为“互协方差”）是用来表示两个信号之间相似性的一个度量，通常通过与已知信号比较用于寻找未知信号中的特性。它是两个信号之间相对于时间的一个函数，有时也称为滑动点积，在模式识别以及密码分析学领域都有应用。 matplotlib.pyplot.acorr(x, hold=None, data=None, **kwargs) #x的自相关 matplotlib.pyplot.xcorr(x, y, normed=True, detrend=&lt;function detrend_none&gt;, usevlines=True, maxlags=10, hold=None, data=None, **kwargs) #画出x与y的互相关关系 import matplotlib.pyplot as plt import numpy as np np.random.seed(0) #用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed( )值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同 x, y = np.random.randn(2, 100)#返回一个二维，长度为100的数组 fig = plt.figure() #添加一个画布 ax1 = fig.add_subplot(211)#添加个子图，两行一列中的第一个图 ax1.xcorr(x, y, usevlines=True, maxlags=50, normed=True, lw=2)#互相关函数 ax1.grid(True) #添加网格 ax1.axhline(0, color=&#39;black&#39;, lw=2) ax2 = fig.add_subplot(212, sharex=ax1) ax2.acorr(x, usevlines=True, normed=True, maxlags=50, lw=2)#自相关函数 ax2.grid(True) ax2.axhline(0, color=&#39;black&#39;, lw=2) plt.show() Ref： 1.http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.xcorr]]></content>
      <categories>
        <category>pyplot</category>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sizeof]]></title>
    <url>%2F2017%2F11%2F14%2Fsizeof%2F</url>
    <content type="text"><![CDATA[sizeof是C语言的关键字，返回数据类型长度 32位系统中返回值类型为 unsigned int 64位系统中返回值类型为 long unsigned int #include&lt;stdio&gt; int main(void){ printf(&quot;char_size=%lu\n&quot;, sizeof(char));//32位系统中需改为%u printf(&quot;unsigned_char=%lu\n&quot;, sizeof(unsigned char)); printf(&quot;signed_char=%lu\n&quot;, sizeof(signed char)); printf(&quot;\n\n&quot;); printf(&quot;int_size=%lu\n&quot;, sizeof(int)); printf(&quot;short_int_size=%lu\n&quot;, sizeof(short int)); printf(&quot;long_int_size=%lu\n&quot;, sizeof(long int)); printf(&quot;unsigned_long_size=%lu\n&quot;, sizeof(unsigned long)); printf(&quot;float_size=%lu\n&quot;, sizeof(float)); printf(&quot;double_size=%lu\n&quot;, sizeof(double)); printf(&quot;long_double_size=%lu\n&quot;, sizeof(long double)); return 0; } 输出 //32位 //64位 char_size=1 char_size=1 unsigned_char=1 unsigned_char=1 signed_char=1 signed_char=1 int_size=4 int_size=4 unsigned_int_size=4 unsigned_int_size=4 short_int_size=2 short_int_size=2 long_int_size=4 long_int_size=8 unsigned_long_size=4 unsigned_long_size=8 float_size=4 float_size=4 double_size=8 double_size=8 long_double_size=12 long_double_size=16]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[/etc/shadow及相关c函数]]></title>
    <url>%2F2017%2F11%2F13%2Fetc%20shadow%E5%8F%8A%E7%9B%B8%E5%85%B3c%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[/etc/shadow包含系统用户的密码信息，普通用户不能查看此文件 每行分为9个片段，通过冒号’:’分隔 hys:$6$q1Jaurz4$8hPn.i0F6k7xPeLNcKDGdKZODMx7uTnHjAXjgvND2dRrz7jk16O.DbI15qe.G9SqSbFO.O0PMsW.yHv1X2/z60:17421:0:99999:7::: 用户名： 加密后的密码(单向散列)： 最后修改密码的时间(为距1970.1.1的天数)： 密码最小存活时间(指用户还有多少天可以修改密码，空或0表示没有密码最小存活时间): 最大密码存活时间(超过这个时间，用户需修改密码，如果这个值小于密码最小存活时间，则用户不能修改密码)： 密码过期前多少天提醒： 密码超过最大存活时间后还能进行登录的时间： 用户过期时间： 保留域 struct spwd { char *sp_namp; /* Login name */ char *sp_pwdp; /* Encrypted password */ long sp_lstchg; /* Date of last change(measured in days since 1970-01-01 00:00:00 +0000 (UTC)) */ long sp_min; /* Min # of days between changes */ long sp_max; /* Max # of days between changes */ long sp_warn; /* # of days before password expires to warn user to change it */ long sp_inact; /* # of days after password expires until account is disabled */ long sp_expire; /* Date when account expires(measured in days since 1970-01-01 00:00:00 +0000 (UTC)) */ unsigned long sp_flag; /* Reserved */ }; struct spwd *getspnam(const char *name);//通过用户名获取指向struct spwd结构体的指针 struct spwd *getspent(void);//获取/etc/shadow文件中的信息，返回struct spwd结构体指针 void setspent(void);//初始化输入流 void endspent(void);//释放资源 例： #include &lt;stdio.h&gt; #include &lt;shadow.h&gt; #include &lt;unistd.h&gt; int main(){ struct spwd *p,*ptr; p = getspnam(usbmux); if(p == NULL){ printf(&quot;error&quot;); } printf(&quot;%s\n&quot;, p-&gt;sp_namp); printf(&quot;%s\n&quot;, p-&gt;sp_pwdp); printf(&quot;%ld\n&quot;, p-&gt;sp_lstchg); printf(&quot;%ld\n&quot;, p-&gt;sp_min); printf(&quot;%ld\n&quot;, p-&gt;sp_max); printf(&quot;%ld\n&quot;, p-&gt;sp_warn); printf(&quot;%ld\n&quot;, p-&gt;sp_inact); printf(&quot;%ld\n&quot;, p-&gt;sp_expire); printf(&quot;%lu\n&quot;, p-&gt;sp_flag); setspent(); while((ptr = getspent())!=NULL){ printf(&quot;%s\n&quot;, ptr-&gt;sp_namp); } endspent(); return 0; } Ref：1.http://manpages.ubuntu.com/manpages/precise/man5/shadow.5.html2.https://linux.die.net/man/3/getspnam]]></content>
      <categories>
        <category>C语言</category>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装zsh,oh-my-zsh,autojump,terminator]]></title>
    <url>%2F2017%2F11%2F12%2Fbuntu%E5%AE%89%E8%A3%85zsh%2Coh-my-zsh%2Cautojump%2Cterminator%2F</url>
    <content type="text"><![CDATA[1.安装zsh,oh-my-zsh sudo apt-get install zsh git wget wget --no-check-certificate https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | sh chsh -s /bin/zsh 注销重新进入后生效 2.安装autojump git clone git://github.com/joelthelion/autojump.git cd autojump sudo ./install.py . /home/hys/.autojump/etc/profile.d/autojump.sh #在~/.zshrc文件最后添加 source ~/.zshrc 重启终端后生效 或者直接通过软件仓库安装 sudo apt-get install autojump . /usr/share/autojump/autojump.sh #在~/.zshrc文件最后添加 source ~/.zshrc 重启终端后生效 3.安装terminator sudo apt-get install terminator 安装oh-my-zsh可能会出现这样的错误 1.you may not change the shell for ‘username’ sudo usermod -s /bin/zsh username]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[extern "C"]]></title>
    <url>%2F2017%2F11%2F11%2Fextern-c%2F</url>
    <content type="text"><![CDATA[C++为了与C兼容，在符号的管理上，C++提供了一 个用来声明或定义C符号的”extern “C””关键字。C++编译器会将在extern “C”的大括号内部的代码当作C语言处理，C++的名称修饰机制将不起作用。 很多时候我们会碰到有些头文件声明了一些C语言的 函数和全局变量，但是这个头文件可能会被C语言代码或c++代码包含。比 如很常见的，我们的C语言库函数中的string.h中声明了memset这个函数，它的原型如下： void *memset (void *, int ,size); 如果不加任何处理，当我们的C语言程序包含string.h的时候，并且用到了memset这个函数 ，编译器会将memset符号引用正确处理；但是在C++语言中，编译器会认为这个memset函数是一个C++函数，将memset的符号修饰成_Z6memsetPvii,这样链接器就无法与C语言库中的memset符号进行链接。所以对于C++来说，必须使用extern “C”来声明memset这个函数。但是C语言又不支持extern “C”语法，如果为了兼容C语言和C++语言定义两套头文件，未免于麻烦。幸好我们有一种很好的方法可以解决上述问题，就是使用C++的宏”__cplusplus”,C++编译会在编译c++的程序默认定义这个宏，我们可以使用条件宏来判断当前编译单元是不是C++代码。具体代码如下： #ifdef __cplusplus extern &quot;C&quot; { #endif void *memset (void *, int, size_t); #ifdef __cplusplus } #endif 如果当前编译单元是C++代码，那么memset会在extern “C”里面被声明；如果是C代码，就直接声明。 Ref： 1.程序员的自我修养 2.https://www.cnblogs.com/stonecrazyking/archive/2006/09/23/512552.html]]></content>
      <categories>
        <category>C语言</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket string类型]]></title>
    <url>%2F2017%2F11%2F09%2Fracket%20string%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[(string-ref str k) → char? ;通过下标获取字符串元素 str : string? k : exact-nonnegative-integer?&lt;/pre&gt; &gt;(string-ref &quot;hello&quot; 0) #\h (make-string k [char]) → string? k : exact-nonnegative-integer? char : char? = #\nul 通过给定一个字符串长度，和一个默认填充字符串，返回一个可改变的字符串 &gt;(define s (make-string 5 #\z)) &gt;s &quot;zzzzz&quot; (string-set! str k char) → void? str : (and/c string? (not/c immutable?)) k : exact-nonnegative-integer? char : char? 修改可变字符串str的第k个字符串为char&lt;/pre&gt; &gt;(string-set! s 2 #\&amp;) &gt;s &quot;zz&amp;zz&quot; (string&lt;? str1 str2 ...+) → boolean? str1 : string? str2 : string? 字符串比较，如果str1&lt;str2 ...+,刚返回#t, (string&lt;=? str1 str2 ...+) → boolean? str1 : string? str2 : string? (string=? str1 str2 ...+) → boolean? str1 : string? str2 : string? 判断str1,str2 ...+是否相等 (string&gt;? str1 str2 ...+) → boolean? str1 : string? str2 : string?&lt;/pre&gt; (string-copy! dest ;目标字符串 dest-start ;目标字符串起始下标 src ;源字符串 [src-start ;源字符串起始下标 src-end]) → void? ;源字符串结束下标 dest : (and/c string? (not/c immutable?)) dest-start : exact-nonnegative-integer? src : string? src-start : exact-nonnegative-integer? = 0 src-end : exact-nonnegative-integer? = (string-length src) &gt;(string-copy! s 0 &quot;hello&quot; 0 5) &gt;s &quot;hello&quot; (string-fill! dest char) → void? dest : (and/c string? (not/c immutable?)) char : char? 填充字符串&lt;/pre&gt; &gt;(string-fill! s #\q) &gt;s &quot;qqqqq&quot; (string-append str ...) → string? str : string? 返回一个新的拼接的字符串&lt;/pre&gt; &gt;(string-append &quot;Apple&quot; &quot;Banana&quot;) &quot;AppleBanana&quot; (string-length str) → exact-nonnegative-integer? str : string? 返回字符串长度&lt;/pre&gt; &gt;(string-length &quot;Hello&quot;) 5 (substring str start [end]) → string? str : string? start : exact-nonnegative-integer? end : exact-nonnegative-integer? = (string-length str) 获取子字符串&lt;/pre&gt; &gt;(substring &quot;hello&quot; 0 4) &quot;hell&quot; (string-&gt;list str) → (listof char?) str : string? 将字符串转换为列表&lt;/pre&gt; &gt;(string-&gt;list &quot;Apple&quot;) &#39;(#\A #\p #\p #\l #\e) (list-&gt;string lst) → string? lst : (listof char?) 将列表转换为string&lt;/pre&gt; &gt;(list-&gt;string (list #\A #\p #\p #\l #\e)) &quot;Apple&quot; (build-string n proc) → string? n : exact-nonnegative-integer? proc : (exact-nonnegative-integer? . -&gt;. char?) 通过proc以0-n的顺序创建一个长度为n的字符串&lt;/pre&gt; &gt;(build-string 5 (lambda (i) (integer-&gt;char (+ i 97)))) &quot;abcde&quot; (string-ci=? str1 str2 ...+) → boolean? str1 : string? str2 : string? 当参数str1,str2 ...+经过函数string-foldcase处理，若str1,str2 ...+相等则返回true (string-ci&lt;? str1 str2 ...+) → boolean? str1 : string? str2 : string? (string-ci&lt;=? str1 str2 ...+) → boolean? str1 : string? str2 : string? (string-ci&gt;? str1 str2 ...+) → boolean? str1 : string? str2 : string?&lt;/pre&gt; &gt;(string-ci=? &quot;Apple&quot; &quot;apple&quot;) #t (string-upcase str) → string? str : string? 将字符串转换为大写 (string-downcase string) → string? string : string? 将字符串转换为小写 (string-titlecase string) → string? string : string? 将单词的第一个字母进行大写 (string-foldcase string) → string? string : string? 将字符串转换为小写，特殊字符存在某种转换&lt;/pre&gt; &gt;(string-foldcase &quot;Straße&quot;) &quot;strasse&quot; &gt;(string-downcase &quot;Straße&quot;) &quot;straße&quot; (string-join strs ;以指定分隔符连接字符串 [sep #:before-first before-first #:before-last before-last #:after-last after-last]) → string? strs : (listof string?) sep : string? = &quot; &quot; before-first : string? = &quot;&quot; before-last : string? = sep after-last : string? = &quot;&quot;) &gt;(string-join &#39;(&quot;one&quot; &quot;two&quot; &quot;three&quot; &quot;four&quot;) &quot;;&quot;) &quot;one;two;three;four&quot; (string-split str ;切分字符串，返回一个列表 [sep #:trim? trim? #:repeat? repeat?]) → (listof string?) str : string? sep : (or/c string? regexp?) = #px&quot;\\s+&quot; trim? : any/c = #t repeat? : any/c = #f)&lt;/pre&gt; &gt;(string-split &quot; foo bar baz \r\n\t&quot;) &#39;(&quot;foo&quot; &quot;bar&quot; &quot;baz&quot;) (string-trim str ;去掉字符串前后的sep [sep #:left? left? #:right? right? #:repeat? repeat?]) → string? str : string? sep : (or/c string? regexp?) = #px&quot;\\s+&quot; left? : any/c = #t right? : any/c = #t repeat? : any/c = #f&lt;/pre&gt; &gt;(string-trim &quot; foo bar baz \r\n\t&quot;) &quot;foo bar baz&quot; (non-empty-string? x) → boolean? x : any/c 当x是字符串且不为空时，返回#t，否则返回#f&lt;/pre&gt; &gt;(non-empty-string? &quot;&quot;) #f (string-contains? s contained) → boolean? s : string? contained : string? 判断containde是否存在于s中&lt;/pre&gt; &gt;(string-contains? &quot;ab&quot; &quot;a&quot;) #t (string-prefix? s prefix) → boolean? s : string? prefix : string? 判断是否是开头 (string-suffix? s suffix) → boolean? s : string? suffix : string? 判断是否是结尾&lt;/pre&gt; &gt;(string-prefix? &quot;hello&quot; &quot;he&quot;) #t &gt;(string-suffix? &quot;hello&quot; &quot;lo&quot;) #t Ref： 1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket lambda和let]]></title>
    <url>%2F2017%2F11%2F09%2Fracket%20lambda%E5%92%8Clet%2F</url>
    <content type="text"><![CDATA[lambda (lambda kw-formals body ...+);kw-formals：参数，body：表达式 &gt; ((lambda (x y) (+ x y)) 1 2);参数为x,y,表达式(+ x y),实参1,2 &gt; 3 &gt; (define add (lambda (x y) (+ x y))) &gt; (add 1 2) 3 &gt; ((lambda (x [y 5]) (list y x)) 1 2);参数y有一个默认值5,若没有提供参数，则y输出5 &#39;(2 1) let location和top-level变量类似，location产生的时候使用一个从未使用过的名称，不能被重新生成和直接访问， 这里top-level变量应该指的是其它语言的全局变量。 (let ([id val-expr] ...) body ...+) (let proc-id ([id init-expr] ...) body ...+) 从左到执行表达式，为每个id创建一个location,并将值赋给这个location，然后执行body中的语句。 &gt; (let ([x 5] [y 5]) (+ x y)); 10 &gt; (let fac ([n 10]) (if (zero? n) 1 (* n (fac (sub1 n))))) 3628800 (let* ([id val-expr] ...) body ...+) 和let相像，一个一个执行val-expr,为每个id创建location,绑定ids到val-exprS和bodyS中。 &gt;(let* ([x 1] [y (+ x 1)]) (list y x)) &#39;(2 1) 如果此处将let*改为let,则会报错(x未定义,因为x为绑定到var-exprS中) (letrec ([id val-expr] ...) body ...+) 和let相似，从左到右执行val-exprS，但所有idS的location是先创建的，当执行val-expr后， idS马上被初始化，绑定到val-exprS和bodyS命名空间中。 (let-values ([(id ...) val-expr] ...) body ...+) 和let相似，val-expr中返回的变量必须和id中的数量一致，绑定到body中， &gt; (let-values ([(x y) (quotient/remainder 10 3)]) (list y x)) &#39;(1 3) (let*-values ([(id ...) val-expr] ...) body ...+);和let-values相似，但会把location绑定到val-exprS中。 (letrec-values ([(id ...) val-expr] ...) body ...+);和letrec相似，对id赋值的方式不一样 Ref： 1.官方文档lambda 2.官方文档let]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket自定义数据类型]]></title>
    <url>%2F2017%2F11%2F08%2Fracket%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[racket的自定义数据类型可以 看作C语言的结构体 &gt;(struct posn (x y)) ;定义个结构体，成员为x,y,默认是不透明结构体 &gt;(define p1 (posn 1 2));定义了p1变量 &gt; (define p2 (struct-copy posn p1 [x 3]));拷贝posn p1 将p1.x修改为3,并将这个变量赋值给p2 &gt; (list (posn-x p2) (posn-y p2));p2.x=3 &#39;(3 2) &gt; (list (posn-x p1) (posn-x p2));而p1.x=1,使用struct-copy并不会改变p1.x的值 &#39;(1 3) &gt;(struct posn (x y)) &gt;(struct 3d-posn posn (z)); 这两行相当于如下C语言： struct posn { int x; int y; } struct 3d-posn { struct posn; int z; } &gt;(define p (3d-posn 1 2 3)) &gt; (3d-posn-z p) 3 ;3d-posn并不包含x,y变量直接访问报错 &gt; (3-posn-x p) . . 3-posn-x: undefined; cannot reference an identifier before its definition 可以这样访问 &gt; (posn-x p) 1 透明结构体，会显示成员变量的值，结构体默认是不透明的 &gt; (struct posn (x y) #:transparent) &gt; (posn 1 2) (posn 1 2) 结构体的比较 &gt; (struct posn(x y)) &gt; (struct tr (x y) #:transparent) &gt; (equal? (tr 1 2) (tr 1 2)) #t &gt; (struct op (x y)) &gt; (equal? (op 1 2) (op 1 2));要比较不透明结构体，可通过#:methods,gen:equal+hash实现三种方法 #f 未完 Ref：1.官方文档]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Racket数据类型]]></title>
    <url>%2F2017%2F11%2F04%2FRacket%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1.1 Numbers包括小数和虚数 &gt; 1 1 &gt; 1/2 1/2 &gt; 1+2i 1+2i &gt; 3.14 3.14 1.2 Booleans,#t表示真，#f表示假 1.3 Byte,值在0-255的整数 &gt;(byte? 0) #t &gt; (byte? 256) #f 1.4 Characters,单个字符 &gt; (integer-&gt;char 65) #\A &gt; (display #\A) A 1.5 Strings,用双引号包围 &gt; &quot;Hello,World&quot; &quot;Hello,World&quot; 1.6 Symbols 就原子值，打印的时候以’开始 &gt; &#39;a &#39;a &gt; (symbol? &#39;a) #t 1.7 Keyword,和symbol类似，但打印的时候以#:开始 &gt; (string-&gt;keyword &quot;apple&quot;) &#39;#:apple 1.8 Pairs,可以连接两个任意值，cons函数用来构造Pairs &gt; (pair? (cons 1 2)) #t 1.9 Lists 列表 &gt; (list 1 2 3) &#39;(1 2 3) 2.0 Vector,是固定长度可以存放任意值的列表，支持常量 时间获取元素和更新元素 &gt; (vector-ref #(name (that tune)) 1) &#39;(that tune) 2.1 Hash Tables,字典 &gt;(define ht (make-hash)) &gt;(hash-set! ht &quot;apple&quot; &#39;(red round)) &gt;(hash-ref ht &quot;apple&quot;) &#39;(red round) 2.2 Boxes,是一个只有一个元素的vertor,打印以#&amp;开始 &gt; (define b (box &quot;apple&quot;)) &gt; b &#39;#&amp;&quot;apple&quot; &gt; (unbox b) &quot;apple&quot; &gt; (set-box! b &#39;(banana boat)) &gt; b &#39;#&amp;(banana boat)]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[supervisor can not find 'uwsgi' command]]></title>
    <url>%2F2017%2F11%2F04%2Fsupervisor-can-not-find-uwsgi-command%2F</url>
    <content type="text"><![CDATA[在一次关掉supervisor控制的进程，再进行开启的时候，报supervisor can not find ‘uwsgi’ command错误，而[program:uwsgi]是存在于配置文件中的。此时杀死[program:uwsgi]中command所执行的命令的进程，再将配置文件中的autorestart=True注释掉，reload后重新启动进程即可。 1.http://m.blog.csdn.net/lf8289/article/details/45478639 2.https://www.v2ex.com/t/326059]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql一些语句]]></title>
    <url>%2F2017%2F10%2F28%2Fmysql%E4%B8%80%E4%BA%9B%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[获取一张表的所有字段名select COLUMN_NAME from information_schema.columns where table_name=&#39;TableName&#39; 对一个数据库进行赋权grant all privileges on database.* to username@ip identified by &#39;passwd&#39; flush privileges]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 安装mysqlclient]]></title>
    <url>%2F2017%2F10%2F28%2FLinux%20%E5%AE%89%E8%A3%85mysqlclient%2F</url>
    <content type="text"><![CDATA[centos安装 mysqlclient失败，需先安装 yum install mysql-devel pip install mysqlclient ubuntu apt-get install libmysqlclient-dev python3-dev]]></content>
      <categories>
        <category>Python Web</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[lua表达式]]></title>
    <url>%2F2017%2F10%2F09%2Flua%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.关系运算符 &lt; &gt; &lt;= &gt;= == ~= 这些操作符返回结果为false或true.==和~=比较两个值，lua是强类型比较，如果两个值类型不同，Lua认为两者不同。 Lua通过引用比较tables、userdata、functions，只有当两者是同一对象时才相等。 &gt; a = {x=1,y=0} &gt; b = {x=1,y=0} &gt; c == a false &gt; c = a &gt; a == c true &gt; a == b false 数字、字符串的比较 &gt; 0 == &quot;0&quot; false &gt; 2 &lt; 15 true &gt; &quot;2&quot; &lt; &quot;15&quot; --按字母顺序进行比较 false 2.逻辑运算符 and or not 逻辑运算符认为false和nil是假，其他为真（true),0也是true。 a and b -- 如果a为false,则返回a,否则返回b a or b --如果a为true,则返回a,否则返回b &gt; print(1 and 2) 2 &gt; print(nil and 1) nil &gt; print(1 or 2) 1 &gt; print(false or 1) 1 例： x = x or v 等价于： if not x then x = v end C语言中的三元运算符 a ? b:c 在Lua中可以这样实现: (a and b) or c not 结果只返回false或者true &gt; print(not 0) false 3.连接运算符 .. --两个点 字符串连接，如果操作数为数字，Lua将数字转成字符串 &gt; print(&quot;Hello &quot; .. &quot;World&quot;) Hello World &gt; print(0 .. 1) 01]]></content>
      <categories>
        <category>LUA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[super(Entity,self).__init__()]]></title>
    <url>%2F2017%2F10%2F02%2Fsuperentityself-init%2F</url>
    <content type="text"><![CDATA[super(Entity,self).__init__() 这是对继承自父类的属性进行初始化。而且是用父类的初始化方法来初始化继承的属性。 也就是说，子类继承了父类的所有属性和方法，父类属性自然会用父类方法来进行初始化。当然，如果初始化的逻辑与父类的不同，不使用父类的方法，自己重新初始化也是可以的 和php中的parent::__construct()的作用应该是一样的。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python异常处理]]></title>
    <url>%2F2017%2F10%2F02%2Fpython%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[python提供了一个异常类，来处理代码中可能出现的异常class BaseException BaseException是所有内建异常都继承于这个类。但用户自定义的异常类用不继承这个类，而是继承自Exception。 class Exception Exception traceback 当异常发生时python会自动创建traceback对象，并关联到traceback属性中，你可以使用with_traceback()创建异常和设置你自己的traceback。(Python3) raise python2到python3 raise E, V 改为raise E(V),raise E, V, T 改为raise E(V).with_traceback(T) 兼容python2和python3的写法 peewee PY3: def reraise(tp, value, tb=None):#type,value,traceback if value.__traceback__ is not tb: raise value.with_traceback(tb) raise value PY2: exec(&#39;def reraise(tp, value, tb=None): raise tp, value, tb&#39;) #在python3下会报语法错误，通过exec绕过检查 bottle PY3: def _raise(*a): raise a[0](a[1]).with_traceback(a[2]) PY2: exec(compile(&#39;def _raise(*a): raise a[0], a[1], a[2]&#39;, &#39;&lt;py3fix&gt;&#39;, &#39;exec&#39;)) Ref： 1.https://mozillazg.github.io/2016/08/python-the-right-way-to-catch-exception-then-reraise-another-exception.html]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python魔法变量之__init__,__del__]]></title>
    <url>%2F2017%2F10%2F02%2Fpython%E9%AD%94%E6%9C%AF%E6%96%B9%E6%B3%95%E4%B9%8B__init__%2C__del__%2F</url>
    <content type="text"><![CDATA[init相当于构造函数，del相当于析构函数,而事实上new才是构造函数，new返回一个实例，而init则进行一些初始化工作。 In [3]: class A(object): ...: def __init__(self): ...: print(&quot;init&quot;) ...: def __del__(self): ...: print(&quot;del&quot;) ...: In [4]: a = A() init In [5]: del a del]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python编码]]></title>
    <url>%2F2017%2F10%2F02%2Fpython%E7%BC%96%E7%A0%81%2F</url>
    <content type="text"><![CDATA[python2中的默认编码为ASCII,在python3改为了unicode。 python2中,python3中已经没有(unicode,basestring)unicode_type = unicode string_type = basestring python3中 unicode_type = str string_type = bytes Ref： 1.http://www.jianshu.com/p/2bb8a1300bfd]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django内置的密码管理]]></title>
    <url>%2F2017%2F10%2F02%2FDjango%E5%86%85%E7%BD%AE%E7%9A%84%E5%AF%86%E7%A0%81%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[Django提供了内在的登录，登出以及密码管理。 在此记录密码管理功能的使用 1.在urls.py添加如下： url(&#39;^&#39;, include(&#39;django.contrib.auth.urls&#39;)) 相当于 ^login/$ [name=&#39;login&#39;] ^logout/$ [name=&#39;logout&#39;] ^password_change/$ [name=&#39;password_change&#39;] ^password_change/done/$ [name=&#39;password_change_done&#39;] ^password_reset/$ [name=&#39;password_reset&#39;] ^password_reset/done/$ [name=&#39;password_reset_done&#39;] ^reset/(?P&lt;uidb64&gt;[0-9A-Za-z_\-]+)/(?P&lt;token&gt;[0-9A-Za-z]{1,13}-[0-9A-Za-z]{1,20})/$ [name=&#39;password_reset_confirm&#39;] ^reset/done/$ [name=&#39;password_reset_complete&#39;] 2.配置好发送邮件配置 3.在前端配置好url就可实现密码修改，重置功能。 Ref： 1.官方文档]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django通过qq邮箱发送邮件]]></title>
    <url>%2F2017%2F09%2F27%2FDjango%E9%80%9A%E8%BF%87qq%E9%82%AE%E7%AE%B1%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[通过Django的send_mail发送邮件，send_mail(‘subject’, content, from_mail,[‘to_mail’]) 企业邮箱和普通邮箱所用的服务是不一样的，要区分开， 配置如下 EMAIL_USE_TLS = True EMAIL_HOST = &#39;smtp.qq.com&#39; #若是企业邮箱要改为：&#39;smtp.exmail.qq.com&#39; EMAIL_PORT = 587 EMAIL_HOST_USER = &#39;12345678@qq.com&#39; EMAIL_HOST_PASSWORD = &#39;xxxxxxxxxxxxxxxx&#39; #16位授权码(不是登录密码)，通过邮箱中的“设置”获取，可自行百度 DEFAULT_FROM_EMAIL = &#39;12345678@qq.com&#39;]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django权限与分组]]></title>
    <url>%2F2017%2F09%2F27%2FDjango%E6%9D%83%E9%99%90%E4%B8%8E%E5%88%86%E7%BB%84%2F</url>
    <content type="text"><![CDATA[from django.contrib.auth.models import Group, Permission from django.contrib.contenttypes.models import ContentType g = Group(name=&#39;group_test&#39;) #创建一个组 g.save() g = Group.objects.get(name=&#39;group_test&#39;) #获取存在的组 content_type = ContentType.objects.get_for_model(ViewPermission) # ViewPermission为models.py中的一个类 class ViewPermission(models.Model): class Meta: permissions = ( (&#39;ych_order&#39;, &#39;xxx&#39;), ) p = Permission.objects.create(codename=&#39;view_ych&#39;, name=&#39;Can see ych&#39;, content_type=content_type) #创建权限 p = Permission.objects.get(codename=&#39;view_ych&#39;) #获取已存在的权限 g.permissions.add(p) #为组g添加权限p user.groups.add(g) #将用户user添加到组g user.has_perm(&#39;report.view_ych&#39;) #判断权限]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[lua变量类型]]></title>
    <url>%2F2017%2F09%2F27%2Flua%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[lua是动态语言，变量不需要定义。lua中有8个基本类型：nil, bollean, number, string, userdata, thread和table. Lua 5.3.4 Copyright (C) 1994-2017 Lua.org, PUC-Rio &gt; print(type(&quot;hello, lua&quot;)) string &gt; print(type(10)) number &gt; print(type(print)) function &gt; print(type(true)) boolean &gt; print(type(nil)) nil 1.nil nil是lua中的特殊值，它只有一个值;变量未赋值前值为nil,可以通过给变量赋值为nil,删除变量 2.Booleans Booleans类型有两个值：false和true。Lua中所有的值都可以做为判断条件。在控制结构中的条件判断中除了false和nil为假，其它值都为真，也就是说Lua中0和空字符串也是真。 3.Numbers Numbers表示实数，Lua中没有整数。 4.Strings 字符串，Lua中字符串是不可修改的。Lua自动进行内存分配和释放，一个string可以只包含一个字母也可以包含一本书，Lua可以高效的处理长字符串，1M的string在Lua中是很常见的。可以使用单引号或者双引号表示字符串。 &gt; a = &quot;one string&quot; &gt; b = string.gsub(a, &quot;one&quot;, &quot;another&quot;) &gt; print(a) one string &gt; print(b) another string 可以使用[[…]]表示字符串，这种 形式的字符串可以包括多行，可以嵌套且不会解释转义序列。 Lua会自动在string和numbers 之间自动进行类型转换，当一个字符串使用算术操作符时，string就会被转成数字。 &gt; print(&quot;10&quot;+1) 11.0 &gt; print(&quot;10+1&quot;) 10+1 当Lua期望一个string而碰到数字时，会将数字转成string。如进行字符拼接时 &gt; print(10 .. 20) 1020 &gt; 10 == &quot;10&quot; false 5.Functions(函数) 函数是第一类值（和其他变量相同），意味着函数可以存储在变量中，可以作为函数的参数，也可以作为函数的返回值。]]></content>
      <categories>
        <category>LUA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python阶乘]]></title>
    <url>%2F2017%2F09%2F27%2Fpython%E9%98%B6%E4%B9%98%2F</url>
    <content type="text"><![CDATA[fact = lambda n:1 if n == 0 else n * fact(n-1) &gt;&gt;&gt; fact(6) 720 &gt;&gt;&gt; reduce(lambda x,y:x*y, range(1, 7)) 720]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy,pandas安装]]></title>
    <url>%2F2017%2F09%2F27%2Fnumpy%2Cpandas%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[sudo apt-get install python3-tk pip3 install numpy pip3 install pandas pip3 intall matplotlib pip3 install scipy]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[racket安装]]></title>
    <url>%2F2017%2F09%2F25%2Fracket%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[系统ubuntu16.04，64位 1.从官网根据系统下载安装包 2.安装，如下图 3.添加环境变量 在~/.bashrc添加 export PATH=&amp;quot;/usr/local/racket/bin:$PATH&amp;quot; source ~/.bashrc 4.racket,进入REPL环境]]></content>
      <categories>
        <category>Racket</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[apt-get选项]]></title>
    <url>%2F2017%2F09%2F25%2Fapt-get%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[通过man apt-get查看--选项 -d #只下载安装包 -f #尝试修正系统依赖损坏处 -m #忽略未找到的包 -q #输出到日志 - 无进展指示 -y #假定对所有的询问选是，不提示]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker常用命令]]></title>
    <url>%2F2017%2F09%2F25%2Fdocker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[-docker安装[通过脚本安装] curl -sSL https://get.daocloud.io/docker | sh -常用命令 sudo apt-get update sudo systemctl enable docker sudo systemctl start docker #启动docker sudo docker push | pull # 推|拉镜像 sudo docker images #列出本地镜像 sudo docker run -t -i ubuntu:16.04 /bin/bash #-t:让Docker分配一个伪终端，-i:让容器的的标准输入保持打开，-d:后台运行，启动后会返回一个唯一的id sudo docker tag #修改镜像的标签 sudo docker save #保存镜像到本地文件 sudo docker load #从导出的本地文件再加载本地镜像库[sudo docker load --input ubuntu.rar OR sudo docker load &lt; ubuntu.rar] sudo docker rmi #移除镜像 sudo docker rm #移除容器 在删除镜像之前要先用docker rm删掉依赖于这个镜像的所有容器。 docker run所涉及的操作 - 检查本地是否存在指定的镜像，不存在就从公有仓库下载 - 利用镜像创建并启动一个容器 - 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 - 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 - 从地址池配置一个ip地址给容器 - 执行用户指定的应用程序 - 执行完毕后容器被终止 sudo docker ps #查看容器信息，-a:所有容器 docker logs [container ID or NAMES]获取容器的输出信息 docker stop|start|restart [container ID or NAMES] 终止容器|启动容器|重启容器 sudo docker export #导出本地某个容器，导出容器快照到本地文件 sudo docker import #从容器快照文件中再导入为镜像，如cat ubuntu.tar | sudo docker import - test/ubuntu:v1.0 用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以 使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容 器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状 态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入 时可以重新指定标签等元数据信息。 sudo docker rm 用来删除一个处于终止状态的容器 用 docker ps -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量 太多要一个个删除可能会很麻烦，用 docker rm $(docker ps -a -q) 可以全 部清理掉。 *注意：这个命令其实会试图删除所有的包括还在运行中的容器，不过就像上面提过 的 docker rm 默认并不会删除运行中的容器。 #-m：指定提交说明信息，-a:指定更新的用户信息， 用于创建镜像的容器的ID,最后指定目标镜像的仓库名和tag信息 sudo docker commit -m &quot;add python3&quot; -a &quot;hys&quot; 425fffab3201 hys/test:v1.0 -进入通过参数-d启动的容器 docker attach | nsenter - sudo docker attach [container ID or NAMES] 但是使用 attach 命令有时候并不方便。当多个窗口同时 attach 到同一个容器的 时候，所有窗口都会同步显示。当某个窗口因命令阻塞时,其他窗口也无法执行操作 - nsenter安装 . sudo wget https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz . tar -xzvf util-linux-2.24.tar.gz . cd util-linux-2.24 . ./configure --without-ncurses . make . sudo cp nsenter /usr/local/bin sudo docker inspect --format &quot;{ {.State.Pid }}&quot; [container ID] sudo nsenter --target 40621 --mount --uts --ipc --net --pid 执行出错：nsenter: failed to execute /bin/zsh: No such file or directory sudo env SHELL=&quot;/bin/bash&quot; nsenter --target 40907 --mount --uts --ipc --net --pid ok -另一种方法，.bashrc_docker将里面的内容放到~/.bashrc文件中， .bashrc_docker # Some useful commands to use docker. # Author: yeasy@github # Created:2014-09-25 alias docker-pid=&quot;sudo docker inspect --format &#39;{ {.State.Pid}}&#39;&quot; alias docker-ip=&quot;sudo docker inspect --format &#39;{ { .NetworkSettings.IPAddress }}&#39;&quot; #the implementation refs from https://github.com/jpetazzo/nsenter/blob/master/docker-enter function docker-enter() { #if [ -e $(dirname &quot;$0&quot;)/nsenter ]; then #Change for centos bash running if [ -e $(dirname &#39;$0&#39;)/nsenter ]; then # with boot2docker, nsenter is not in the PATH but it is in the same folder NSENTER=$(dirname &quot;$0&quot;)/nsenter else # if nsenter has already been installed with path notified, here will be clarified NSENTER=$(which nsenter) #NSENTER=nsenter fi [ -z &quot;$NSENTER&quot; ] echo &quot;WARN Cannot find nsenter&quot; return if [ -z &quot;$1&quot; ]; then echo &quot;Usage: `basename &quot;$0&quot;` CONTAINER [COMMAND [ARG]...]&quot; echo &quot;&quot; echo &quot;Enters the Docker CONTAINER and executes the specified COMMAND.&quot; echo &quot;If COMMAND is not specified, runs an interactive shell in CONTAINER.&quot; else PID=$(sudo docker inspect --format &quot;{ {.State.Pid}}&quot; &quot;$1&quot;) if [ -z &quot;$PID&quot; ]; then echo &quot;WARN Cannot find the given container&quot; return fi shift OPTS=&quot;--target $PID --mount --uts --ipc --net --pid&quot; if [ -z &quot;$1&quot; ]; then # No command given. # Use su to clear all host environment variables except for TERM, # initialize the environment variables HOME, SHELL, USER, LOGNAME, PATH, # and start a login shell. #sudo $NSENTER &quot;$OPTS&quot; su - root sudo $NSENTER --target $PID --mount --uts --ipc --net --pid su - root else # Use env to clear all host environment variables. sudo $NSENTER --target $PID --mount --uts --ipc --net --pid env -i $@ fi fi } 通过docker-pid可以获取某个容器的PID；而docker_enter可以进入容器 或直接在容器内执行命令 echo $(docker-pid &lt;container&gt;) docker-enter &lt;container&gt; -Dockerfile FROM 指令告诉Docker使用哪个镜像作为基础 # 如果镜像不存在本地则从远程仓库进行拉取，存在本地则不会拉取 MAINTAINER 维护者的信息 RUN apt-get install ADD 复制本地文件到镜像 EXPOSE 用来向外部开放端口 docker build -t=&quot;hys/test:v2.0&quot; . #-t 添加tag,指定新的镜像用户信息。&#39;.&#39;表示Dockerfile所在的路径(当前目录),当然也可以指定具体路径。 sudo apt-get install -y xxx #直接安装 如：构建django开发环境的镜像 FROM ubuntu:16.04 MAINTAINER hys &lt;img.hysyeah.top&gt; RUN apt-get upadate RUN apt-get install -y python3-pip RUN pip3 install django Ref：Dockers-从入门到实践]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装五笔]]></title>
    <url>%2F2017%2F09%2F22%2Fubuntu%E5%AE%89%E8%A3%85%E4%BA%94%E7%AC%94%2F</url>
    <content type="text"><![CDATA[1.sudo apt-get install ibus-table-wubi 2.reboot 3.Text Entry添加输入法]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[lua -e]]></title>
    <url>%2F2017%2F09%2F22%2Flua-e%2F</url>
    <content type="text"><![CDATA[lua -e &quot;print(&quot;hello&quot;)&quot; --ouput nil lua -e &quot;print(&quot;1&quot;)&quot; -- output 1 为什么lua -e “print(“hello”)”输出来nil, 而不是hello,这与shell的解析机制有关。 lua -e “print(“hello”)”相当于lua -e “print(hello)”(“print” + hello + “)”)，而hello未定义所以输出nil。 可以通过单引号禁止shell解析，或者使用转义符， lua -e &#39;print(&quot;hello&quot;)&#39; -- output: hello lua -e &quot;print(\&quot;hello\&quot;)&quot; -- output: hello]]></content>
      <categories>
        <category>LUA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[lua求阶乘发生溢出]]></title>
    <url>%2F2017%2F09%2F21%2Flua%E6%B1%82%E9%98%B6%E4%B9%98%E5%8F%91%E7%94%9F%E6%BA%A2%E5%87%BA%2F</url>
    <content type="text"><![CDATA[function fact(n) if n == 0 then return 1 end return n * fact(n-1) end for i=1, 100, 1 do print(i,fact(i)) --发生溢出 end 通过使用lua的’bn’库可以解决这问题，从lbn下载lbn.tar.gz， tar -xzvf lbn.tar.gzcd bn 修改Makefile如下 make 在当前目录下会生成一个bn.so文件，将bn.so拷贝到代码文件所在目录，或者通过LUA_CPATH指定require搜索路径 local bn = require “bn” function bn_fact(bn.number(n)) if n:tonumber() == 0 then return 1 end return n * bn_fact(n-1)end function fact(n) return bn_fact(bn.number(n))end for i=1,100,1 do print(i,fact(i)) -- 未发生溢出 end Ref： 1.stackoverflow 2.http://webserver2.tecgraf.puc-rio.br/~lhf/ftp/lua/install.html]]></content>
      <categories>
        <category>LUA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python魔法变量之__getattribute__,__getattr__]]></title>
    <url>%2F2017%2F09%2F20%2Fpython%E9%AD%94%E6%9C%AF%E6%96%B9%E6%B3%95%E4%B9%8B__getattribute__%2C__getattr__%2F</url>
    <content type="text"><![CDATA[object.getattribute(self, name) 无条件调用，通过实例访问。如果类同时定义了getattr()，不会调用getattr()除非显式调用或产生了AttributeError。 object.getattr(self, name) 当未查找到访问的属性时，将会调用getattr()方法 In [49]: class T(object): ...: a = &#39;hello&#39; ...: def __getattribute__(self, *args, **kwargs): ...: print(&quot;called __getattribute__&quot;) ...: return object.__getattribute__(self, *args, **kwargs) ...: def __getattr__(self, name): ...: print(&quot;called __getattr__&quot;) ...: return name In [50]: t = T() In [51]: t.a called __getattribute__ Out[51]: &#39;hello&#39; In [52]: t.b called __getattribute__ called __getattr__ Out[52]: &#39;b&#39; Ref： 1.官方文档]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python魔法变量之__class__,__bases__,__mro__]]></title>
    <url>%2F2017%2F09%2F20%2Fpython%E9%AD%94%E6%B3%95%E5%8F%98%E9%87%8F%E4%B9%8B__class__%2C__bases__%2C__mro__%2F</url>
    <content type="text"><![CDATA[bases:一个类的基类 In [4]: class B: ...: pass ...: In [5]: class A(B): ...: pass ...: In [6]: A.__bases__ Out[6]: (&lt;class __main__.B at 0x7fdafd3f89a8&gt;,) class:实例所对应的类（仅新式类中） In [12]: class T(object): #新式类 ...: pass ...: In [13]: t = T() In [14]: type(t) Out[14]: __main__.T In [15]: t.__class__ Out[15]: __main__.T In [21]: type(t) is t.__class__ Out[21]: True In [29]: T.__class__ Out[29]: type In [30]: T.__bases__ Out[30]: (object,) In [31]: t.__bases__ --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-28-92db851817a5&gt; in &lt;module&gt;() ----&gt; 1 t.__bases__ AttributeError: &#39;T&#39; object has no attribute &#39;__bases__&#39; In [16]: class T1: # 旧式类,type(t)为instance,而新式类为class ...: pass ...: In [17]: t = T1() In [18]: type(t) Out[18]: instance In [24]: type(t) is t.__class__ Out[24]: False In [44]: class Base(object): ...: pass ...: In [45]: class A(Base): ...: pass ...: In [46]: A.__class__ Out[46]: type In [47]: A.__mro__ #返回值一个元素为class元组，相当于js的原型链，在方法解析时会用到(作为查找路径) Out[47]: (__main__.A, __main__.Base, object) In [48]: Base.__subclasses__() #子类 Out[48]: [__main__.A]]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql-UNION与UNION ALL]]></title>
    <url>%2F2017%2F09%2F19%2Fmysql-union-e4-b8-8eunion-all%2F</url>
    <content type="text"><![CDATA[SELECT ... UNION [ALL | DISTINCT] SELECT ... [UNION [ALL | DISTINCT] SELECT ...] UNION用于将多个SELECT语句的结果整合并放在一个表中,并消除重复的行，UNION ALL并不会消除重复的行 mysql&gt;&gt; select 1 union select 2 union select 2; +---+ | 1 | +---+ | 1 | | 2 | +---+ 2 rows in set (0.01 sec) mysql&gt;&gt; select 1 union select 2 union all select 2; +---+ | 1 | +---+ | 1 | | 2 | | 2 | +---+ 3 rows in set (0.00 sec) 第一个SELECT语句中的列名将会作为整个结果集的列名。SELECT中对应的列应具有相同的数据类型。 在UNION中使用SELECT有以下的限制 - 只有最后一个SELECT语句才可以使用INTO OUTFILE - 不能使用HIGH_PRIORITY 除非确定需要服务器消除重复的行，否则就一定要使用UNION ALL，如果没有ALL关键字，MYSQL会给临时表加上 DISTINCT选项，这会导致对整个临时表的数据做唯一性检查。即使有ALL关键字，MYSQL仍然会使用临时表存储 结果。MYSQL总是将结果入入临时表，然后再读出，再返回客户端。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[wordpress屏蔽更新提示]]></title>
    <url>%2F2017%2F09%2F15%2Fwordpress%E5%B1%8F%E8%94%BD%E6%9B%B4%E6%96%B0%E6%8F%90%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[wordpress/wp-includes/update.php 在最后面添加 add_filter(&#39;pre_site_transient_update_core&#39;, create_function(&#39;$a&#39;, &quot;return null;&quot;));//关闭核心提示 add_filter(&#39;pre_site_transient_update_plugins&#39;, create_function(&#39;$a&#39;, &quot;return null;&quot;));//关闭插件提示 add_filter(&#39;pre_site_transient_update_themes&#39;, create_function(&#39;$a&#39;, &quot;return null;&quot;));//关闭主题提示 并注释掉 //add_action( &#39;admin_init&#39;, &#39;_maybe_update_plugins&#39; ); //add_action( &#39;admin_init&#39;, &#39;_maybe_update_themes&#39; ); 重启php-fpm即可。]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django为已有的数据库生成model]]></title>
    <url>%2F2017%2F09%2F15%2FDjango%E4%B8%BA%E5%B7%B2%E6%9C%89%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E7%94%9F%E6%88%90model%2F</url>
    <content type="text"><![CDATA[python manage.py inspectdb # 将settings.py中DATABASES中指定的数据库的所表的model导入到models.py中 python manage.py inspectdb &gt; models.py # 也可以指定数据表 python manage.py inspectdb xxx xxx &gt;&gt; models.py]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pip使用豆瓣源]]></title>
    <url>%2F2017%2F09%2F15%2Fpip%E4%BD%BF%E7%94%A8%E8%B1%86%E7%93%A3%E6%BA%90%2F</url>
    <content type="text"><![CDATA[将pip源修改为豆瓣源 vim ~/.pip/pip.conf 没有则新建 添加如下内容: [global] index-url = http://pypi.douban.com/simple --trusted-host = pypi.douban.com 2.临时更换 sudo pip install pudb -i http://pypi.douban.com/simple --trusted-host=pypi.douban.com]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql的limit,offset分页]]></title>
    <url>%2F2017%2F09%2F15%2Fmysql%E7%9A%84limit%2Coffset%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[1.SELECT * FROM br_apply_check_info LIMIT 20 OFFSET 612000; --0.769s 2.SELECT * FROM br_apply_check_info INNER JOIN (SELECT id from br_apply_check_info LIMIT 20 OFFSET 612000) AS x USING(ID);-- 0.438s 当偏移量的增加，MYSQL需要花费大量的时间来扫描需要丢弃的数据。反范式化，预先计算和缓存可能是解决这类查询的仅有策略。 优化这类索引的另一个比较好的策略是使用延迟关联，通过使用覆盖索引查询返回需要的主键，再根据这些主键关联原表获得需要的行。这样可以减少MYSQL扫描那些需要丢弃的行数。 如上：第二方法比第一种方法快一倍(当偏移量足够大的时候) Ref：&lt;&lt;高性能MYSQL&gt;&gt;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[定义一个可以通过属性访问的字典]]></title>
    <url>%2F2017%2F09%2F15%2F%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%E5%B1%9E%E6%80%A7%E8%AE%BF%E9%97%AE%E7%9A%84%E5%AD%97%E5%85%B8%2F</url>
    <content type="text"><![CDATA[In [6]: class attrdict(dict): ...: def __getattr__(self, attr): ...: try: ...: return self[attr] ...: except KeyError: ...: raise AttributeError(attr) ...: In [7]: op = attrdict(AND=&#39;and&#39;,OR=&#39;or&#39;) In [8]: op Out[8]: {&#39;AND&#39;: &#39;and&#39;, &#39;OR&#39;: &#39;or&#39;} In [9]: op.AND Out[9]: &#39;and&#39; In [10]: op.OR Out[10]: &#39;or&#39; Ref:1.来自peewee源码]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python各时间段获取和时间类型的转换]]></title>
    <url>%2F2017%2F09%2F15%2Fpython%E5%90%84%E6%97%B6%E9%97%B4%E6%AE%B5%E8%8E%B7%E5%8F%96%E5%92%8C%E6%97%B6%E9%97%B4%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[python获取各时段起始时间和各种类型时间的相互转换In [1]: import datetime In [2]: from datetime import timedelta 1.获取今天时间 In [3]: now = datetime.datetime.now() In [4]: now Out[4]: datetime.datetime(2017, 9, 14, 9, 38, 44, 878801) 2.获取明天，昨天的时间 In [5]: tomorrow = now + timedelta(days=1) In [6]: tomorrow Out[6]: datetime.datetime(2017, 9, 15, 9, 38, 44, 878801) In [7]: yesterday = now - timedelta(days=1) In [8]: yesterday Out[8]: datetime.datetime(2017, 9, 13, 9, 38, 44, 878801) 3.获取本月时间 In [9]: this_month_start = datetime.datetime(now.year, now.month, 1) In [10]: this_month_start Out[10]: datetime.datetime(2017, 9, 1, 0, 0) In [11]: this_month_end = datetime.datetime(now.year, now.month+1, 1) - timedelta(days=1) In [12]: this_month_end Out[12]: datetime.datetime(2017, 9, 30, 0, 0) 4.获取上月时间 In [13]: last_month_end = this_month_start - timedelta(days=1) In [14]: last_month_start = datetime.datetime(last_month_end.year, last_month_end.month, 1) In [15]: last_month_start Out[15]: datetime.datetime(2017, 8, 1, 0, 0) In [16]: last_month_end Out[16]: datetime.datetime(2017, 8, 31, 0, 0) 5.获取近30天时间 In [17]: last_thirty_days = now - timedelta(days=30) In [18]: last_thirty_days Out[18]: datetime.datetime(2017, 8, 15, 9, 38, 44, 878801) 6.获取本季起始时间(可通过本季起始时间获取上季开始时间，从而获取上季起始时间) In [19]: m = (now.month -1) - (now.month - 1)%3 +1 In [20]: m Out[20]: 7 In [21]: this_quarter_satart = datetime.datetime(now.year, m, 1) In [23]: this_quarter_end = datetime.datetime(now.year, m+3, 1) - timedelta(days=1) In [24]: this_month_start Out[24]: datetime.datetime(2017, 9, 1, 0, 0) In [25]: this_month_end Out[25]: datetime.datetime(2017, 9, 30, 0, 0) In [26]: this_quarter_satart Out[26]: datetime.datetime(2017, 7, 1, 0, 0) In [27]: this_quarter_end Out[27]: datetime.datetime(2017, 9, 30, 0, 0) 7.本周起始时间 In [29]: this_week_start = now - timedelta(days=now.weekday()) In [30]: this_week_end = now + timedelta(days=6-now.weekday()) In [31]: this_week_start Out[31]: datetime.datetime(2017, 9, 11, 9, 38, 44, 878801) In [32]: this_week_end Out[32]: datetime.datetime(2017, 9, 17, 9, 38, 44, 878801) 8.上周起始时间 In [33]: last_week_start = now - timedelta(days=now.weekday()+7) In [34]: last_week_end = now - timedelta(days=now.weekday()+1) In [35]: last_week_start Out[35]: datetime.datetime(2017, 9, 4, 9, 38, 44, 878801) In [36]: last_week_end Out[36]: datetime.datetime(2017, 9, 10, 9, 38, 44, 878801) 9.字符串时间和datetime的相互转换 In [37]: str_time = now.strftime(&quot;%Y-%m-%d %H:%m:%s&quot;) In [38]: str_time Out[38]: &#39;2017-09-14 09:09:1505396324&#39; In [41]: dt = datetime.datetime.strptime(&#39;2017-09-14&#39;,&quot;%Y-%m-%d&quot;) In [42]: dt Out[42]: datetime.datetime(2017, 9, 14, 0, 0) 10.date转换为时间戳 In [16]: dt = datetime.date(2017, 8, 8) In [17]: dt Out[17]: datetime.date(2017, 8, 8) In [18]: timestamp = time.mktime(dt.timetuple()) In [19]: timestamp Out[19]: 1502121600.0 11. 时间戳到--字符串时间 datetime.datetime.fromtimestamp(psutil.boot_time()).strftime(&quot;%Y-%m-%d %H:%M:%S&quot;) 字符串到时间戳 s = time.mktime(time.strptime(&#39;2016-06-06 10:00:00&#39;,&#39;%Y-%m-%d %H:%M:%S&#39;)) Ref：1.http://www.cnblogs.com/prolifes/articles/5195528.html]]></content>
      <categories>
        <category>python</category>
        <category>python标准库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python魔法变量之__slots__]]></title>
    <url>%2F2017%2F09%2F08%2Fpython%E9%AD%94%E6%B3%95%E5%8F%98%E9%87%8F%E4%B9%8B__slots__%2F</url>
    <content type="text"><![CDATA[slots变量，用来限制类能添加的属性。由于’score’没有被放到slots中，所以不能绑定score属性，所以报错。 使用slots要注意，slots定义的属性仅对当前类起作用，对继承的子类是不起作用的： 除非在子类中也定义slots这样，子类允许定义的属性就是自身的slots加上父类的slots. In [1]: class Fruit(object): ...: __slots__ = (&#39;name&#39;, &#39;age&#39;) ...: In [2]: f = Fruit() In [3]: f.name = &#39;hys&#39; In [4]: f.age = 18 In [5]: f.score = 33 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-5-f65d994e4176&gt; in &lt;module&gt;() ----&gt; 1 f.score = 33 AttributeError: &#39;Fruit&#39; object has no attribute &#39;score&#39; In [6]: class Apple(Fruit): ...: pass ...: In [7]: a = Apple() In [8]: a.name = &#39;hys&#39; In [9]: a.score = 44 In [10]: class Bnana(Fruit): ...: __slots__ = () ...: In [11]: b = Bnana() In [12]: b.name = &#39;hys&#39; In [13]: b.age = 33 In [14]: b.socre = 6 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-14-140c427ae185&gt; in &lt;module&gt;() ----&gt; 1 b.socre = 6 AttributeError: &#39;Bnana&#39; object has no attribute &#39;socre&#39; 当定义了slots属性，Python会针对实例采用一种更加紧凑的内部表示。不再让每个实例都创建一个dict字典，现在的实例是围绕着一个固定长度的小型数组来构建的，这和一个元组或者列表很相似。在slots中列出的属性名会在内部映射到这个数组的索引下。使用slots节省下来的内存根据创建实例数量以及保存的属性类型而有所不同。 优点：节省内存 缺点：Python中有许多部分都依赖于传统的基于字典的实现。定义了slots属性的类不支持某些特定的功能，比如多重继承。就大部分情况而言，我们应该只针对那些在程序中被当做数据结构而频繁使用的类上采用slots。 Ref： 1.博客 2.python cookbook]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[django添加自定义过滤器]]></title>
    <url>%2F2017%2F09%2F08%2Fdjango%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1.在app目录下与models.py同级目录下新建一个templatetags文件夹， 2.utils.py from django import template register = template.Library() @register.filter def val_type(val): return type(val).__name__ 3.此函数是返回变量的类型名，在模版文件中通过 {% load utils %} 导入 { { val|val_type }} 返回变量的类型]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一行把tuple转换为list]]></title>
    <url>%2F2017%2F09%2F06%2F%E4%B8%80%E8%A1%8C%E6%8A%8Atuple%E8%BD%AC%E6%8D%A2%E4%B8%BAlist%2F</url>
    <content type="text"><![CDATA[In [16]: tup = ((1, 2, 3), (4, 5, 6)) In [17]: list(map(list,tup)) Out[17]: [[1, 2, 3], [4, 5, 6]]]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[namedtuple映射名称到序列元素]]></title>
    <url>%2F2017%2F09%2F06%2Fnamedtuple%E6%98%A0%E5%B0%84%E5%90%8D%E7%A7%B0%E5%88%B0%E5%BA%8F%E5%88%97%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[In [1]: from collections import namedtuple In [2]: dt = namedtuple(&#39;s&#39;, [&#39;name&#39;, &#39;job&#39;]) In [3]: ret = dt(&#39;hys&#39;, &#39;coder&#39;) In [4]: ret.name Out[4]: &#39;hys&#39; In [5]: ret.job Out[5]: &#39;coder&#39; 将cursor.fetchall()返回的元组转换成可以通过名称来访问的元组 In [1]: from collections import namedtuple ...: ...: def tuple_to_namedtuple(t, name_list): ...: ret_list = [] ...: s_namedtuple = namedtuple(&#39;s_namedtuple&#39;,name_list) ...: for item in t: ...: ret_list.append(s_namedtuple._make(item)) ...: return ret_list ...: In [2]: import sqlite3 In [3]: conn = sqlite3.connect(&#39;tst.sqlite3&#39;) In [4]: cursor = conn.cursor() In [5]: cursor.execute(&#39;select * from t&#39;) Out[5]: &lt;sqlite3.Cursor at 0x7f48b8079500&gt; In [6]: r = tuple_to_namedtuple(cursor.fetchall(),[&#39;id&#39;, &#39;name&#39;, &#39;value&#39;]) In [7]: r Out[7]: [s_namedtuple(id=1, name=&#39;hys&#39;, value=&#39;a&#39;), s_namedtuple(id=2, name=&#39;huang&#39;, value=&#39;b&#39;)] 通过map实现,简洁许多 In [10]: def tuple_to_dict(seq, name_list): ...: s_namedtuple = namedtuple(&#39;s_namedtuple&#39;,name_list) ...: return map(s_namedtuple._make, seq) ...: In [12]: ret = tuple_to_dict(cursor.fetchall(),[&#39;id&#39;,&#39;name&#39;,&#39;value&#39;]) In [13]: ret Out[13]: &lt;map at 0x7f48b2fa2a90&gt; In [14]: for elt in ret: ...: print(elt) ...: s_namedtuple(id=1, name=&#39;hys&#39;, value=&#39;a&#39;) s_namedtuple(id=2, name=&#39;huang&#39;, value=&#39;b&#39;)]]></content>
      <categories>
        <category>python标准库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[flatten]]></title>
    <url>%2F2017%2F09%2F05%2Fflatten%2F</url>
    <content type="text"><![CDATA[In [1]: from compiler.ast import flatten In [2]: flatten([1, [2], [3, [[4,]]]]) Out[2]: [1, 2, 3, 4] 源代码： def flatten(seq): l = [] for elt in seq: t = type(elt) if t is tuple or t is list: for elt2 in flatten(elt): l.append(elt2) else: l.append(elt) return l 上面方法在python3已经被移除 生成器方法实现： In [36]: def f(seq): for elt in seq: if isinstance(elt, Iterable): for elt2 in f(elt): yield elt2 else: yield elt In [37]: In [37]: a = f([1,2,[3,4]]) In [38]: l = [x for x in a] In [39]: l Out[39]: [1, 2, 3, 4] In [40]: a Out[40]: &lt;generator object f at 0x7fead3fca0f8&gt; 扩展求任意嵌套字典{&#39;a&#39;: 1,&#39;b&#39;:{&#39;c&#39;:1,&#39;d&#39;:{&#39;e&#39;:2,&#39;d&#39;:3}}}所有值的和，如1+1+2+3的和为7 def f(d): for elt in d.values(): if isinstance(elt, collections.Iterable): for elt2 in f(elt): yield elt2 else: yield elt a = f(d) l = sum(x for x in a) print(l) # 7]]></content>
      <categories>
        <category>python标准库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy数据类型基本操作]]></title>
    <url>%2F2017%2F09%2F05%2Fnumpy%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.导入numpy包，调用array()函数创建一维数组 In [3]: import numpy as np In [4]: a = np.array(range(6)) In [5]: a Out[5]: array([0, 1, 2, 3, 4, 5]) In [6]: print(a) [0 1 2 3 4 5] 2.array的shape属性 In [7]: a.shape Out[7]: (6,) In [8]: a.shape = 2, 3 In [9]: a Out[9]: array([[0, 1, 2], [3, 4, 5]]) 3.通过reshape()函数在a的基础上创建一个新的二维结构的数组a1,a的shape属性值不会变 In [17]: a1 = a.reshape(3,2) In [18]: print(a1) [[0 1] [2 3] [4 5]] In [19]: a1.shape Out[19]: (3, 2) In [20]: a.shape Out[20]: (2, 3) 数组a和a1共用内存中的数据存储值，若更改其中任意一个数组中的元素值，则另一个相对应的元素也会改变。 In [23]: print(a1) [[0 1] [2 3] [4 5]] In [24]: a[1, 2] = 88 In [25]: print(a1) [[ 0 1] [ 2 3] [ 4 88]] 4.利用arange([start,] stop[, step,], dtype=None)函数生成数组 In [2]: a = np.arange(13, 1, -1) In [3]: print(a) [13 12 11 10 9 8 7 6 5 4 3 2] 5.利用linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)函数生成数组 In [9]: a = np.linspace(1, 12, 12) In [10]: print(a) [ 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12.] In [11]: a.dtype Out[11]: dtype(&#39;float64&#39;) 6.常用函数 zeros(shape, dtype=float, order=&#39;C&#39;)生成元素全部为0的数组 ones(shape, dtype=None, order=&#39;C&#39;)生成元素全部为1的数组 empty(shape, dtype=float, order=&#39;C&#39;)生成给定维度无初始值的数组]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zeppelin创建mysql解析器,连接mysql]]></title>
    <url>%2F2017%2F09%2F04%2Fzeppelin%E5%88%9B%E5%BB%BAmysql%E8%A7%A3%E6%9E%90%E5%99%A8%2C%E8%BF%9E%E6%8E%A5mysql%2F</url>
    <content type="text"><![CDATA[zeppelin环境的配置 1.创建mysql解析器 2.根据上图填写配置 3.创建Note 4.编写sql语句，执行后生成下图 Ref： 1.https://zeppelin.apache.org/docs/0.7.0/interpreter/jdbc.html]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[修改.zshrc文件报badassiment错误]]></title>
    <url>%2F2017%2F09%2F04%2F%E4%BF%AE%E6%94%B9.zshrc%E6%96%87%E4%BB%B6%E6%8A%A5badassiment%E9%94%99%E8%AF%AF%2F</url>
    <content type="text"><![CDATA[修改~/.zshrc文件添加下如下 export PATH=&quot;/usr/local/bin:$PATH&quot; source ~/.zshrc报badassiment错误 原因：=号之间不能有空格]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[wordpress隐藏登录入口]]></title>
    <url>%2F2017%2F09%2F04%2Fordpress%E9%9A%90%E8%97%8F%E7%99%BB%E5%BD%95%E5%85%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[在wordpress根目录下打开wp-login.php文件，在 require( dirname(__FILE__) . &#39;/wp-load.php&#39; ); 后面加上下面 if($_GET[&#39;hehe&#39;] != &#39;haha&#39;){ header(&#39;Location: http://www.taobao.com/&#39;); } 当别人访问img.hysyeah.top/wp-login.php时会被重定向到http://www.taobao.com/,只有img.hysyeah.top/wp-login.php?hehe=haha才能正常打开后台登录界面。 Ref1.http://www.luoxiao123.cn/5434.html]]></content>
      <categories>
        <category>未分类</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark,zeppelin环境的配置]]></title>
    <url>%2F2017%2F09%2F04%2Fspark%2Czeppelin%E7%8E%AF%E5%A2%83%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[系统环境为Ubuntu16.04 1.安装java,从官网下载java包，放下/opt目录下，解压，往~/.zshrc文件添加环境变量，如下 export JAVA_HOME=/opt/jdk1.8.0_144 export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=&quot;/opt/jdk1.8.0_144/bin:$PATH&quot; 2.scala安装，下载安装包，解压到/opt/目录下，往~/.zshrc文件添加环境变量，如下 export SCALA_HOME=/opt/scala-2.11.11 export PATH=&quot;/opt/scala-2.11.11/bin:$PATH&quot; 3.下载spark(http://spark.apache.org/downloads.html),解压到/opt/，往~/.zshrc文件添加环境变量，如下 export SPARK_HOME=/opt/spark-2.1.0-bin-hadoop2.7 export PATH=&quot;/opt/spark-2.1.0-bin-hadoop2.7/bin:$PATH&quot; export PYSPARK_PYTHON=python3 //表示使用python3 export PYSPARK_DRIVER_PYTHON=ipython3 4.下载zeppelin,解压到/opt/，往~/.zshrc文件添加环境变量，如下 export PATH=&quot;/opt/zeppelin-0.7.2-bin-all/bin:$PATH&quot; 5.运行zeppelin, sudo chown -R hys:hys zeppelin-0.7.2-bin-all zeppelin-daemon.sh start 6.访问http://127.0.0.1:8080出现如下界面则表示安装成功。]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Django开发环境搭建与项目结构]]></title>
    <url>%2F2017%2F09%2F01%2FDjango%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1.安装python3 2.安装virtualenv sudo apt-get install python3-virtualenv(ubuntu环境下) 或者 pip3 install virtualenv 3.建立虚拟环境 virtualenv venv 4.激活虚拟环境 source venv/bin/activate(Linux环境） venv\Scripts\activate(Windows） 根据自己文件路径，需适当修改 5.在虚拟环境下通过pip3安装所需软件，如为旧项目，根据项目中的requests.txt文件安装所需依赖。 pip3 install -r requests.txt 6.建立项目，APP(如新建好，则跳过此步骤) django-admin startproject xxxx django-admin startapp xxxx 7.项目结构 - report/static 一些静态文件，如js,css文件 - report/templates 项目模版 - report/urls.py 路由配置文件 - utils/sql.py 项目中用到的SQL查询语句 - report/views/views.py 视图函数 - report/views/xls_export.py xsl文件导出相关 - xfbank/config 一些项目配置文件,dev_settings.py(测试环境),pro_settings.py(正式环境) - xfbank/urls.py 项目主配置urls文件&lt;/pre&gt; 8.运行Django项目 python manage.py runserver]]></content>
      <categories>
        <category>Django</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python迭代器与生成器]]></title>
    <url>%2F2017%2F08%2F30%2Fpython%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Iterable—可迭代 可迭代的序列类型包括，list,str,tuple,非序列类型包括dict,file,任何类只是你定义了iter方法就会被看作为可迭代对象。 In [3]: from collections import Iterable, Iterator In [4]: class A: ...: def __init__(self): ...: self.n = 0 ...: def __iter__(self): ...: return self ...: In [5]: a = A() In [6]: isinstance(a, Iterable) Out[6]: True Iterator—迭代器 Iterator必须实现iter()和next()(python3)或next()(python2)两个方法，组成了迭代器协议。 In [13]: class yrange: ...: def __init__(self, n): ...: self.i = 0 ...: self.n = n ...: ...: def __iter__(self): ...: return self ...: ...: def __next__(self): ...: if self.i &lt; self.n: ...: i = self.i ...: self.i += 1 ...: return i ...: else: ...: raise StopIteration() ...: In [14]: it = yrange(3) In [15]: it.__next__() Out[15]: 0 In [16]: it.__next__() Out[16]: 1 In [17]: it.__next__() Out[17]: 2 In [18]: it.__next__() --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) &lt;ipython-input-18-a1cc91b7bc73&gt; in &lt;module&gt;() ----&gt; 1 it.__next__() &lt;ipython-input-13-55955e9c2cc7&gt; in __next__(self) 13 return i 14 else: ---&gt; 15 raise StopIteration() 16 StopIteration: 反向迭代一个序列，使用内置的reversed()函数，可以通过在自定义类上实现reversed () 方法来实现反向 迭代。 In [19]: class yrange: ...: def __init__(self, n): ...: self.i = 0 ...: self.n = n ...: ...: def __iter__(self): ...: return self ...: ...: def __next__(self): ...: if self.i &lt; self.n: ...: i = self.i ...: self.i += 1 ...: return i ...: else: ...: raise StopIteration() ...: def __reversed__(self): ...: index = self.n - 1 ...: while index &gt;= 0: ...: yield index ...: index -= 1 ...: In [20]: it = reversed(yrange(3)) In [21]: it.__next__() Out[21]: 2 In [22]: it.__next__() Out[22]: 1 In [23]: it.__next__() Out[23]: 0 generator—生成器 生成器是一个返回迭代器对象的函数。它和普通函数看起来一样，除了它包含了yield语句，产生可以通过for循环访问的值，或者可以通过调用next方法一次访问一个值。 生成的iter()和next()方法是自动创建的。。每次next()调用时，生成器再恢复它离开的位置（它记忆语句最 后一次执行的位置和所有的数据值） In [25]: def integer(): ...: i = 1 ...: while True: ...: yield i ...: i += 1 ...: In [26]: i = integer() In [27]: i Out[27]: &lt;generator object integer at 0x7fb940696fc0&gt; In [29]: i.__next__() Out[29]: 1 生成器—send方法,调用send()方法前，必须先调用next()方法（pep342) In [37]: def double_input(): ...: while True: ...: x = yield ...: yield x * 2 In [59]: gen = double_input() In [60]: gen.__next__() In [61]: gen.send(4) Out[61]: 8 In [62]: gen.send(4) In [63]: gen.send(4) Out[63]: 8 In [64]: gen.__next__() In [65]: gen.send(5) Out[65]: 10 Ref： 1.http://anandology.com/python-practice-book/iterators.html 2.官方文档]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pandas数据类型Series及基本操作]]></title>
    <url>%2F2017%2F08%2F30%2Fpandas%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8BSeries%E5%8F%8A%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Series类型的数据由一列数据及与之对应的标签（索引，位于数据的左侧）两部分组成。Series对象本质上 是一个NumPy数组，因此NumPy的数组处理函数同样适用于Series对象。每个Series对象都具有index和values 两大属性。 - index:保存标签信息 - values:保存值 1.创建Series对象 In [1]: import pandas as pd In [2]: import numpy as np In [3]: s1 = pd.Series() In [4]: s1 Out[4]: Series([], dtype: float64) 2.Series对象的访问 In [8]: s2 = pd.Series([1, 3, 5, 7, 9], index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;]) In [9]: s2 Out[9]: a 1 b 3 c 5 d 7 e 9 dtype: int64 In [10]: s2.values Out[10]: array([1, 3, 5, 7, 9]) In [11]: s2.index Out[11]: Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;) In [12]: s2[&#39;e&#39;] Out[12]: 9 3.添加新元素 In [13]: s2[&#39;f&#39;] = 66 In [14]: s2 Out[14]: a 1 b 3 c 5 d 7 e 9 f 66 dtype: int64 4.将字典对象转换为Series对象 In [15]: pd.Series({&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}) Out[15]: a 1 b 2 c 3 dtype: int64 5.Series对象的元素提取与切片 obj.head(n=5) #默认查看对象的前五个数据 obj.tail(n=5) #默认查看对象最后五个数据 obj.take() #通过传入索引值列表来提取元素 In [18]: s2.head() Out[18]: a 1 b 3 c 5 d 7 e 9 dtype: int64 In [19]: s2.tail() Out[19]: c 5 d 7 e 9 f 66 g 11 dtype: int64 In [20]: s2.take([2, 4, 0]) #指定索引值 Out[20]: c 5 e 9 a 1 dtype: int64 6.切片 In [22]: s2[0:4] #位置切片，不包括结束位置 Out[22]: a 1 b 3 c 5 d 7 dtype: int64 In [23]: s2[&#39;a&#39;:&#39;d&#39;] #标签切片，包括结束位置 Out[23]: a 1 b 3 c 5 d 7 dtype: int64 7.时间序列 Timestamp对象由Pandas包中的Timestamp()来创建，参数可以为str类型，也可以为datetime类型。 In [28]: date Out[28]: datetime.datetime(2017, 9, 16, 0, 0) In [29]: date = pd.Timestamp(date) In [30]: date Out[30]: Timestamp(&#39;2017-09-16 00:00:00&#39;) 通过to_datetime()将Series的index属性转换为DatetimeIndex(实际上对于datetime对象，我们可以直接 将其作为index，Pandas会自动将其转换成Timestamp对象 In [31]: dates = [&#39;2017-01-01&#39;, &#39;2017-01-02&#39;, &#39;2017-01-03&#39;] In [32]: ts = pd.Series([1, 2, 3], index=pd.to_datetime(dates)) In [33]: ts Out[33]: 2017-01-01 1 2017-01-02 2 2017-01-03 3 dtype: int64 In [34]: ts.index Out[34]: DatetimeIndex([&#39;2017-01-01&#39;, &#39;2017-01-02&#39;, &#39;2017-01-03&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) 8.截取时间段数据 In [35]: ts[&#39;20170101&#39;] Out[35]: 1 In [36]: ts[&#39;2017-01-01&#39;] Out[36]: 1 In [37]: ts[&#39;01/01/2017&#39;] Out[37]: 1 In [38]: ts[&#39;2017&#39;] Out[38]: 2017-01-01 1 2017-01-02 2 2017-01-03 3 dtype: int64 In [39]: ts[&#39;2017-01&#39;: &#39;2017-02&#39;] Out[39]: 2017-01-01 1 2017-01-02 2 2017-01-03 3 dtype: int64 In [41]: ts.truncate(after=&#39;2017-01-02&#39;) Out[41]: 2017-01-01 1 2017-01-02 2 dtype: int64 9.滞后或者超前操作 In [43]: ts.shift(1) #正数为滞后 Out[43]: 2017-01-01 NaN #2017-01-01的数据为前一项的数据，但前一项没有数据所以为NaN 2017-01-02 1.0 # 2017-01-02数据为2017-01-01的数据，所以为1 2017-01-03 2.0 dtype: float64 In [44]: ts.shift(-1) #负数为超前 Out[44]: 2017-01-01 2.0 2017-01-02 3.0 2017-01-03 NaN dtype: float64 10.计算收益率 In [45]: (ts - ts.shift(1))/ts.shift(1) Out[45]: 2017-01-01 NaN 2017-01-02 1.0 2017-01-03 0.5 dtype: float64 In [124]: a = pd.Series([&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]) In [125]: a.value_counts() Out[125]: a 2 b 1 c 1 dtype: int64]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python魔法变量之__all__]]></title>
    <url>%2F2017%2F08%2F30%2Fpython%E9%AD%94%E6%B3%95%E5%8F%98%E9%87%8F%E4%B9%8B__all__%2F</url>
    <content type="text"><![CDATA[如果用户写成 from sound.effects import * 将会发生什么？理想情况下，你是希望到文件系统寻找这个包下的所有的模块，并导入它们。这可能要花费较长的时间，而且可能产生意想不到的副作用，这些作用本应该只有当子模块是显式导入时才会发生。 唯一的解决办法是包的作者为包提供显式的索引。import语句采用以下约定：如果包中的init.py代码定义了一个名为all的列表，那么在遇到 from package import * 语句时，会把列表中的所有模块导入。 例如，文件sound/effects/init.py可以包含以下代码： __all__ = [&quot;echo&quot;, &quot;surround&quot;, &quot;reverse&quot;] 这意味着 from sound.effects import * 将导入sound包的三个子模块。 如果all没有定义， from sound.effects import * 不会从sound.effects包中导入所有的子模块到当前命名空间，它只保证sound.effects包已经被导入(可能会运行init.py中任何初始化 的代码)，然后导入包中定义的任何名称。这包括 由init.py定义的任何的名称（以及显式加载的子模块）。还包括这个包中已经由前面的import 语句显式加载的子模块。 例： import sound.effects.echo import sound.effects.surround from sound.effects import * 当执行from … import语句时，echo和surround模块被导入当前命名空间，因为前面已经通过import语句显式加载。 Ref：官方文档]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy中的金融函数]]></title>
    <url>%2F2017%2F08%2F27%2Fnumpy%E4%B8%AD%E7%9A%84%E9%87%91%E8%9E%8D%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[fv: Futrue Value 终值 pv:Present Value 现值 npv:Net Present Value 净现值,净现值是指投资方案所产生的【现金净流量】（流入-流出）以资金成本为贴现率折现之后与原始投资额现值的差额 pmt:Payment 每次所要还的本金加利息 ppmt:Principal Payment 每期所还的本金 ipmt:Interst Payment 每期所还的利息 irr:Internal Rate of Return 内部收益率，内部收益率 mirr:Modified Internal Rate of Return 修正内部收益率 nper:Number of periodic payments 付款期数 rate: 利率 numpy.fv(rate, nper, pmt, pv, when=&#39;end&#39;) #计算未来的值 #例子：年利率为5%,每年定期存款14000，40年为累积财富为1691196.84 In [4]: np.fv(0.05, 40-1, -14000, -14000) Out[4]: 1691196.839394904 numpy.pv(rate, nper, pmt,fv=0.0, when=&#39;end&#39;) #计算现值 #例子：年利率5%，每月投入100，需要投入多少本金才可以在10年后的15682.93 In [8]: np.pv(0.05/12, 10*12, -100, 15692.93) Out[8]: -100.00067131625819 numpy.pmt(rate, nper, pv, fv=0, when=&quot;end&quot;)#计算每期应还的本金加利息 #例子：贷款434000，利率4.66%,240期 In [9]: np.pmt(0.0466/12, 20*12, 434000) Out[9]: -2783.3228668907836 numpy.ppmt(rate, per, nper, pv, fv=0.0, when=&quot;end&quot;)#计算应还本金 per:表示第几个还款日 #例子：贷款434000，利率4.66%,240期 In [3]: np.ppmt(0.0466/12, 1, 240, 434000) Out[3]: -1097.9562002241169 numpy.ipmt(rate, per, nper, pv, fv=0.0, when=&quot;end&quot;)#每期应还利息 #例子：贷款434000，利率4.66%,240期 In [5]: np.ipmt(0.0466/12, 1, 240, 434000) Out[5]: array(-1685.3666666666668) numpy.nper(rate, pmt, pv, fv=0, when=&quot;end&quot;)#计算付款次数 #例子：贷款434000，利率4.66%,每月还款2782 In [9]: np.nper(0.0466/12, -2782.14, 434000) Out[9]: 240.1684400099434 numpy.npv(rate, values) In [3]: np.npv(0.281,[-100, 39, 59, 55, 20]) Out[3]: -0.0084785916384513271 numpy.irr(values) #返回内部收益率 In [5]: np.irr([-100, 39, 59, 55, 20]) Out[5]: 0.28094842115996066 numpy.mirr(values, finance_rate, reinvest_rate) #返回修正内部收益率 values：现金流，一个列表，正数代表&#39;收入&#39;或&#39;取款&#39;，负数代表&#39;投资&#39;或&#39;存款&#39; In [6]: np.mirr([-1000, -4000, 5000, 2000], 0.1, 0.12) Out[6]: 0.17908568603489283 Ref：1.http://www.jianshu.com/p/9ad1318560782.https://en.wikipedia.org/wiki/Modified_internal_rate_of_return#Calculation_of_the_MIRR]]></content>
      <categories>
        <category>python科学计算</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hello-world]]></title>
    <url>%2F2017%2F08%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[1print(&quot;hello world&quot;)]]></content>
  </entry>
</search>
